\documentclass[../chapter-1-introduction.tex]{subfiles}
\begin{document}

The Cholesky decomposition of a positive-definite matrix has the form

\begin{equation}\label{eq:standard-cholesky-decomposition}
\Sigma = CC',
\end{equation}
\noindent
where $C = \left(c_{ij} \right)$ is a unique lower-triangular matrix with positive diagonal entries. This factorization is frequently encountered in optimization techniques and matrix computation; see \citet{golub2012matrix}. It is difficult to attach any statistical interpretation to the entries of $C$ in this form \citet{pinheiro1996unconstrained}. But by transforming $C$ to unit lower-triangular matrices, statistically interpreting of the diagonal entries of $C$ and the resulting unit lower-triangular matrix is much easier. To do this, one must simply divide the $i^{th}$ column of $C$ by its $i^{th}$ diagonal element $c_{ii}$. Letting $D^{1/2} = diag\left( c_{11},\dots, c_{MM} \right)$, the standard Cholesky decomposition \ref{eq:standard-cholesky-decomposition} can be written

\begin{equation}\label{eq:standard-cholesky-decomposition-transform}
\Sigma = CD^{-1/2}D^{1/2} D^{1/2} D^{-1/2}C' = L D L',
\end{equation}
\noindent
where $L = D^{-1/2}C$. This is commonly referred to as the modified Cholesky decomposition (MCD) of $\Sigma$. We can also write the modified Cholesky decomposition of the inverse covariance matrix:

\begin{equation}\label{eq:modified-cholesky-decomposition}
D = T\Sigma T', \quad \Sigma^{-1} = T'D^{-1} T,
\end{equation}
 \noindent
where $T = L^{-1}$. Like $P$ as in the spectral decomposition, the lower triangular matrix $T$ diagonalizes $\Sigma$. However, the Cholesky decomposition is perhaps more attractive since unlike the entries of the orthogonal matrix of the spectral decomposition, the entries of $T$ are unconstrained, and furthermore, have a specific statistical interpretation.

\bigskip

Like the variance-correlation decomposition of the inverse covariance matrix \ref{eq:inverse-covariance-decomposition}, the Cholesky factor $T$ and diagonal matrix $D$ can be constructed using components of a regression model. Consider regressing $y_t$ on its predecessors $y_1, \dots, y_{t-1}$. Let $Y = \left( y_1,\dots, y_M \right)'$ denote a mean zero random vector with positive definite covariance matrix $\Sigma$, and let $\hat{y}_t$ be the linear least-squares predictor of $y_t$ based on previous measurements $y_{t-1}, \dots , y_1$. Let  $\epsilon_t$ denote the corresponding prediction residual having variance  $\sigma_t^2 = Var\left(\epsilon_t\right)$. Standard regression machinery gives us that there exist unique scalars $\phi_{tj}$ so that

\begin{equation} \label{eq:mcd-ar-model}
y_t = \sum_{j = 1}^{t-1} \phi_{t,j} y_j + \sigma_t\epsilon_t, \quad t = 2, \dots, M
\end{equation}
\noindent
where 
\begin{equation*}
\epsilon_t = \left\{ \begin{array}{lr} 
y_t  -  \hat{y}_t, & t > 1 \\
y_t, & t = 1\end{array} \right. 
\end{equation*}
\noindent
are i.i.d. mean zero random variables with unit variance.  The connection between the Cholesky decomposition and the autoregressive model (\ref{eq:mcd-ar-model}) is established by noting that the Cholesky factor contains the negatives of the regression coefficients and the prediction error variances are the diagonal elements of $D$.  Let $\epsilon = \left(y_1, \dots, y_M\right)'$ denote the vector of uncorrelated prediction residuals with

\[
Cov\left(\epsilon\right) = D = diag\left(\sigma_1^2,\dots, \sigma_M^2\right)'.
\]
\noindent
Then model (\ref{eq:mcd-ar-model}) can be written in vector form $\epsilon = TY$,  where the $\left(t, j\right)$ entry of $T$ is $-\phi_{tj}$ , and the $(t, t)$ entry of $D$ is the $t^{th}$ prediction variance $\sigma_t^2 = var\left(\epsilon_t\right)$. 

\begin{align}
\begin{bmatrix}
1&&&&\\
-\phi_{21}&1&&&\\
-\phi_{31}&-\phi_{32}&1&&\\
\vdots &&&\ddots& \\
-\phi_{m1}&-\phi_{m2}& \dots & -\phi_{m,m-1}&1\\
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\ \ddots \\ y_m
\end{bmatrix} = \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\ \ddots \\ \epsilon_m
\end{bmatrix}
\end{align}


Table~\ref{table:cholesky-decomposition-successive-regressions} illustrates how the components of a covariance matrix are obtained through successive regressions. Specifically, this representation demonstrates how modeling a covariance matrix is equivalent to fitting a sequence of $M - 1$ varying-coefficient and varying-order regression models. Since the $\phi_{ij}$s are regression coefficients, for any unstructured covariance matrix, these and the log innovation variances are unconstrained. The regression coefficients of the model in (\label{eq:mcd-ar-model}) are referred to as the \textit{generalized autoregressive parameters} (GARP) and \textit{innovation variances} (IV) (\citet{pourahmadi1999joint}, \citet{pourahmadi2000maximum}). The powerful implication of the parallel regression framework of decomposition (\ref{eq:modified-cholesky-decomposition}) is the accessibility of the entire portfolio of regression methods for the service of modeling covariance matrices. Moreover, the estimator $\hat{\Sigma}^{-1} = \hat{T}' \hat{D}^{-1} {T}$ constructed from the unconstrained parameters $\phi_{ij}$, $\sigma_j^2$ is guaranteed to be positive definite. 
\bigskip

\begin{table}[H]
\centering
\caption{Autoregressive coefficients and prediction error variances of successive regressions.}
\begin{tabular}{cccccc}
 $y_{1}$&$y_{2}$ & $y_{3}$ & $\dots$ &$y_{m-1}$& $y_{m}$\\ \midrule
 $1$& &&&&\\
$\phi_{21}$& 1 &&&& \\
$\phi_{31}$& $\phi_{32}$& 1 &&& \\ 
$\vdots$ & $\vdots$ & & $\ddots$&& \\
$\vdots$ & $\vdots$ & && $\ddots$& \\
$\phi_{m1}$& $\phi_{m2}$&$\dots$ &$\dots$ &$\phi_{m,m-1}$ & 1\\ \midrule
$\sigma_1^2$ & $\sigma_1^2$ & $\dots$&$\dots$ &$\sigma_{m-1}^2$ &$\sigma_m^2$
\end{tabular} \label{table:cholesky-decomposition-successive-regressions}
\end{table}

%\bigskip
%
%immediately leads to the modified Cholesky decomposition \ref{eq:cholesky-matrix-decomposition}. It also can be used to clarify the close relation between the decomposition (2) and the time series ARMA models in that the latter is means to diagonalize a Toeplitz covariance matrix, for details see Pourahmadi (2001, Sec. 4.2.5).
%
%
%
%\needsparaphrased{In sharp contrast, the fact that the lower triangular matrix $T$ in the Cholesky decomposition of a covariance matrix $\Sigma$ is unconstrained makes it ideal for nonparametric estimation.
%Wu and Pourahmadi (2003) have used local polynomial estimators to smooth the subdiagonals of $T$. For the moment, denoting such estimators of $T$ and $D$ in (2) by $T$ and $D$, an
%estimator of $\Sigma$ given by $\Sigma = \hat{T}^{-1}D{\hat{T}^{-1}}^{\prime}$ is guaranteed to be positive-definite. Although one could smooth rows and columns of $T$,  the idea of smoothing along its subdiagonals is motivated by the similarity of the regressions in (3) to the varying-coefficients autoregressions (Kitagawa and Gersch, 1985, 1996; Dahlhaus, 1997): Xm
%
%Xm
%j=0
%\begin{equation}
%f_{j,p}\left(t/p\right)y_{t_j} = \sigma_p\left(t/p\right)\epsilon_t, \quad t = 0, 1, 2, \dots, M,
%\end{equation}
%\noindent
%where $f_{0,p}\left(�\right) = 1$, $f_{j,p}\left(�\right)$, 1 ? j ? m, and ?p(�) are continuous functions on $\left[0, 1\right]$ and 
%30 is a sequence of independent random variables each with mean zero and variance one. This analogy and comparison with the matrix $T$ for stationary autoregressions having constant
%entries along subdiagonals suggest taking the subdiagonals of $T$ to be realizations of some smooth univariate functions:
%
%\begin{equation*}
%\phi_{t,t-j} = f_{j,M}\left(t/M\right),\quad \sigma_t + \sigma_M \left(t/M\right). 
%\end{equation*}
%\begin{equation}
%z_{ijk}^T = \left(1, t_{ij} - t_{ik},\left( t_{ij} - t_{ik} \right)^2, \dots, \left(t_{ij} - t_{ik}\right)^{q-1} \right) \label{covmodel}
%\end{equation}




\bigskip

%
%From this perspective, it is apparent that the presentation of covariance estimation as a least squares regression problem suggests that the familiar ideas of model regularization for least-squares regression can be used for estimating covariances.  . \citet{huang2007estimation} 
%
%however, their two-step method did not utilize the information that many of the subdiagonals of T are essentially zeros at the first step. Inefficient estimation may result because of ignoring regularization structure in constructing the raw estimator. 
%
%\bigskip
%
%Several have applied these approaches to covariance estimation; 
%\bigskip
%
%Alternatively, one can view $T$ as a bivariate function,
%
%Several others have considered this approach to covariance estimation; \citet{kaufman2008covariance} assume a stationary process, restricting covariance estimates to a specific class of functions.  They as well as  Huang, Liu, and Liu \citet{huang2007estimation} follow the hueristic argument presented by \citet{pourahmadi1999joint} that $\phi_{t,t-l}$ is monotone decreasing in $l$ and set off-diagonal elements of either the covariance matrix or the Cholesky factor corresponding to large lags to zero.   As in \citet{huang2007estimation}, \citet{kaufman2008covariance}, and \citet{yao2005functional}, we treat covariance estimation as a function estimation problem where the covariance matrix is viewed as the evaluation of a smooth function at particular design points. 
%
%including \citet{bickel2008regularized} and \citet{huang2006covariance}  have proposed nonparametric estimators of a specific covariance matrix (or its inverse) rather than the parameters of a covariance function. 
%
%\bigskip
%
%\citet{yao2005functional} do not utilize the Cholesky parameterization, and their estimates are not guaranteed to be positive definite.  We combine the advantages of bivariate smoothing as in \citet{yao2005functional} with the added utility of the Cholesky parameterization in \citet{huang2007estimation}; in doing so, we present a flexible and coherent approach to covariance estimation, while simultaneously we ensuring positive definiteness of estimates.Rather than shrinking element of the Cholesky factor to zero after a particular value of $l$, we choose to softly enforce monotonicity in $l$ by using a hinge penalty as in the work of \citet{tibshirani2011nearly}. 
\end{document}
