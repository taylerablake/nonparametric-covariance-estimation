%\documentclass[12pt]{article}
%\usepackage{graphicx,psfrag,amsfonts,float,mathbbol,xcolor,cleveref}
%\usepackage{arydshln}
%\usepackage{amsmath}
%\usepackage{subfiles}
%\usepackage{tikz}
%\usepackage[mathscr]{euscript}
%\usepackage{enumitem}
%\usepackage{accents}
%\usepackage{framed}
%\usepackage[utf8]{inputenc}
%\usepackage{subcaption}
%\usepackage{algorithm}
%\usepackage{etoolbox}
%\usepackage{lscape}
%\usepackage{algorithmic}
%\usepackage{nomencl}
%\usepackage{natbib}
%\usepackage{mathtools}
%\usepackage{IEEEtrantools}
%\usepackage{times}
%\usepackage{arydshln}
%\usepackage{cite}
%\usepackage{amsthm}
%\usepackage[letterpaper, left=1in, top=1in, right=1in, bottom=1in,nohead,includefoot, verbose, ignoremp]{geometry}
%\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
%\newcommand*\needsparaphrased{\color{red}}
%\newcommand*\needscited{\color{orange}}
%\newcommand*\needsproof{\color{blue}}
%\newcommand*\outlineskeleton{\color{green}}
%\newcommand{\ms}{\scriptscriptstyle}
%\newcommand{\hilbert}{\mathcal{H}}
%\newcommand{\hilbertl}{\mathcal{H}_{\langle l \rangle}}
%\newcommand{\hilbertm}{\mathcal{H}_{\langle m \rangle}}
%\newcommand{\hilbertlnull}{\mathcal{H}_{0\langle l \rangle}}
%\newcommand{\hilbertmnull}{\mathcal{H}_{0\langle m \rangle}}
%\newcommand{\hilbertlpen}{\mathcal{H}_{1\langle l \rangle}}
%\newcommand{\hilbertmpen}{\mathcal{H}_{1\langle m \rangle}}
%\newcommand{\PP}{\mathcal{P}}
%\newcommand{\vphistar}{\mbox{\boldmath $\phi$}}
%\newcommand{\vsigmasq}{\mbox{\boldmath $\sigma^2$}}
%\newcommand{\bfeps}{\mbox{\boldmath $\epsilon$}}
%\newcommand{\bfgamma}{\mbox{\boldmath $\gamma$}}
%\newcommand{\bflam}{\mbox{\boldmath $\lambda$}}
%\newcommand{\bfphi}{\mbox{\boldmath $\phi$}}
%\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
%\newcommand{\bfkappa}{\mbox{\boldmath $\kappa$}}
%\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
%\newcommand{\bfalpha}{\mbox{\boldmath $\alpha$}}
%\newcommand{\bftheta}{\mbox{\boldmath $\theta$}}
%\newcommand{\bfe}{\mbox{\boldmath $e$}}
%\newcommand{\bft}{\mbox{\boldmath $t$}}
%\newcommand{\bfg}{\mbox{\boldmath $g$}}
%\newcommand{\bfv}{\mbox{\boldmath $v$}}
%\newcommand{\bff}{\mbox{\boldmath $f$}}
%\newcommand{\bfone}{\mbox{\boldmath $1$}}
%\newcommand{\bfo}{\mbox{\boldmath $0$}}
%\newcommand{\bfO}{\mbox{\boldmath $O$}}
%\newcommand{\bfx}{\mbox{\boldmath $x$}}
%\newcommand{\bfX}{\mbox{\boldmath $X$}}
%\newcommand{\bfz}{\mbox{\boldmath $z$}}
%\newcommand{\tildeY}{\tilde{Y}}
%\newcommand{\tildey}{\tilde{y}}
%\newcommand{\tildeQ}{\tilde{Q}}
%\newcommand{\tildeR}{\tilde{R}}
%\newcommand{\tildeA}{\tilde{A}}
%\newcommand{\tildeepsilon}{\tilde{\epsilon}}
%\newcommand{\bfepsilon}{\mbox{\boldmath $\epsilon$}}
%\newcommand{\tildeS}{\tilde{S}}
%
%\newcommand{\bfm}{\mbox{\boldmath $m}}
%\newcommand{\bfy}{\mbox{\boldmath $y$}}
%\newcommand{\bfa}{\mbox{\boldmath $a$}}
%\newcommand{\bfb}{\mbox{\boldmath $b$}}
%\newcommand{\bfY}{\mbox{\boldmath $Y$}}
%\newcommand{\bfB}{\mbox{\boldmath $B$}}
%\newcommand{\bfZ}{\mbox{\boldmath $Z$}}
%\newcommand{\tildeB}{\tilde{B}}
%
%\newcommand{\cardT}{\vert \mathcal{T} \vert}
%%\newenvironment{theorem}[1][Theorem]{\begin{trivlist}
%%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%%\newenvironment{corollary}[1][Corollary]{\begin{trivlist}
%%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%%\newenvironment{proposition}[1][Proposition]{\begin{trivlist}
%%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%%\newenvironment{definition}[1][Definition]{\begin{trivlist}
%%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%
%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{corollary}[theorem]{Corollary}
%
%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]
%\newtheorem{example}{Example}[section]
%\def\bL{\mathbf{L}}
%
%\begingroup\lccode`~=`_
%\lowercase{\endgroup\def~}#1{_{\scriptscriptstyle#1}}
%\AtBeginDocument{\mathcode`_="8000 \catcode`_=12 }
%
%\makeatletter
%\renewcommand{\theenumi}{\Roman{enumi}}
%\renewcommand{\labelenumi}{\theenumi.}
%\renewcommand{\theenumii}{\Alph{enumii}}
%\renewcommand{\labelenumii}{\theenumii.}
%\renewcommand{\p@enumii}{\theenumi.}
%\makeatother
%
%\begin{document}
%\listoftables
%\nocite{*}
%\def\bL{\mathbf{L}}
%\usepackage{mathtime}

%%UNCOMMENT following line if you have package


%\title{ Nonparametric Covariance Estimation for Longitudinal Data via Penalized Tensor Product Splines}
%
%\author{Tayler A. Blake\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201} \and  Yoonkyung Lee\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201}}
%
%\bibliographystyle{plainnat}
%\maketitle

\chapter{Modeling the Cholesky decomposition via smoothing spline ANOVA models} \label{chapter-2}
%
%A predominant difficulty in the estimation of covariance matrices is the potentially high dimensionality of the problem, as the number of unknown elements in the covariance matrix grows quadratically with the size of the matrix. It is well-known that the sample covariance matrix can be unstable in high dimensions; ways for controlling the complexity of estimates is highly desirable for improving stability of estimates. In the longitudinal-data literature, it is a common practice to use parametric models for the covariance structure.  Many have specified parsimonious parametric models for $\phi_{ijk}$ to overcome the issue of dimensionality.  
%
%\bigskip
%
%We naturally accommodate irregularly spaced data and unequal sample sizes between subjects by defining the autoregressive parameters as the values of a smooth function evaluated at within-subject pairs of observed time points.  Furthermore, by viewing $\phi\left(t,s\right)$ as a smooth \emph{bivariate} function, we can utilize the information across the subdiagonals of $T$ to inform the fit, rather than treating each subdiagonal separately.  As in the classical nonparametric function estimation setting, we assume $\phi$ to vary in a high-dimensional (possibly infinite) function space. We propose two representations of $\phi\left(\cdot, \cdot\right)$ and $\sigma\left(\cdot, \cdot\right)$: approximation by smoothing splines and approximation by B-spline basis expansion. 
%
%We assume $Y\left(t\right)$ has covariance function $G\left(t,s\right)$ and that $\epsilon\left(t\right)$ follows a zero mean Gaussian white noise process with unit variance. Under mild assumptions regarding the behaviour of $Y$, then $G\left(t,s\right)$ satisfies some smoothness conditions, where smoothness is defined in terms of square integrability of certain derivatives. We view the entries of $\Sigma$ as values of $G$ evaluated at the distinct pairs of within-subject observed time points. 
%\bigskip


If we consider the Cholesky decomposition of $\Sigma$ within such functional context, it is natural to extent the same notion to the elements of $T$ and $D$. We take the GARPs $\lbrace \phi_{tj} \rbrace$ and innovation variances to be the evaluation of the smooth functions $\tilde{\phi}\left(t,s\right)$ and $\sigma^2\left(t\right)$ at observed time points, which we assume  are drawn from some distribution having compact domain $\mathcal{T}$. Without loss of generality, we take $\mathcal{T} = \left[0,1\right]$. Henceforth, we view $\tilde{\phi}$ and $\sigma^2$ as a smooth continuous functions, but for ease of exposition, we let $\tilde{\phi}_{ij}$ denote the varying coefficient function evalutated at $\left(t_i,t_j\right)$: 
\[
\tilde{\phi}_{tj} = \tilde{\phi}\left(t_{i},t_{j}\right). 
\]
Adopting similar notation for the innovation variance function, denote
\[
\sigma_{j}^2 = \sigma^2\left(t_{j}\right),
\]
where $0 \le t_{j} < t_{i} \le 1$ for $j < i$. This leads to varying coefficient model

\begin{equation}  \label{eq:cholesky-regression-model-1} 
y\left(t_{i} \right)  = \sum_{j=1}^{i-1} \tilde{\phi}\left(t_{i} ,t_{j}\right) y\left(t_{j}\right) + \sigma\left(t_{j}\right)\epsilon\left({t_j}\right) \;\;\;\; i=1,\dots, M, 
\end{equation}
\noindent

Our goal is now to estimate the above model, utilizing bivariate smoothing to estimate $\tilde{\phi}\left(t,s\right)$ for $0 \le s < t \le 1$,  and one-dimensional smoothing to estimate $\sigma\left(t \right)$, $0 \le t \le 1$. Our proposed method for covariance estimation defines a flexible, general framework which makes all of the existing techniques for penalized regression accessible for the seemingly far different task of estimating a covariance matrix.

\bigskip

Our approach to estimation is constructed to provide a fully data-driven methodology for selecting the optimal covariance model (given some optimization criterion) from a expansive class of estimators ranging in complexity from that of the previously aforementioned parametric models to that of completely unstructured estimators, like the sample covariance matrix. We leverage the collection of regularization techniques that are accessible in the usual function estimation setting. By properly specifying the roughness penalty, our optimization procedure results in null models which correspond to the parametric and semiparametric models for $\phi$ and $\sigma^2$ discussed in \ref{chapter-1-parametric-semiparametric-garp-models}. To facilitate the penalty specification that achieves this, we consider modeling the varying coefficient function which takes inputs

\begin{align} 
\begin{split}\label{eq:l-m-transformation}
l &= t - s \\
m &= \frac{t + s}{2}, \\
\end{split}
\end{align}
\noindent
 where $l$ is the continuous analogue of the usual ``lag'' between time points $t$ and $s$, and $m$ is simply its orthogonal direction. We have discussed many parsimonious covariance structures which model $y\left(t\right)$ as a stationary process with covariance function which depends on time points $t_i$ and $t_j$ only through the Euclidean distance $\vert \vert t_i - t_j \vert \vert$ between them. Covariance functions taking the form $Cov\left(y\left( t_i \right),y\left( t_j \right)\right) =G\left(t_i,t_j\right) = G\left(\vert \vert t_i - t_j \vert \vert \right)$ can then be written as 

\begin{equation*}
Cov\left(y\left( t_i \right),y\left( t_j \right)\right) = G\left( l_{ij}  \right)
\end{equation*}
\noindent
where $l_{ij} =  \vert  t_i - t_j  \vert $. Regularizing the functional components of the Cholesky decomposition so that functions incurring large penalty correspond to functions which vary in only $l$ and are constant in $m$ allows us to model nonstationarity in a fully data-driven way.  Our goal is to estimate

\begin{equation} 
\phi\left(l,m\right) = \phi\left(s-t, \frac{1}{2}\left(s+t\right)\right) = \tilde{\phi}\left(t,s\right).
\end{equation}

\bigskip

While our framework allows for estimation of the autoregressive coefficient function and the innovation variance function via any nonparametric regression setup, we focus on two primary approaches for representing $\phi$ and $\sigma$. First, we assume that $\phi$ belongs to a reproducing kernel Hilbert space, $\mathcal{H}$ and employ the smoothing spline methods of Kimeldorf and Wahba (see \citet{kimeldorf1971some} and \citet{wahba1990spline} for comprehensive presentation.)  To enhance the statistical interpretability of model parameters, we decompose $\phi$ into functional components similar to the notion of the main effect and the interaction terms in classical analysis of variance. We adopt the smoothing spline analogue of the classical ANOVA model proposed by Gu \citet{gu2013smoothing}, and estimation is achieved through similar computational strategies.

\subsection{Penalized maxiumum likelihood estimation of $\phi$, $\log\sigma^2$}

Let random vector $Y$ follow a multivariate normal distribution with zero mean vector and covariance $\Sigma$. The loglikelihood function $\ell \left( Y, \Sigma \right)$ satisfies

\begin{equation} \label{eq:loglik-general-form}
-2\ell\left( Y, \Sigma \right) = \log \vert \Sigma \vert + Y' \Sigma Y
\end{equation}
\noindent
Using $T \Sigma T' = D$, we can write 
\[
\vert \Sigma\vert = \vert D \vert = \prod_{i = 1}^m \sigma_i^2
\]
and 
\[
\Sigma^{-1} = T' D^{-1} T.
\]
Writing \ref{eq:loglik-general-form} in terms of the prediction errors and their variances of the non-redundant entries of $\left(T , D\right)$, we have

\begin{align}
\begin{split} \label{eq:loglik-cholesky-form}
-2\ell\left( Y, \Sigma \right) &= \log \vert D \vert + Y' T' D^{-1} T Y \\
&= \sum_{i = 1}^m \log \sigma_i^2  + \sum_{i = 1}^m \frac {\epsilon_i^2}{\sigma_i^2},
\end{split}
\end{align}
\noindent
where 
\begin{equation} \label{eq:loglik-cholesky-form}
\epsilon_i = \left\{\begin{array}{lr}y\left(t_1\right), & i = 1, \\
y\left(t_i\right) - \sum_{j = 1}^{i-1} \phi\left(\bfv_{ij}\right) y_j, & i= 2, \dots, M, \\
\end{array} \right.
\end{equation}
\noindent
where $\phi\left(\bfv_{ij}\right) = \phi\left(l_{ij},m_{ij}\right) = \tilde{\phi}\left(t_i,t_j\right)$.  Accommodating subject-specific sample sizes and measurement times merely requires appending an additional index to observation times. Let  $Y_1, \dots, Y_N$ denote a sample of $N$ independent mean zero random trajectories from a  multivariate normal distribution with common covariance $\Sigma$. We associate with each trajectory $Y_i = \left(y_{i1}, \dots, y_{i,m_i}\right)'$ with a vector of potentially subject-specific observation times $\left(t_{i1}, \dots, t_{i,m_i}\right)'$, so that the $j^{th}$ measurement of trajectory $i$ is modeled

\begin{align}
\begin{split} \label{eq:cholesky-regression-model-2} 
y\left(t_{ij} \right)  &= \sum_{k=1}^{j-1} \tilde{\phi}\left(t_{ij} ,t_{ik}\right) y\left(t_{ik}\right) + \sigma\left(t_{ij}\right)\epsilon\left(t_{ij}\right)  \\
&= \sum_{k=1}^{j-1} \phi\left(\bfv_{ijk}\right) y\left(t_{ik}\right) + \sigma\left(t_{ij}\right)\epsilon\left(t_{ij}\right)
\end{split}
\end{align}
\noindent
for $i = 1,\dots, N$, $j = 2,\dots, m_i$.
\noindent
Making similar ammendments to indexing, the joint log likelihood for the sample $Y_1, \dots, Y_N$ is given by  

\begin{equation} \label{eq:joint-loglik}
-2\ell\left( Y_1,\dots, Y_N, \phi, \sigma^2 \right) = \sum_{i = 1}^N \sum_{j = 1}^{m_i} \log \sigma_{ij}^2  + \sum_{i = 1}^N \sum_{j = 1}^{m_i} \frac {\epsilon_{ij}^2}{\sigma_{ij}^2},
\end{equation}

\bigskip

With this, we can estimate $\phi$ and $\log\sigma^2$ using maximum likelihood or any of its penalized variants by appending a roughness penalty (penalties) to \ref{eq:joint-loglik}. Employing regularization, we take $\phi$, $\sigma^2$ to minimize 

\begin{equation} \label{eq:penalized-joint-loglik}
-2\ell\left( Y_1,\dots, Y_N, \phi, \sigma^2 \right) +    \lambda J\left( \phi \right) +  \breve{\lambda}\breve{J}\left( \sigma^2 \right),
\end{equation}
\noindent
where $J$ and $\breve{J}$ are roughness penalties on $\phi$ and $\sigma^2$, and $\lambda$, $\breve{\lambda}$ are non-negative smoothing parameters.  To jointly estimate the GARP function and the IV function, we adopt an iterative approach in the spirit of \citet{huang2006covariance}, \citet{huang2007estimation}, and \citet{pourahmadi2000maximum}. A procedure for minimizing \ref{eq:joint-loglik} starts with initializing $\left\{\sigma^2_{ij}\right\} = 1$ for $i = 1,\dots, N$, $j = 1,\dots, m_i$.  For fixed $\sigma^2$, the penalized likelihood (as a function of $\phi$) is given by

\begin{equation} \label{eq:penalized-joint-loglik-given-sigma}
-2\ell_\phi + \lambda J\left(\phi\right) = \sum_{i=1}^N \sum_{j=2}^{m_i} \sigma^{-2}_{ij}\left( y_{ij} - \sum_{k<j} \phi\left(\bfv_{ijk}\right) y_{ik}  \right)^2 + \lambda J\left( \phi \right),
\end{equation}
\noindent
which corresponds to the usual penalized least squares functional encountered  in the nonparametric function estimation literature. The first term, the residual sums of squares, encourages the fitted function's fidelity to the data. The second term penalizes the roughness of $\phi$, and $\lambda$ is a smoothing parameter which controls the tradeoff between the two conflicting concerns. Given $\phi^*$ the minimizer of \ref{eq:penalized-joint-loglik-given-sigma} and setting $\phi = \phi^*$, we update our estimate of $\sigma^2$ by minimizing 

\begin{equation} \label{eq:penalized-joint-loglik-given-phi}
-2\ell_{\sigma^2} + \breve{\lambda} \breve{J}\left(\sigma^2\right) = \sum_{i=1}^N \sum_{j=2}^{m_i} \log \sigma^2_{ij} + \sum_{i=1}^N \sum_{j=1}^{m_i} \sigma_{ij}^{-2} {r_{ij}^*}^2 + \breve{\lambda} \breve{J}\left(\sigma^2 \right),
\end{equation}
where the $\left\{{r_{ij}^*}^2  =\left( y_{ij} - \sum_{k<j} \phi^*\left(\bfv_{ijk}\right) y_{ik}  \right)\right\}$ denote the working residuals based on the current estimate of $\phi$. This process of iteratively updating $\phi^*$ and ${\sigma^2}^*$ is repeated until convergence is achieved. 
\bigskip

The remainder of the chapter is reserved for presenting two functional representations of $\left(\phi, \sigma^2\right)$. The first leverages the rich theoretical foundation of reproducing kernel Hilbert space techniques for function estimation. This framework has been studied extensively for the problem of estimating a function nonparametrically (see \citet{aronszajn1950theory}, \citet{wahba1990spline}, and \citet{berlinet2011reproducing} for detailed examinations), but to our knowledge has received little attention in the context of covariance models. We use a smoothing spline ANOVA decomposition of the varying coefficient function $\phi$ to construct a flexible class of covariance models while simultaneously maintaining interpretability. The second approach is based on the penalized B-splines, or P-splines, of \citet{eilers1996flexible}; these models exhibit many of the attractive numerical properties of the basis functions on which they are built. The formulation of the penalty is independent of the basis, which provides added modeling flexibility due to the ease with which one can employ various types of regularization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Smoothing spline representation of $\phi$, $\sigma$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{An RKHS framework for estimating $\phi$} \label{RKHS-framework-for-phi}
%\subfile{chapter-2-subfiles/chapter-2-smoothing-spline-representation}
This section presents a method for regularized estimation of the varying coefficient function $\phi$ using a reproducing kernel Hilbert space (RKHS) framework. To do so, we first must establish some notation and review the relevant mathematical details of the surrounding framework. A Hilbert space $\hilbert$ of functions on a set $\mathcal{V}$ with inner product $\langle \cdot, \cdot\rangle_\hilbert$ is defined as a complete inner product linear space. A Hilbert space is called a reproducing kernel Hilbert space if the evaluation functional $\left[\bfv\right]f = f\left(\bfv\right)$ is continuous in $\hilbert$ for all $\bfv \in \mathcal{V}$. The Reisz Representation Theorem gives that there exists $Q \in \hilbert$, the representer of the evaluation functional $\left[\bfv\right]\left(\cdot\right)$, such that $\langle Q_\bfv, \phi \rangle_\hilbert = \phi\left(\bfv\right)$ for all $\phi \in \mathcal{H}$. See \citet{gu2013smoothing} Theorem 2.2.

\bigskip

The symmetric, bivariate function $Q\left(\bfv_1, \bfv_2 \right) = Q_{\bfv_2 }\left(\bfv_1\right) = \langle Q_{\bfv_1}, Q_{\bfv_2} \rangle_\hilbert$ is called the reproducing kernel (RK) of $\hilbert$. The RK satisfies that for every $v \in \mathcal{V}$ and $f \in \mathcal{H}$,

\begin{enumerate}
\item $Q\left(\cdot, \bfv \right) \in \hilbert$ 
\item $f\left(\bfv\right) = \langle f, Q\left(\cdot, v\right)\rangle_\hilbert$\label{rkhs-reproducing-property}
\end{enumerate}
\noindent
The first property is called the reproducing property of $Q$. Every reproducing kernel uniquely determines the RKHS, and in turn, every RKHS has unique reproducing kernel. See \citet{gu2013smoothing}, Theorem 2.3. The kernel satisfies that for any $\left\{\bfv_1,\dots, \bfv_{n_1}\right\}$, $\left\{\breve{\bfv}_1,\dots, \breve{\bfv}_{n_2}\right\} \in \mathcal{V}$ and $\left\{a_1,\dots, a_{n_1}\right\}$, $\left\{a_1,\dots, a'_{n_2}\right\} \in \Re$,

\begin{equation}
 \langle\sum_{i = 1}^{n_1} a_i Q\left(\cdot, \bfv_i\right), \sum_{j = 1}^{n_2} a'_j Q\left(\cdot, \breve{\bfv}_j\right) \rangle_\hilbert.
\end{equation}

\bigskip


Let $\mathcal{N}_J = \left\{ \phi:\; J\left(\phi\right) = 0\right\}$ denote the null space of $J$, and consider the decomposition

\[
\hilbert = \mathcal{N}_J \oplus \hilbert_J.
\]
\noindent
The space $\hilbert_J$ is a RKHS having $J\left(\phi\right)$ as the squared norm. The minimizer of \ref{eq:penalized-likelihood-vectorized} has form 

\begin{equation} \label{eq:RKHS-functional-form}
\phi\left(\bfv\right) = \sum_{\nu = 1}^{d_0} d_\nu \eta_\nu\left( \bfv \right) + \sum_{i=1}^n c_i Q\left(\bfv_i, \bfv \right),
\end{equation} 
\bigskip
\noindent
where $\lbrace \eta_\nu \rbrace$ is a basis for $\mathcal{N}_J$, and $Q_J$ is the RK in $\hilbert_J$. 

\bigskip


The objective function \ref{eq:penalized-least-squares} can be rewritten in terms of the squared norm with respect to $\langle\cdot,\cdot\rangle_\hilbert$:
\begin{equation} \label{eq:penalized-least-squares-2}
-2\ell_\phi + \lambda J\left(\phi\right) = \sum_{i=1}^N \sum_{j=2}^{m_i} \sigma^{-2}_{ij}\left( y_{ij} - \sum_{k<j}\left( L_{ijk}\phi\right) y_{ik}  \right)^2 + \lambda \vert\vert P_J \phi \vert \vert^2
\end{equation}
\noindent
where $P_J$ is the projection operator which projects $\phi$ onto the subspace $\hilbert_J$, and $L_{ijk}$ denotes the evalutation functional $\left[v_{ijk}\right] \phi$. Let $\xi_{ijk}$ denote the representer of $L_{ijk}$; \citet{kimeldorf1971some} established that the minimizer of \ref{eq:penalized-least-squares-2} has form

\begin{equation} \label{eq:form-of-smoothing-spline-solution}
\phi\left( \bfv \right) = \sum_{\nu=1}^{m} d_\nu \eta_\nu \left( v \right) + \sum_{i = 1}^{\vert V \vert} c_{i}\left(P_J \xi_i \right)
\end{equation}
\noindent
where $V = \bigcup\limits_{i,j,k} \bfv_{ijk}$, and $\left\{\eta_1,\dots, \eta_{m}\right\}$ span $\hilbert_0$, the null space of $P_J$. To show this, we start by noting that any $\phi \in \mathcal{H}$ can be written 

\begin{equation} \label{eq:smoothing-spline-representer-expansion-1}
\phi\left( \bfv \right) = \sum_{\nu=1}^{m} d_\nu \eta_\nu \left( v \right) + \sum_{i = 1}^{\vert V \vert} c_{i}\left(P_J \xi_i \right) + \rho\left(\bfv\right)
\end{equation}
\noindent
where $\rho \perp \mathcal{H}_0,\; \textup{span}\lbrace \left(P_1 \xi_{j} \right) \rbrace_{j = 1}^{\vert V\vert}$. To establish that the solution has form \ref{eq:form-of-smoothing-spline-solution} requires showing that the minimizer of \ref{eq:penalized-least-squares-2} has $\rho = 0$. The proof entails demonstrating that $\rho$ does not improve the residual sums of squares and only adds to the penalty term, $J\left(\phi\right)$. Details are similar to those in the proof provided in \citet{wahba1990spline} and are left to the appendix \ref{chapter-7-appendix}.

\bigskip

%\subfile{chapter-2-subfiles/tensor-product-hilbert-space-construction}
\begin{equation}
{Q_{\left[i\right]} }_v'\left(v\right) = k_{m_i}\left(v\right)k_{m_i}\left(v'\right) + \left(-1\right)^{m_i-1}k_{2m_i}\left(v' - v\right)
\end{equation}
\noindent
See \citet{gu2002smoothing} Example 2.3.3 for proof. The tensor product smoothing spline results from letting $m_1 = m_2 = 2$, so that the marginal subspaces can be written

\begin{align} \label{eq:cubic-spline-hilbert-space}
\left\{ \phi: \phi'' \in \mathcal{L}_2\left[0,1\right] \right\} = &\left\{ \phi: \phi \propto 1 \right\} \oplus  \left\{ \phi: \phi \propto k_1 \right\} \oplus \left\{ \phi: \int_0^1 \phi dv = \int_0^1 \phi' dv = 0,\; \phi'' \in \mathcal{L}_2\left[0,1\right]  \right\} \\
&= \hilbert_{00} \oplus \hilbert_{01} \oplus \hilbert_1,
\end{align}
\noindent
where $ \hilbert_{01} \oplus \hilbert_1$ forms the contrast in a one-way ANOVA decomposition with averaging operator $\mathcal{A}\phi = \int_0^1 \phi\;dv$. The corresponding reproducing kernels are
\begin{align} \label{eq:cubic-spline-hilbert-space-rks}
Q_{00}\left(v,v'\right) &= 1\\
Q_{01}\left(v,v'\right) &= k_1\left(v\right)k_1\left(v'\right)\\
Q_{1}\left(v,v'\right) &= k_2\left(v\right)k_2\left(v'\right) - k_4\left(v-v'\right).
\end{align}
\noindent
The tensor product space can be constructed with nine tensor sum terms; the construction of the tensor product space from the terms of the tensor sum. The corresponding reproducing kernels and inner products are given in Table~\ref{table:tensor-product-cubic-spline-RKHS-table} and Table~\ref{table:tensor-product-cubic-spline-RK-table}, respectively.

\begin{table}[H]
\centering % used for centering table
\begin{tabular}{r|c|c|c|} % centered columns (4 columns)
\multicolumn{1}{c}{} & \multicolumn{1}{c}{	$\hilbert_{00\left[2\right]}$}	&	\multicolumn{1}{c}{$\hilbert_{01\left[2\right]}$}	&\multicolumn{1}{c}{ $\hilbert_{1\left[2\right]}$}\\ [1.5ex] 
\cline{2-4}  % inserts single horizontal line\\
$\hilbert_{00\left[1\right]}$		& $\hilbert_{00\left[1\right]}\otimes \hilbert_{00\left[2\right]}$ 	&	$\hilbert_{00\left[1\right]}	\otimes \hilbert_{01\left[2\right]} $	&	$\hilbert_{00\left[1\right]}	\otimes \hilbert_{1\left[2\right]}$   \\ [1.5ex] 
$\hilbert_{01\left[1\right]}$		& $\hilbert_{01\left[1\right]} \otimes \hilbert_{00\left[2\right]}$			& 	$\hilbert_{01\left[1\right]} \otimes \hilbert_{01\left[2\right]}$   &   $\hilbert_{01\left[1\right]} \otimes \hilbert_{1\left[2\right]}$\\ [1.5ex] 
 $\hilbert_{1\left[1\right]}$	& 	 $\hilbert_{1\left[1\right]} \otimes \hilbert_{00\left[2\right]}$	&	$\hilbert_{1\left[1\right]} \otimes \hilbert_{01\left[2\right]}$ 	&	$\hilbert_{1\left[1\right]} \otimes \hilbert_{1\left[2\right]}$ \\ [1.5ex] 
\cline{2-4}
\end{tabular}
\caption{Construction of the tensor product cubic spline subspace from marginal subspaces $\hilbert_{\left[1\right]}$, $\hilbert_{\left[2\right]}$} % title of Table
\label{table:tensor-product-cubic-spline-RKHS-table}
\end{table}

\begin{landscape}
\begin{table}[H]
\caption{Tensor product cubic spline subspace reproducing kernels and inner products} % title of Table
\centering % used for centering table
\begin{tabular}{lll} % centered columns (4 columns)
\hline\hline %inserts double horizontal lines
Subspace 	& 		Reproducing kernel 		& 	Inner product \\
\hline % inserts single horizontal line
$\hilbert_{00\left[1\right]} \otimes \hilbert_{00\left[2\right]}$ & 	$1$								     & 	$\left( \int_0^1 \int_0^1 f \right) \left( \int_0^1 \int_0^1 g \right)$ \\ [1ex] 
$\hilbert_{01\left[1\right]} \otimes \hilbert_{00\left[2\right]} $& 	$k_1\left(l\right)k_1\left(l'\right)$						     & 	$\left( \int_0^1 \int_0^1 f'_{\left[1\right]} \right) \left( \int_0^1 \int_0^1 g'_{\left[1\right]} \right)$ \\ [1ex] 
$\hilbert_{01\left[1\right]} \otimes \hilbert_{01\left[2\right]}$ & 	$k_1\left(l\right)k_1\left(l'\right)k_1\left(m\right)k_1\left(m'\right)$ & $\left( \int_0^1 \int_0^1 f''_{\left[12\right]} \right) \left( \int_0^1 \int_0^1 g''_{\left[12\right]} \right)$ \\ [1ex] 
$\hilbert_{1\left[1\right]} \otimes \hilbert_{00\left[2\right]}$  	& 	$k_2\left(l\right)k_2\left(l'\right) - k_4\left(l - l'\right)$	      & $\int_0^1 \left( \int_0^1 f''_{\left[12\right]}\;dl' \right) \left(  \int_0^1 g''_{\left[12\right]} \;dl'\right)\;dl $\\ [1ex] 
$\hilbert_{1\left[1\right]} \otimes \hilbert_{01\left[2\right]}$ 	& 	$\left[k_2\left(l\right)k_2\left(l'\right) - k_4\left(l - l'\right)\right]k_1\left(m\right)k_1\left(m'\right)$ & $\int_0^1 \left( \int_0^1 f^{\left(3\right)}_{\left[112\right]}\;dl' \right) \left(  \int_0^1 g^{\left(3\right)}_{\left[112\right]} \;dl'\right)\;dl$ \\ [1ex]  
$\hilbert_{1\left[1\right]} \otimes \hilbert_{1\left[2\right]}$  		& $\left[k_2\left(l\right)k_2\left(l'\right) - k_4\left(l - l'\right)\right]\left[k_2\left(m\right)k_2\left(m'\right) - k_4\left(m - m'\right)\right]$ & $\int_0^1  \int_0^1 f^{\left(4\right)}_{\left[1122\right]}g^{\left(4\right)}_{\left[1122\right]}$ \\ [1ex]  
\hline %inserts single line
\end{tabular}
\label{table:tensor-product-cubic-spline-RK-table}
\end{table}
\end{landscape}
\end{example}


\bigskip

For $\bfv \in V$ where $V$ is a product domain, ANOVA decompositions can be characterized by 
\begin{equation}\label{eq:ssanova-decomposition-of-RKHS}
\hilbert = \bigoplus\limits_{\beta=0}^{g} \hilbert_\beta
\end{equation}
\noindent
and
\begin{equation}\label{eq:ssanova-decomposition-of-penalty}
J\left(\phi\right) = \sum_{\beta=0}^{g} \theta^{-1}_\beta J_\beta \left( \phi_\beta \right),
\end{equation}
\noindent
where $\phi_\beta \in \hilbert_\beta$, $J_\beta$ is the square norm in $\hilbert_\beta$, and $0 < \theta_\beta < \infty$. This gives 

\begin{align*}
\hilbert_0 &= \mathcal{N}_J \\
\hilbert_J &= \bigoplus\limits_{\beta=1}^{g} \hilbert_\beta, \mbox{ and} \\
Q &= \sum_{\beta=1}^g \theta_\beta Q_\beta,
\end{align*}
\noindent
where $Q_\beta$ is the RK in $\hilbert_\beta$. The $\left \{ \theta_\beta \right\}$ are additional smoothing parameters, which are implicit in notation to follow for the sake of concise demonstration. 

\bigskip


\bigskip
\noindent
Let $Y$ denote the vector of length $n_y= \sum_{i} M_i - N$  constructed by stacking the $N$ observed response vectors $Y_1,\dots, Y_N$ less their first element $y_{i1}$ one on top of each other:

\begin{align*}
Y &= \left( Y'_1, Y'_2, \dots, Y'_{N} \right)'\\
 &= \left( y_{12}, y_{13},\dots, y_{1,m_1}, \dots, y_{N2}, y_{N3},\dots, y_{N,m_N} \right)'
\end{align*}
\noindent
Define $X_i$ to be the $m_i \times \vert V \vert$ matrix containing the covariates necessary for regressing each measurement $y_{i2}, \dots, y_{i,m_i}$ on its predecessors as in model~\ref{eq:cholesky-regression-model-2}, and stack these on top of one another to obtain

\begin{equation} \label{eq:ar-design-matrix-1}
X = \begin{bmatrix}
X_1 \\
X_2\\
\vdots \\
X_N
\end{bmatrix},
\end{equation}
\noindent
which has dimension $n_y \times \vert V \vert$. Then the solution $\phi$  minimizing \ref{eq:penalized-least-squares-2}  is the solution to the minimization problem

\begin{equation} \label{eq:ar-design-matrix-1}
\vert \vert D^{-1/2}\left( Y - X \left( Bd + Qc \right) \right) \vert \vert^2  + \lambda c^\prime Q c 
\end{equation}
\noindent
where the $\left(i,j\right)$ entry of the $\vert V \vert \times \vert V \vert$ matrix $Q$ is given by $\langle P_1 \xi_i,  P_1 \xi_j \rangle_\hilbert$, and $B$ is the $\vert V \vert \times d_0$ matrix with $i$-$\nu^{th}$ element $\eta_\nu\left(v_i\right)$, which we assume to be full column rank.  The diagonal matrix $D$ holds the $n_y \times n_y$  innovation variances $\sigma^2_{ijk}$. 

\bigskip

\begin{example}{Construction of $X_i$ with complete data} 
\\
\vspace{.5cm} 
\\
Straightforward construction of the autoregressive design matrix $X_i$ is straight forward in the case that there are an equal number of measurements on each subject at a common set of measurement times $t_1,\dots, t_M$. When complete data are available for measurement times $t_1, \dots, t_M$, 

\begin{equation}
X_i =  \begin{bmatrix} 
y_{i, t_1} & 0 & 0 &0&& \dots & 0 \\
 0 & y_{i, t_1} &  y_{i, t_2}&0 &0& \dots & 0 \\
 \vdots &&&&&&\\
 0 & \dots &0 & \dots& y_{i,t_1} & \dots &  y_{i, t_{M-1}}
\end{bmatrix}
\end{equation}
\noindent
for all $i = 1,\dots, N$. Note that this design matrix specification does not require that measurement times be regularly spaced.  
\end{example}

\begin{example}{Construction of $X_i$ with incomplete data}
\\
We demonstrate the construction of the autoregressive design matrices when subjects do not share a universal set of observation times for $N = 2$; the construction extends naturally for an arbitrary number of trajectories. Let subjects have corresponding sample sizes $m_1 = 4$, $m_2 = 4$, with measurements on subject 1 taken at $t_{11} = 0, t_{12} = 0.2, t_{13} = 0.5, t_{14} = 0.9$ and on subject 2 taken at $t_{21} = 0, t_{22} = 0.1, t_{23} = 0.5, t_{24} = 0.7$.  Then the unique within-subject pairs of observation times $\left(t,s\right)$ such that $0 \le s < t \le 1$ are

\begin{table}[H]
\centering
\begin{tabular}{l|r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r}
t & 0.1 & 0.2 & 0.5 & 0.5 & 0.5 & 0.7 & 0.7 & 0.7 & 0.9 & 0.9 & 0.9 \\ 
 s & 0.0 & 0.0 & 0.0 & 0.1 & 0.2 & 0.0 & 0.1 & 0.5 & 0.0 & 0.2 & 0.5 \\
\end{tabular}
\end{table}
\noindent
This gives that $V =  \left\{\bfv_{121},\dots, \bfv_{143}  \right\} \bigcup \left\{\bfv_{221},\dots, \bfv_{243}  \right\} = \left\{\bfv_1,\dots, \bfv_{11} \right\}$, where the distinct observed $v = \left(l, m\right)$ are 

\begin{table}[H]
\centering
\begin{tabular}{l|r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r}
l & 0.10 & 0.20 & 0.50 & 0.40 & 0.30 & 0.70 & 0.60 & 0.20 & 0.90 & 0.70 & 0.40 \\ 
  m & 0.05 & 0.10 & 0.25 & 0.30 & 0.35 & 0.35 & 0.40 & 0.60 & 0.45 & 0.55 & 0.70 \\ 
\end{tabular}
\end{table}
\noindent
Then a potential construction of the autoregressive design matrix for subject is given by:
\begin{equation}
X_1 =  \begin{bmatrix} 
0   & y_{1, 1}  &	0            &    0   &    0           & 0 & 0 & 0 & 0 & 0  \\
0   &	0  	      &	y_{1, 1}  &    0   & y_{1, 2}   &  0 & 0 & 0 & 0 & 0 \\
 0   &    0         & 0           &    0   &    0          & 0  & 0	&  y_{1, 1}    & y_{1, 2}& y_{1, 3} 
\end{bmatrix}
\end{equation}
\noindent
and similarly, for subject 2:

\begin{equation}
X_2 =  \begin{bmatrix} 
y_{2, 1}  & 	0  &	  0           &    0            &    0   & 0 & 0 & 0 & 0 & 0  \\
0   	      &  	0  &	y_{2, 1}  &    y_{2,2}   &    0   &  0 & 0 & 0 & 0 & 0 \\
 0   	      &        0  &    0           &    0            &  y_{2, 1}    & y_{2, 2}& y_{2, 3} &    0   & 0  & 0
\end{bmatrix}
\end{equation}
\end{example}

\subsubsection{Construction of the solution $\hat{\phi}$}

Differentiating $-2\ell_\phi + \lambda J\left(\phi\right)$ with respect to $c$ and $d$ and setting equal to zero, we have that 

\begin{align}
\frac{\partial}{\partial c}\left[-2\ell_\phi + \lambda J\left(\phi\right)\right] = Q X^\prime D^{-1}\left[ X\left(Bd + Qc\right) - Y  \right] + \lambda Qc &= 0 \nonumber \\
%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
\iff    X'D^{-1} X \bigg[ Bd + Qc \bigg] + \lambda c  &= X' D^{-1}Y \label{eq:normal-eq-1}
\end{align}

\begin{align}
\frac{\partial}{\partial d}\left[-2\ell_\phi + \lambda J\left(\phi\right)\right] = B^\prime X^\prime D^{-1}\left[ X\left(Bd + Qc\right) - Y  \right] &=0 \nonumber \\
%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
\iff   - \lambda B' c  &= 0  
\end{align}
\bigskip
\noindent
For fixed smoothing parameter, the solution $\phi$ is obtained by finding $c$ and $d$ which satisfy
\begin{align} 
Y &= X \bigg[ Bd + \left(Q  + \lambda \left(X^\prime D^{-1} X \right)^{-1} \right) c \bigg] \label{eq:ssanova-normal-eq-1} \\
B' c  &= 0  \label{eq:ssanova-normal-eq-2}
\end{align}
\noindent


Letting $\tildeY = D^{-1/2} Y$, $\tildeB = D^{-1/2} X B $, and $\tildeQ = D^{-1/2} X Q$, the penalized log likelihood \ref{eq:penalized-likelihood-vectorized} may be written

\begin{equation}\label{eq:penalized-loglik-tilde-vectorized}
-2\ell_\lambda \left(c, d \right) + \lambda J\left( \phi \right) = \bigg[ \tildeY - \tildeB d - \tildeQ c\bigg]'\bigg[ \tildeY - \tildeB d - \tildeQ c\bigg] + \lambda c'Qc.
\end{equation}
\noindent
Taking partial derivatives with respect to $d$ and $c$ and setting equal to zero yields normal equations 

\begin{align}
\begin{split}
\tildeB'\tildeB d + \tildeB'\tildeQ c &= \tildeB' \tildeY \\
\tildeQ'\tildeB d + \tildeQ'\tildeQ c + \lambda Q c &= \tildeQ' \tildeY, 
\end{split}
\end{align}

\noindent
Some algebra yields that this is equivalent to solving the system

\begin{equation} \label{eq:vectorized-normal-equations}
\begin{bmatrix}
\tildeB'\tildeB & \tildeB'\tildeQ \\
\tildeQ'\tildeB & \tildeQ'\tildeQ + \lambda Q\\
\end{bmatrix}
\begin{bmatrix}
d\\
c\\
\end{bmatrix}
= \begin{bmatrix}
\tildeB'\tildeY \\
 \tildeQ'\tildeY\\
\end{bmatrix}
\end{equation}


Fixing smoothing parameters $\lambda$ and $\theta_\beta$ (hidden in $Q$ and $\tildeQ$ if present), assuming that $\tildeQ$ is full column rank, \ref{eq:vectorized-normal-equations} can be solved by the Cholesky decomposition of the $\left( n + d_0 \right) \times \left( n + d_0 \right)$ matrix followed by forward and backward substitution. See \citet{golub2012matrix}. Singularity of $\tildeQ$ demands special consideration. Write the Cholesky decomposition

\begin{equation} \label{eq:normal-equation-cholesky}
\begin{bmatrix}
\tildeB'\tildeB & \tildeB'\tildeQ \\
\tildeQ'\tildeB & \tildeQ'\tildeQ + \lambda Q\\
\end{bmatrix}
= \begin{bmatrix}
C'_1 & 0 \\
C'_2  & C'_3 
\end{bmatrix}
\begin{bmatrix}
C_1 & C_2 \\
0  & C_3 
\end{bmatrix}
\end{equation}
\noindent
where $\tildeB'\tildeB = C'_1 C_1$, $C_2 = C_1^{-T} \tildeB' \tildeQ$, and $C'_3 C_3 = \lambda Q +  \tildeQ'\left( I - \tildeB\left( \tildeB' \tildeB \right)^{-1} \tildeB' \right)\tildeQ$. Using an exchange of indices known as pivoting, one may write 

\begin{equation*}
C_3 = \begin{bmatrix} H_1 & H_2 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} H \\  0 \end{bmatrix},
\end{equation*}
\noindent
where $H_1$ is nonsingular. Define
\begin{equation} \label{eq:cholesky-factor-mod}
\tilde{C}_3 = \begin{bmatrix}
H_1 & H_2 \\
0  & \delta I 
\end{bmatrix}, \;\;
\tilde{C} = \begin{bmatrix}
C_1 & C_2 \\
0  & \tilde{C}_3 
\end{bmatrix};
\end{equation}
\noindent
then
\begin{equation} \label{eq:cholesky-factor-mod-inverse}
\tilde{C}^{-1} = \begin{bmatrix}
C_1^{-1} & -C_1^{-1} C_2 \tilde{C}_3^{-1} \\
0  & \tilde{C}_3^{-1}
\end{bmatrix}.
\end{equation}

Premultiplying \ref{eq:normal-equation-cholesky} by $\tilde{C}^{-T}$, straightforward algebra gives 

\begin{equation} \label{eq:vectorized-normal-equations-cholesky}
\begin{bmatrix}
I & 0 \\
0 & \tilde{C}_3^{-T} C_3^{T} C_3 \tilde{C}_3^{-1}\\
\end{bmatrix}
\begin{bmatrix}
\tilde{d}\\
\tilde{c}\\
\end{bmatrix}
= \begin{bmatrix}
C_1^{-T} \tildeB'\tildeY \\
\tilde{C}_3^{-T} \tildeQ'\left( I - \tildeB\left( \tildeB' \tildeB \right)^{-1} \tildeB' \right) \tildeY\\
\end{bmatrix}
\end{equation}
\noindent
where $\left( \tilde{d}'\;\;\tilde{c}' \right)' =  \tilde{C}' \left( d\;\;c \right)'$. Partition $\tilde{C}_3 = \begin{bmatrix} K &  L\end{bmatrix}$; then $HK = I$ and $HL = 0$. So

\begin{align*}
\tilde{C}_3^{-T} C_3^{T} C_3 \tilde{C}_3^{-1} &= \begin{bmatrix} K' \\ L' \end{bmatrix} C'_3C_3 \begin{bmatrix} K &  L\end{bmatrix} \\
&= \begin{bmatrix} K' \\ L' \end{bmatrix} H'H \begin{bmatrix} K &  L\end{bmatrix} \\
&= \begin{bmatrix} I & 0 \\ 0 & 0 \end{bmatrix}.
\end{align*}
\noindent
If $L'C_3^{T} C_3 L = 0$, then $L'\tildeQ'\left( I - \tildeB\left( \tildeB' \tildeB \right)^{-1} \tildeB' \right)\tildeQ L = 0$, so $L'\tildeQ'\left( I - \tildeB\left( \tildeB' \tildeB \right)^{-1} \tildeB' \right) \tildeY = 0$. Thus, the linear system has form

\begin{equation} \label{eq:vectorized-normal-equations-cholesky-2}
\begin{bmatrix}
I & 0 & 0\\
0 & I & 0 \\
0 & 0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\tilde{d}\\
\tilde{c}_1\\
\tilde{c}_2
\end{bmatrix}
= \begin{bmatrix}
* \\
* \\
0
\end{bmatrix},
\end{equation}
\noindent
which can be solved, but with $c_2$ arbitrary. One may perform the Cholesky decomposition of \ref{eq:vectorized-normal-equations} with pivoting, replace the trailing $0$ with $\delta I$ for appropriate value of $\delta$, and proceed as if $\tildeQ$ were of full rank. 
\bigskip

It follows that

\begin{equation} \label{eq:tildeY-hat-equals-tildeA-tildeY}
\widehat{\tildeY} = \tildeB d + \tildeQ c = \begin{bmatrix} \tildeB & \tildeQ \end{bmatrix} \tilde{C}^{-1} \tilde{C}^{-T} \begin{bmatrix} \tildeB' \\ \tildeQ' \end{bmatrix} \tildeY = \tildeA\left(\lambda, \bftheta\right) \tildeY.
\end{equation} 
\noindent
where
\begin{align}
\begin{split} \label{eq:smoothing-matrix-A-tilde}
\tildeA\left(\lambda, \bftheta \right) =& \begin{bmatrix} \tildeB & \tildeQ \end{bmatrix} \tilde{C}^{-1} \tilde{C}^{-T} \begin{bmatrix} \tildeB' \\ \tildeQ' \end{bmatrix}  \\
&= G + \left(I - G\right) \tildeQ \left[\tildeQ'\left( I - G \right)\tildeQ + \lambda Q\right]^{-1} \tildeQ'\left(I - G\right),
\end{split}
\end{align} 
\noindent
for
\[
G = \tildeB\left(\tildeB' \tildeB \right)^{-1}\tildeB'.
\]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
TO DO: discuss efficient approximation of $\hilbert$ using a low rank smoother technique, as discussed in Gu's book at the end of Chapter 3. \citet{kim2004smoothing}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Smoothing parameter selection}
\subfile{chapter-2-subfiles/chapter-2-smoothing-spline-model-selection}
%\subfile{chapter-2-subfiles/chapter-2-smoothing-spline-solution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{An RKHS framework for estimating $\log \sigma^2$}
\subfile{chapter-2-subfiles/chapter-2-iv-smoothing-spline-representation}


Once we have an initial estimate of the generalized autoregressive coefficient function, $\phi$, we can use the model residuals to estimate the innovation variance function $\sigma^2\left(t\right)$. We use the same estimation approach as outlined in Section~\ref{RKHS-framework-for-phi}. Fixing $\phi = \phi^*$ for given estimate $\phi^*$, the negative log likelihood of the data $Y_1,\dots, Y_N$ is satisfies

\begin{equation} \label{eq:penalized-joint-loglik-given-phi-2}
-\ell\left( Y_1,\dots, Y_N, \phi, \sigma^2 \right) =  \frac{1}{2}\sum_{i = 1}^N \sum_{j = 1}^{m_i} \log \sigma^2_{ij}  + \frac{1}{2}\sum_{i = 1}^N \sum_{j = 1}^{m_i} \frac {\epsilon_{ij}^2}{\sigma^2_{ij}};
\end{equation}
\noindent
where $\epsilon_{ij} =  y_{ij} - \sum_{k<j} \phi^*_{ijk} y_{ik}$. Let 

\begin{equation}
\mbox{RSS}\left( t \right) = \sum_{i,j:t_{ij}= t} \left( y_{ij} - \sum_{k<j} \phi_{ijk} y_{ik}\right)^2
\end{equation}
\noindent
denote the squared residuals for the observations $y_{ij}$ having corresponding measurement time $t_{ij} - t$. Then $\mbox{RSS}\left( t \right)/\sigma^2\left(t\right) \sim \chi^2_{df_t}$, where the degrees of freedom $df_{t}$ corresponds to the number of observations $y_{ij}$ having corresponding measurement time $t$. In this light, for fixed $\phi$, the penalized likelihood \ref{eq:penalized-joint-loglik-given-phi-2} is that of a variance model with the $\epsilon_{ij}^2$ serving as the response.  This corresponds to a generalized linear model with gamma errors and known scale parameter equal to 2. Let $z_{ij} = \epsilon_{ij}^2$, and let $Z_{i} = \left(z_{i1},z_{i,m_i} \right)'$ denote the vector of residuals for the $i^{th}$ observed trajectory. The Gamma distribution is parameterized by shape parameter$\alpha$ and scale parameter $\beta$, where the mean of the distribution given by $\mu = \alpha \beta$. Reparameterizing the Gamma likelihood in terms of $\left(\alpha, \mu \right)$ and dropping terms that don't involve $\mu\left(\cdot\right)$ gives  
\begin{align}
-\ell\left(z,\mu, \alpha \right) &\propto \alpha\left[\frac{z}{\mu} + \log \mu\right]  \label{eq:gamma-iv-likelihood} \\ 
&= \alpha\left[ze^{-\eta} + \eta\right],\label{eq:gamma-iv-likelihood-canonical-link}
\end{align}
\noindent
where $\alpha^{-1}$ is the dispersion parameter and $\eta = \log \mu$. Letting $\mu_{ij}$ denote $E\left[ z_{ij} \right] = \sigma_{ij}^2$, the log likelihood of the working residuals becomes 

\begin{equation} \label{eq:penalized-joint-loglik-given-phi-3}
-\ell\left( Z_1,\dots, Z_N, \phi, \sigma^2 \right) =  \sum_{i = 1}^N \sum_{j = 1}^{m_i} \log \mu_{ij}  + \sum_{i = 1}^N \sum_{j = 1}^{m_i} \frac {z_{ij}}{\mu_{ij}},
\end{equation}
\noindent
which we can see coincides with a Gamma dsitribution with scale parameter $\alpha = 2$. Smoothing spline ANOVA models for exponential familes have been studied extensively (\citet{wahba1995smoothing}, \citet{wang1997grkpack}, \citet{gu2013smoothing}). Parallel to the penalized sums of squares for $\phi$ (\ref{eq:penalized-least-squares-2}), we can append a smoothness penalty to obtain the penalized likelihood for $\eta\left(t\right) = \log\sigma^2\left(t\right)$:

\begin{equation} \label{eq:penalized-joint-loglik-given-phi-3}
-\ell\left( Z_1,\dots, Z_N, \phi, \sigma^2 \right) + =  \sum_{i = 1}^N \sum_{j = 1}^{m_i} \eta_{ij}  + \sum_{i = 1}^N \sum_{j = 1}^{m_i} z_{ij} e^{-\eta_{ij}} + \lambda J\left(\eta\right),  
\end{equation}
noindent
for $\eta \in \hilbert = \oplus_{\beta = 0}^q \hilbert_\beta$, where the penalty $J$ can be written as a square norm and decomposed as in (\ref{eq:ssanova-decomposition-of-penalty}), with

\begin{equation*} 
J\left(\kappa \right) = \langle \eta,\eta \rangle = \sum_{\beta = 1}^q \theta_\beta^{-1}\langle \eta,\eta \rangle_{\beta}.
\end{equation*}
\noindent 
The $\langle \cdot, \cdot \rangle_{\beta}$ are inner products in $\hilbert_\beta$ having reproducing kernels $Q_\beta\left(t,t'\right)$. The penalty $J\left(\kappa\right)$ is an inner product in $\oplus_{\beta = 0}^q \hilbert_\beta$ with reproducing kernel $\sum_{\beta=1}^q \theta_\beta Q_\beta\left(t, t'\right)$ and null space $\mathcal{N}_J = \hilbert_0$. The first term in (\ref{eq:penalized-joint-loglik-given-phi-3}) serves as a measure of the goodness of fit of $\kappa$ to the data, and only depends on $\kappa$ through the evaluation functional $\left[t_{ij}\right]\kappa$. So the argument justifying the form of the minimizer in (\ref{eq:form-of-smoothing-spline-solution}) applies, and the minimizer of the penalized likelihood has the form 

\begin{equation} \label{eq:form-of-smoothing-spline-solution-kappa}
\eta\left( t \right) = \sum_{\nu = 1}^{d_0} d_\nu\kappa_\nu\left( t \right) + \sum_{i = 1}^{\vert \mathcal{T} \vert} c_i Q_J\left( t, t_i \right),
\end{equation}  
\noindent
where $\mathcal{T} = \bigcup_{j=1}^N\bigcup_{k=1}^{m_i} t_{jk}$ denotes the unique values of the observations times pooled across subjects, where $\left\{\kappa_\nu \right\}_{\nu=1}^{d_0}$ is a basis for the null space $\mathcal{N}_J = \hilbert_0$. 

\bigskip

Standard theory for exponential families gives us that the functional 

\begin{align}
\begin{split}
L\left( \eta \right) &= -\sum_{i=1}^N \sum_{j=1}^{m_i} \left[ z_{ij} \eta\left(t_{ij}\right) - b\left(\eta\left(t_{ij}\right)\right) \right] \\
&= -\sum_{i=1}^N \sum_{j=1}^{m_i} \left[ z_{ij} \eta\left(t_{ij}\right) - b\left(\eta\left(t_{ij}\right)\right) \right]
\end{split}
\end{align}
\noindent
is continuous and convex in $\eta \in \hilbert$. We assume that the $\vert V \vert \times d_0$ matrix $B$ which has $i$-$\nu^{th}$ element $\eta_\nu\left(v_i\right)$ is full column rank, so that $L\left(f\right)$ is strictly convex in $\hilbert$ and the minimizer of (\ref{eq:penalized-joint-loglik-given-phi-3}) uniquely exists. See \citet{wahba1995smoothing}. 

\bigskip

For fixed $\lambda$ and $\theta_\beta$, which may be hidden in $J$, the penalized log likelihood (\ref{eq:penalized-joint-loglik-given-phi-3}) is convex in $\eta$, so that the minimizer can be computed via Newton iteration. Let $u_{ij} = -z_{ij} + b'\left( \tilde{\eta}\left(t_{ij}\right) \right) =  -z_{ij} +  \tilde{\mu}\left(t_{ij}\right)$, and $\tilde{\omega}_{ij} = b''left( \tilde{\eta}\left(t_{ij}\right) \right) = \tilde{v}\left(t_{ij}\right)$. The quadratic approximation of $-z_{ij} \eta\left(t_{ij}\right) + b\left(\eta\left(t_{ij}\right)\right)$ at $\tilde{\eta}\left(t_{ij}\right)$ is given by 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Smoothing parameter selection for exponential familes}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subfile{chapter-2-subfiles/chapter-2-iv-smoothing-parameter-selection}

The gamma penalized log likelihood (\ref{eq:form-of-smoothing-spline-solution-kappa}) is non-quadratic, so $\eta_\lambda$ must be computed using iteration even for fixed smoothing parameter. Performance-oriented iteration and generalized approximate cross validation (GACV) are the most common approaches to selecting the smoothing parameter for penalized regression with exponential families. As in our discussion of model selection for $\phi$, we omit dependence of any components on the $\theta_\beta$ and only explicitly express dependence on smoothing parameters through $\lambda$.

\subsubsection{Performance-oriented iteration} 

A measure of the discrepancy between distributions belonging to an exponential family with density $p_Z\left(z\right) = exp\left\{\left(y \eta\left(t\right) - b\left(\eta\left(t\right)\right)\right)/a\left(\phi\right) + c\left(y,\phi\right) \right\}$ is the Kullback-Leibler distance

\begin{align}
\begin{split} \label{eq:kl-distance-definition}
\mbox{KL}\left(\eta, \eta_\lambda\right) &= E_\lambda\left[Z \left(\eta - \eta_\lambda \right) - \left(b\left(\eta\right)- b\left(\eta_\lambda\right) \right)\right]/a\left(\phi\right)\\
&=\left[ b'\left(\eta\right) \left(\eta - \eta_\lambda \right) - \left(b\left(\eta\right)- b\left(\eta_\lambda\right) \right)\right]/a\left(\phi\right),
\end{split}
\end{align}
\noindent
which simplifies to
\[
-\mu\left( e^{-\eta} - e^{-\tilde{\eta}}\right) - \left(\eta-\tilde{\eta}\right)
\]
\noindent
for the Gamma distribution. The KL distance is not symmetric, so sometimes people opt for its symmetrized version:

\begin{align}
\begin{split} \label{eq:skl-distance-definition}
\mbox{SKL}\left(\eta, \eta_\lambda\right) &= \mbox{KL}\left(\eta, \eta_\lambda\right) + \mbox{KL}\left(\eta_\lambda, \eta \right)\\
&= \left(b'\left(\eta\right) - b'\left(\eta_\lambda\right) \right)\left( \nu - \nu_\lambda\right)/a\left(\phi\right), \\
&= \left(\mu - \mu_\lambda \right)\left( \nu - \nu_\lambda\right)/a\left(\phi\right),
\end{split}
\end{align}
\noindent
A natural choice of loss function for measuring the performance of an estimator $\eta_\lambda\left(t\right)$ of $\eta \left(t\right)$ is the symmetrized Kullback-Leibler distance averaged over the observed time points $t_{11}, \dots , t_{1,m_1},\dots,t_{N1}, \dots , t_{N,m_N}$:

\begin{equation}\label{eq:SKL-loss-function}
L\left( \eta,\eta_\lambda \right) = \frac{1}{N}\sum_{i=1}^N \frac{1}{N}\sum_{j=1}^{m_i}  \left(\mu\left(t_{ij}\right) - \mu_\lambda \left(t_{ij}\right)\right)\left( \nu\left(t_{ij}\right) - \nu_\lambda\left(t_{ij}\right)\right),
\end{equation}
\noindent
which reduces to 
\begin{equation}\label{eq:gamma-SKL-loss-function}
L\left( \eta,\eta_\lambda \right) = \frac{1}{N}\sum_{i=1}^N \frac{1}{N}\sum_{j=1}^{m_i}  \left(\mu\left(t_{ij}\right) - \mu_\lambda \left(t_{ij}\right)\right)\left( \nu\left(t_{ij}\right) - \nu_\lambda\left(t_{ij}\right)\right),
\end{equation}
\noindent
for the Gamma distribution. The ideal smoothing parameters are those which minimize (\ref{eq:gamma-SKL-loss-function}). The performance-oriented iteration operates on a alternative expression of the symmetrized Kullback-Leibler loss. The mean value theorem gives us that (\ref{eq:gamma-SKL-loss-function}) can be written
\begin{equation}\label{eq:gamma-SKL-loss-function-mvt}
L_\omega\left( \eta,\eta_\lambda \right) = L\left( \eta,\eta_\lambda \right) = \frac{1}{N}\sum_{i=1}^N \frac{1}{N}\sum_{j=1}^{m_i} \omega^*\left(t_{ij}\right)  \left( \nu\left(t_{ij}\right) - \nu_\lambda\left(t_{ij}\right)\right)^2,
\end{equation}
\noindent
where $\omega^*\left(t_{ij}\right) = b''\left(\eta^*\left(t_{ij}\right)\right)$ and $\eta^*\left(t_{ij}\right)$ is a convex combination of  $\eta\left(t_{ij}\right)$ and $\eta_\lambda\left(t_{ij}\right)$. One can construct an unbiased risk estimate under the weighted loss, $L_\omega$, using re-weighted observations. Letting ${Z_{i}}_\omega = W_i Z_i$, where $W_i$ ist he $m_i \times m_i$ diagonal matrix having diagonal entries $\omega^*\left(t_{i1}\right), \dots, \omega^*\left(t_{i,m_i}\right)$, an unbiased estimate of relative loss is given by 

\begin{equation}\label{eq:weighted-unbiased-risk-estimate}
U_\omega\left( \lambda \right) = L\left( \eta,\eta_\lambda \right) = \frac{1}{N}\sum_{i=1}^N \frac{1}{N}\sum_{j=1}^{m_i} \omega^*\left(t_{ij}\right)  \left( \nu\left(t_{ij}\right) - \nu_\lambda\left(t_{ij}\right)\right)^2,
\end{equation}





%\bibliography{../Master}
%
%\end{document}
