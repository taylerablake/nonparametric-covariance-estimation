
\chapter{Concluding Remarks and Future Work}\label{concluding-remarks-chapter}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The previous discussion proposes a flexible framework for estimating the covariance matrix for longitudinal data. By modeling the Cholesky decomposition of the covariance matrix, we reframe covariance estimation as the estimation of a varying coefficient model, which allows for unconstrained estimation as well as a statistically intuitive interpretation of the elements of a covariance matrix. The varying coefficient model for the Cholesky decomposition naturally accommodates irregularly-spaced longitudinal data and allows varying within-subject sample sizes without requiring imputation of missing observations. The overall framework inherits the flexibility of the varying coefficient model, which allows us to leverage any of the tools classically used for nonparametric regression problems in the context of covariance estimation.

\bigskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Estimation of the varying coefficient model is performed using bivariate smoothing using penalties which are motivated by the prevalent tendency to specify stationary models for the covariance matrix. Penalties enforce regularization of the fitted function so that under heavy penalization, the fitted components of the Cholesky factor correspond to covariance matrices which are close to stationary. We demonstrate the estimation procedure with two proposed representations of the varying coefficient function and the innovation variance function. A smoothing spline ANOVA model for the generalized autoregressive varying coefficient and the innovation variance function allow the fitted functions to be decomposed into their stationary and nonstationary functional components. We propose an alternative functional representation for $\left(\phi, \log\sigma^2\right)$ using tensor product B-splines; smoothness is achieved by applying penalties to discrete differences of the vector of basis coefficients. The discrete penalties, which are constructed independently of the basis, offer flexibility over the smoothing spline penalties and require little computational complexity to implement. 

\bigskip
The choice of basis is important when the unknown functions parameterizing the varying coefficient model are better represented by one or the other. Simulation studies reveal the advantages and disadvantages of our smoothing spline estimator and our P-spline estimator. The simulations illustrate the relative performance of both estimators compared to alternative estimators proposed in the longitudinal data literature. 

\bigskip

We apply our method to data generated from a longitudinal experiment examining the effectiveness of two treatments for intestinal parasites in cattle as measured by subject body weight over time. For a single treatment group, our nonparametric estimator echoes some of the modeling assumptions made to specify parametric models in previous analysis of the same data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip

Minimizing computational demand is an obvious motivator for future extensions of our work. The smoothing spline estimator circumvents the need for knot selection since it is constructed using a basis function for each of the unique within-subject pairs of measurement times. This is suitable when there is a fixed set of measurement times and unbalanced date arise due to missing observations. For the case that there is little overlap in measurement times across subjects so that these times are ``nearly'' unique for each subject, the size of the set $\vert V \vert $ can be so large that the dimension of the kernel matrix $K_n$ as defined in (\ref{eq:penalized-likelihood-vectorized}) presents serious computational problems. An infinite dimensional Hilbert space is not necessarily required for representing the unknown function to be estimated, since the penalty effectively enforces a low dimensional model space. Efficient approximation can be carried by using a subset of the elements in $V$ to represent the unknown function. Algorithm~\ref{alg:SSANOVA-algorithm} can directly accommodate such a low dimensional representation. \cite{kim2004smoothing} provide detailed discussion of the efficient approximation of $\hilbert$.

\bigskip 

The versatility of this framework leaves many paths open for further modeling exploration. In particular, an obvious example is the classification of observations into one of $K$ groups by quadratic discriminant analysis. A lot of recent attention has been directed toward the problem of estimating separate $p\times p$ covariance matrices $\Sigma_1,\dots, \Sigma_K$ for $K$ separate groups, where the number of groups $K$ and the dimension $p$ are both potentially large. Often, there is not enough data to estimate separate $\Sigma_i$ well for each group. For example, problems in financial management including portfolio selection can be reduced to the prediction of a sequence of large $p \times p$ covariance matrices \citep{tsay2005analysis}. Our procedure to covariance estimation encourages exploration of this problem; the regression model associated with the Cholesky decomposition \eqref{eq:mcd-ar-model} can incorporate additional group-specific covariates. 

\bigskip

The construction of the penalty for the P-spline estimator is convenient and easily appended to the log likelihood. This allows us to easily use both shrinkage and smoothing for covariance estimation, and combining shrinkage and smoothing may produce better estimates than using shrinkage and smoothing alone. While adding additional penalty parameters can introduce added computational requirements, the connection between nonparametric regression modes and mixed models presents a way to mitigate this complexity. Smoothing parameters are interpreted as variances of random effects, so model estimation and smoothing parameter selection can be performed simultaneously using the stable and efficient algorithms and software that are available for mixed models. Restricted maximum likelihood (REML) has proven to be very useful as a model selection tool, often producing smoother fits than generalized cross validation due to its better resistance to over-fitting \citep{wand2003semiparametric}. 


\bigskip
Recently \cite{eilers1999discussion} pointed how to interpret P-splines as a mixed model. \cite{lee2011p} proposed the use of P-splines within a mixed modeling framework to estimate multidimensional functions which can be decomposed into their functional components as with smoothing spline ANOVA models. This approach adds attractiveness to the interpretability of the models, and it allows for computationally convenient model fitting and selection. Application of the mixed model framework presented in \cite{lee2011p} to estimation of $\phi$ is attractive, because it not only provides an avenue for stable smoothing parameter selection, but it also permits the decomposition of the tensor product into functional components as in the SSANOVA model presented in Chapter~\ref{SSANOVA-chapter}. Direct application of this approach, however, is inaccessible due to the deconstruction of the marginal B-spline bases to adjust for the triangular domain of the autoregressive varying coefficient. Figure~\ref{fig:triangle-domain} illustrates how to ``trim'' the pairs of B-splines that don't overlap with the domain of $\phi$, which lies on the triangle $0 < s < t< 1$. Trimming the basis inhibits identifiability of functional components, though in the case that an additive model is appropriate, this trimming is unnecessary and REML may be employed for model estimation.

\bigskip

Alternatively, bivariate B-splines inherit several of the appealing properties of univariate B-splines and are applicable in various modeling problems, particularly for those involving non-rectangular domains. They have been used extensively in the field of graphics for the construction of smooth surfaces over irregular domains, but thus have far received little attention in the field of statistics. However, a recent paper by \cite{zhou2014principal} employs a mixed effects model for the functional principal components as bivariate splines on triangulations for data observed on an irregular grid. The application of their ideas to covariance estimation presents a promising approach to estimation of the Cholesky decomposition via bivariate smoothing.  


