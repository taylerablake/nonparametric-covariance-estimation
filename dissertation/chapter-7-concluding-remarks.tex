
\chapter{Concluding remarks and future work}\label{concluding-remarks-chapter}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Our formulation of covariance estimation supplies a flexible framework which free of the impediment presented by the positive definite constraint and a statistically intuitive interpretation of the elements of a covariance matrix. Modeling the Cholesky decomposition rather than the covariance matrix itself allows us to reframe covariance estimation as a regression problem.We estimate the parameters of the corresponding regression model using bivariate smoothing, and in doing so, we naturally accommodate irregularly-spaced longitudinal data of varying within-subject sample sizes by without the need for data imputation methods. An attractive quality of the general estimation framework is that it permits the flexibility of specifying any choice of basis, so as to agree with the underlying surface (curve) to be estimated. 

\bigskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We demonstrate covariance estimation within this framework with two proposed representations of $\left(\phi, \log\sigma^2\right)$ the functional components of the Cholesky decomposition. We present a smoothing spline model for the generalized autoregressive varying coefficient and the innovation variance function and demonstrate how to specify penalties on the functional components of the functional ANOVA decompositions to enforce regularization of the fitted components to so that under heavy penalization, the fitted components correspond to commonly recruited structured covariance models. We demonstrate the fitting procedure using tensor product B-splines as an alternative basis expansion and impose regularization with discrete different penalties on the basis coefficients. The discrete penalties, which are constructed independently of the basis, offer flexibility over the smoothing spline penalties and require little computational complexity to implement. Simulation studies illustrate the relative performance of the estimators based on each basis compared to alternative estimators proposed in the longitudinal data literature. We apply our method to data generated from a longitudinal experiment examining the effectiveness of two treatments for intestinal parasites in cattle as measured by subject body weight over time. For a single treatment group, our nonparametric estimator echoes some of the modeling assumptions made to specify parametric models in previous analysis of the same data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip 


The versatility of the framework surrounding the two detailed approaches for estimating the Cholesky decomposition of a covariance matrix leaves a number of open questions pertaining to its use. More recently, a problem receiving attention is that of estimating separate $p\times p$ covariance matrices $\Sigma_1,\dots, \Sigma_k$ for $k$ groups, having covariance where $k$ and $p$ are potentially large. Often, there is not enough data to estimate separate $\Sigma_i$ well for each group.For example, many tasks in financial management including portfolio selection and risk management can be reduced to the prediction of a sequence of large $p \times p$ covariance matrices \citep{tsay2005analysis}. Our procedure to covariance estimation encourages exploration of this problem; the regression model associated with the Cholesky decomposition \eqref{eq:mcd-ar-model} can incorporate additional group-specific covariates. 

\bigskip

The smoothing spline estimator circumvents the need for knot selection since it is constructed using a basis function for each of the unique within-subject pairs of measurement times. This is suitable when there is a fixed set of measurement times and unbalanced date arise due to missing observations. For the case that there is little overlap in measurement times across subjects so that these times are ``nearly'' unique for each subject, the size of the set $\vert V \vert $ can be so large that the dimension of the kernel matrix $Q$ as defined in (\ref{eq:penalized-likelihood-vectorized}) presents serious computational problems. An infinite dimensional Hilbert space is not necessarily required for representing the unknown function to be estimated, since the penalty effectively enforces a low dimensional model space. Efficient approximation can be carrying out by using a subset of the elements in $V$ to represent the unknown function; Algorithm~\ref{alg:SSANOVA-algorithm} can directly accommodate such a low dimensional representation. \cite{kim2004smoothing} provides detailed discussion of the efficient approximation of $\hilbert$.

\bigskip

The construction of the penalty for the P-spline estimator is convenient and easily appended to the log likelihood, so introducing both shrinkage and smoothing to covariance estimation is trivial (though selecting multiple smoothing parameter is perhaps not). Combining shrinkage and smoothing may produce better estimates than using shrinkage and smoothing alone, so further investigation is a worthwhile endeavor. 

\bigskip

The connection between nonparametric regression modes and mixed models has long been established \citep{green1987penalized}, but the area has gained more interest with more recent developments in software for fitting mixed models. Most of the early work was based on smoothing splines, \cite{eilers1999discussion} pointed how to
interpret P-splines as a mixed model, prompting a number of works investigating P-splines as mixed models. \cite{lee2011p} proposed the use of P-splines within a mixed modeling framework to estimate multidimensional functions as a decomposition into smooth main effects and interactions. The smoothing parameters are interpreted as variances of random effects, so model estimation and smoothing parameter selection can be performed simultaneously using the stable and efficient algorithms and software that are available for mixed models. Restricted maximum likelihood (REML) has proven to be very useful as a model selection tool, often producing smoother fits than generalized cross validation due to its better resistance to over-fitting \citep{wand2003semiparametric}. 

\bigskip

Application of the mixed model framework presented in \cite{lee2011p} to estimation of $\phi$ is attractive, because it not only provides an avenue for stable smoothing parameter selection, it permits the decomposition of the tensor product into functional components as in the SSANOVA model presented in Chapter~\ref{SSANOVA-chapter}. Direct application of this approach, however, is inaccessible due to the deconstruction of the marginal B-spline bases to adjust for the triangular domain of the autoregressive varying coefficient. Figure~\ref{fig:triangle-domain} illustrates how to ``trim'' the pairs of B-splines that don't overlap with the domain of $\phi$, which lies on the triangle $0 < s < t< 1$. Trimming the basis inhibits identifiability of functional components, though in the case that an additive model is appropriate, this trimming is unnecessary and REML may be employed for model estimation.

\bigskip

Alternatively, bivariate B-splines inherit several of the appealing properties of univariate B-splines and are applicable in various modeling problems, particularly for those involving non-rectangular domains. They have been used extensively in the field of graphics for the construction of smooth surfaces over irregular domains, but thus have far received little attention in the field of statistics. However, a recent paper by \cite{zhou2014principal} employs a mixed effects model for the functional principal components as bivariate splines on triangulations for data observed on an irregular grid. The application of their ideas to covariance estimation presents a promising approach to estimation of the Cholesky decomposition via bivariate smoothing.  


