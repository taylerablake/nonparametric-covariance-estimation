%To satisfy the positive-definiteness constraint, methods have been developed and applied to certain reparameterizations of the covariance structure. Chiu, Leonard, and Tsui modeled the matrix logarithm of the covariance matrix. Early nonparametric work using the spectral decomposition of the covariance matrix included that of Rice and Silverman (1991) which discussed smoothing and smoothing parameter choice for eigenfunction estimation for regularly-spaced data. Staniswalis and Lee (1998) extended kernel-based smoothing of eigenfunctions to functional data observed on irregular grids. However, when the data are sparse in the sense that there are few repeated within-subject measurements and measurement times are quite different from subject-to-subject, approximation of the functional principal component scores defined by the Karhunen-Loeve expansion of the stochastic process by usual integration is unsatisfactory and requires numerical quadrature. Many have explored regression-based approaches using the Spectral decomposition, framing principal components analysis as a least-squares optimization problem. Among many others, Zou, Hastie and Tibshirani (2006) imposed penalties on regression coefficients to induce sparse loadings. {\needsparaphrased[REVIEW THE METHODS OF HUANG, KAUFMAN, YAO HERE]}
%\bigskip

The log link resolves the issued presented by the constrained parameter space associated with the identity link, leading to unconstrained parameterization of a covariance matrix. However, the parameters of the matrix logarithm lack any meaningful statistical interpretation. The hybrid link  constructed from the modified Cholesky decomposition of $\Sigma^{-1}$ given in \ref{eq:cholesky-decompostion-link-function} combines ideas in \citet{edgeworth1892xxii}, \citet{gabriel1962ante}, \citet{anderson1973asymptotically}, \citet{dempster1972covariance}, \citet{chiu1996matrix}, and \citet{zimmerman1997structured}. It leads to unconstrained and statistically meaningful reparameterization of the covariance matrix so that the ensuing GLM overcomes most of the shortcomings of the linear and log-linear models.  For an unstructured covariance matrix $\Sigma$, the nonredundant entries of the components $\left(T, \log D\right)$ of the modified Cholesky decompostion~\ref{eq:modified-cholesky-decomposition} can be written as the entries of 

\begin{equation}\label{eq:cholesky-decompostion-link-function}
g\left( \Sigma \right) = 2I - T - T' + \log D.
\end{equation}
\noindent
These entries are unconstrained, allowing them to be modeled using any desired technique, including parametric, semi- and nonparametric, and Bayesian approaches. Covariates can be included in said models seamlessly. As in the usual GLM setting for estimation of the mean, one can elicit parametric models for $\phi_{tj}$ and $\log\sigma_t^2$.  For example, one might model the nonredundant entries of $T$, say, linearly as in model~\ref{eq:linear-covariance-model} and those of $\log D$ as in, say, model~\ref{eq:log-linear-covariance-model}, letting

\begin{align}
\begin{split} \label{eq:linear-models-for-GARPs-IVs}
\phi_{tj} &= x'_{tj} \beta,\\
\log\sigma_t^2 &= z'_t \gamma,
\end{split}
\end{align}
\noindent
where $x_{tj}$ and $z_{t}$ denote $q \times 1$ and $p \times 1$ vectors of known covariates, and $\beta = \left(\beta_1,\dots, \beta_q \right)'$ and $\gamma = \left(\gamma_1,\dots, \gamma_p \right)'$ are the parameters relating these covariates to the innovation variances and the dependence among the elements of $Y$. Covariates most frequently used in the analysis of real longitudinal data sets are low order polynomials of lag and time, modeling

\begin{align}
\begin{split}  \label{eq:GARP-IV-parametric-model}
z'_{jk} &= \left(1, t_j - t_k, \left(t_j - t_k\right)^2,\dots, \left(t_j - t_k\right)^{p-1}\right)' \\
z'_{i}  &= \left(1, t, \dots, t^{q-1}\right)'
\end{split}
\end{align}
\noindent
Modeling the covariance according to \ref{eq:linear-models-for-GARPs-IVs} and \ref{eq:GARP-IV-parametric-model} reduces the constrained and potentially high dimensional parameter space for $\Sigma$ to an unconstrained parameter space of dimension $q + p$.

\bigskip

\citet{pourahmadi1999joint}, \citet{pourahmadi2000maximum}, and \citet{pan2006regression} prescribe methods for identifying models such as model~\ref{eq:linear-models-for-GARPs-IVs} using model selection criteria, such as AIC, and regressograms, which are a nonstationary analogue of the correlelogram one typically encounters in the time series literature. \citet{pan2003modelling} jointly estimate the mean and covariance of longitudinal data using maximum likelihood, iterating between estimation of the mean vector $\mu$, the log innovation variances $\log \sigma_{ij}^2$, and the generalized autoregressive parameters $\phi_{ij}$. Score functions can be computed  by direct differentiation of the normal log likelihood, and optimization is achieved by solving these via iterative quasi-Newton method. 


\bigskip


\begin{table}\centering
\caption{Ideal shape of repeated measurements.}
\begin{tabular}{cc|cccccc}
\multicolumn{8}{c}{Occasion}\\
& & $1$&$2$ &  $\dots$ & $t$ & $\dots$ & $m$ \\ \midrule
& 1 & $y_{11}$&$y_{12}$ &$\dots$ & $y_{1t}$ & $\dots$& $y_{1m}$ \\
& 2 & $y_{21}$&$y_{22}$ &$\dots$ & $y_{2t}$ & $\dots$& $y_{2m}$ \\
\begin{rotate}{90}%
\mbox{Unit}\end{rotate} & $\vdots$ &$\vdots$&$\vdots$ & &$\vdots$ & & $\vdots$ \\
& $i$ & $y_{i1}$&$y_{i2}$ &$\dots$ & $y_{it}$ & $\dots$& $y_{im}$ \\
 & $\vdots$ &$\vdots$&$\vdots$ & &$\vdots$ & & $\vdots$ \\
 & $N$ & $y_{N1}$&$y_{N2}$ &$\dots$ & $y_{Nt}$ & $\dots$& $y_{Nm}$ \\
\end{tabular}
\end{table}


\begin{table}\centering
\caption{Autoregressive coefficients and prediction error variances of successive regressions.}
\begin{tabular}{cccccc}
 $y_{1}$&$y_{2}$ & $y_{3}$ & $\dots$ &$y_{m-1}$& $y_{m}$\\ \midrule
 $1$& &&&&\\
$\phi_{21}$& 1 &&&& \\
$\phi_{31}$& $\phi_{32}$& 1 &&& \\ 
$\vdots$ & $\vdots$ & & $\ddots$&& \\
$\vdots$ & $\vdots$ & && $\ddots$& \\
$\phi_{m1}$& $\phi_{m2}$&$\dots$ &$\dots$ &$\phi_{m,m-1}$ & 1\\ \midrule
$\sigma_1^2$ & $\sigma_1^2$ & $\dots$&$\dots$ &$\sigma_{m-1}^2$ &$\sigma_m^2$
\end{tabular}
\end{table}

\begin{align}
\begin{bmatrix}
1&&&&\\
-\phi_{21}&1&&&\\
-\phi_{31}&-\phi_{32}&1&&\\
\vdots &&&\ddots& \\
-\phi_{m1}&-\phi_{m2}& \dots & -\phi_{m,m-1}&1\\
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\ \ddots \\ y_m
\end{bmatrix} = \begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\ \ddots \\ \epsilon_m
\end{bmatrix}
\end{align}


In most longitudinal studies, the functional trajectories of the involved smooth random processes are not directly observable. Often, the observed data are noisy, sparse and irregularly spaced measurements of these trajectories. Removing the restriction that subjects having common covariance structure also share common, equally-spaced observation times encourages the interpretation of vectors $Y_i$ and $\epsilon_i = \left(\epsilon_{i1}, \dots, \epsilon_{i, m_i} \right)'$ as continuous time processes $Y\left(t\right)$, $\epsilon\left(t\right)$ observed at discrete measurement times $t_1,\dots, t_{m_i}$. Using a likelihood-based estimation approach alongside a functional interpretation of the GARPs permits a natural way to regularize the estimator while making minimal assumption about the form of the dependency structure itself. The fact that the entries of $T$ are unconstrained makes the Cholesky decomposition ideal for nonparametric estimation. 

\bigskip
Using penalized maximum likelihood, \citet{levina2008sparse} impose a banded structure on the Cholesky factor, and select the bandwidth adaptively for each row of the Cholesky factor, using a novel penalty we call nested Lasso.
This structure has more flexibility than regular banding, but, unlike regular Lasso applied to the entries of the Cholesky factor, results in a sparse estimator for the inverse of the covariance matrix. A
\bigskip

Several others have proposed modeling the GARPs $\phi_{ij}$ and IVs $\sigma_j$ as smooth functions, using $M-1$ univariate functions $\phi_l\left(t\right)$, $l = 1,\dots, M-1$ to approximate the subdiagonals of $T$. \citet{wu2003nonparametric}, \citet{huang2006covariance} propose models based on this approach, each resulting in a variant of banded estimator of $T$. \citet{wu2003nonparametric} proposed a two-step estimation procedure using nonparametric smoothing for regularized estimation of large covariance matrices.  In the first step, they derive a raw estimate of the covariance matrix and the estimated covariance matrix is subject to the modified Cholesky decomposition. In the second step, they apply local polynomial smoothing to the diagonal elements of $D$ and the subdiagonals of $T$. Their procedure is not capable of handling missing or irregular data. \citet{huang2007estimation} jointly model the mean and covariance matrix of longitudinal data using basis function expansions. They treat the subdiagonals of $T$ as smooth functions, approximated by B-splines and carry out estimation maximum (normal) likelihood. Their method permits subject-specific observations times, but assumes that observation times lie on some notion of a regular grid. They treat within-subject gaps in measurements as missing data and which they handle using the E-M algorithm. Regularization is achieved through the choice of $k$, the number of nonzero subdiagonals, and the total number of basis functions used to approximate the $k$ smoothed diagonals. They treat these as tuning parameters and use BIC for model selection. 

\bigskip

Denoting estimators of $T$ and $D$ in \ref{eq:modified-cholesky-decomposition} by $\hat{T}$ and $\hat{D}$, we can construct an estimator of $\Sigma$ 

\begin{equation}
\hat{T}^{-1}\hat{D}\hat{T}'^{-1}
\end{equation}
\noindent
is guaranteed to be positive-definite. Although one could smooth rows and columns of $T$, or the whole T viewed as a bivariate function, the idea of smoothing along its subdiagonals is motivated by the similarity of the regressions in (3) to the varying-coefficients autoregressions (Kitagawa and Gersch, 1985, 1996; Dahlhaus, 1997):


%  \citet{pourahmadi1999joint}, \citet{pan2006regression}, \citet{pourahmadi2002dynamic}. In the spirit of the GLM, they model the components of the Cholesky decomposition using covariates, modeling the GARPs and IVs parametrically as polynomials of lag and time, respectively:



