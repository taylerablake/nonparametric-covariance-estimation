\documentclass[../chapter-1-introduction.tex]{subfiles}
\begin{document}


\subsection{Structured parametric covariances} \label{parametric-covariance-models}


In the applied statistics literature, particularly for repeated measure data, it is quite common to pick a stationary covariance matrix for the covariance structure. Typical choices are simple models which depend on a small number of parameters such as compound symmetry and autoregressive models of order $k$, where $k$ is small.We will review a selection of modeled frequently encountered in the applied statistics literature in sections to follow. This approach is attractive because it is computationally inexpensive, and software packages implementing fitting procedures for a growing number of simple models are readily accessible. The compound symmetric model was at one time a very popular choice for parametric covariance structure, specifying

\begin{equation}\label{eq:compound-symmetric-model}
\sigma_{ij} = \left\{ \begin{array}{lr}
\rho, & i \ne j,\\
\sigma^2, & i = j, 
\end{array}\right.
\end{equation}
\noindent
where $\sigma_{ij}$ denotes the $\left(i,j\right)$ element of $\Sigma$. With only two parameters to be estimated, this model is highly parsimonious, but has received less attention with the development of models that allow for heterogeneous variances and non-constant correlation. 

\bigskip

The first order autoregressive model for response variable $y_t$ associated with measurement time $t$ specifies
\begin{equation}\label{eq:ar-1-model}
y_{t} = \left\{ \begin{array}{lr}
\mu_t + \epsilon_t, & t = 1,\\
& \\
\mu_t + \rho\left(y_{t-1} - \mu_{t-1}\right) + \epsilon_t, & t > 1,
\end{array}\right.
\end{equation}
\bigskip
\noindent 
where $\vert \rho \vert < 1$, and the innovations $\left\{\epsilon_t\right\}$ are independently distributed according to $N\left(0,\sigma_t^2\right)$ with $\sigma_1^2 = \sigma^2/\left(1-\rho^2\right)$, and $\sigma_t^2 = \sigma^2$ for $t = 2, \dots, M$. The corresponding dependence components of the covariance structure are monotonically decreasing in $l = \vert i-j \vert$; specifically,

\begin{equation}\label{eq:compound-symmetric-model}
\sigma_{ij} = \left\{ \begin{array}{lr}
\rho^{\vert i - j \vert}, & i \ne j,\\
& \\
\sigma^2, & i = j, 
\end{array}\right.
\end{equation}
\bigskip
\noindent
The AR(1) model generalizes to any arbitrary order $p$ by simply adding additional predecessors to the covariates in the linear model for $y_t$:
\begin{equation*}
y_{t} = \left\{ \begin{array}{lr}
\mu_t + \epsilon_t, & t = 1,\\
& \\
\mu_t + \sum\limits_{j = 1}^{p^*} \phi_j\left(y_{t-j} - \mu_{t-j}\right) + \epsilon_t, & t > 1,
\end{array}\right.
\end{equation*}
\noindent
where $p^* = \min\left(p,t-1\right)$, and the $\left\{\epsilon_t\right\}$ are independent mean zero Normal random variables. The variance of $\left\{\epsilon_t\right\}$ is constant for $t > p$, and for $t \le p$, the variance is specified so as to ensure that the variance is constant across all responses $y_t$ and the covariance between $y_i$ and $y_j$ depends only on $\vert i - j\vert$. 

\bigskip

The response specification for $q^{th}$ order moving average model  is given by 

\begin{equation}\label{eq:ma-q-model}
y_{t} = \sum_{j = 0}^{q} \theta_j \epsilon_{t-j},
\end{equation}
\bigskip
\noindent
where the $\left\{\epsilon_t\right\}$ are independently and identically distributed mean zero Normal random variables with variance $\sigma^2$. This model corresponds to covariance structures with elements given by
\begin{equation*}
\sigma_{ij} = \left\{ \begin{array}{ll}
\left(\theta_{i-j} + \theta_{1}\theta_{i-j +1} + \dots + \theta_{q-i+j}\theta_{q}\right)/\left(1 + \sum_{j = 1}^q \theta_j^2\right), & \vert i-j\vert \le q,\\ 
& \\
& \\
0, &  \vert i-j\vert > q, \\
& \\
\sigma^2 \sum\limits_{j = 0}^q \theta_j^2, & i = j,\\
\end{array}\right.
\end{equation*}
\bigskip
\noindent
Thus, variances are constant and correlations between $y_t$ and $y_{t-l}$ vanish beyond a finite, constant lag $l$. Here $\rho_1,\dots, \rho_q$are arbitrary parameters subject only to positive definiteness constraints. This model generalizes to a $q^{th}$-order Toeplitz model, which specifies
\begin{equation} \label{eq:toeplitz-covariance-model}
\sigma_{ij} = \left\{ \begin{array}{ll}
\rho_{i-j} & \vert i - j \vert\le q, \\ 
&\\
0 & \vert i - j \vert >  q, \\ 
& \\
\sigma^2  & i = j,\\
\end{array}\right.
\end{equation}
or covariance matrix of the form
\begin{equation} \label{eq:toeplitz-covariance-matrix}
M = \begin{bmatrix} m_0 & m_1 & m_2 & \dots & m_{p-1}\\ m_1 & m_0 & m_1 & \dots & m_{p-2}\\m_2 & m_1 & m_0 & \dots & m_{p-3}\\ \vdots & \vdots & \vdots & \ddots & \vdots\\  m_{p-1} & m_{p-2} & m_{p-3} & \dots & m_0 \end{bmatrix}, 
\end{equation}
\noindent
where $m_j = 0$ for all $j > q$.


\bigskip


In turn, one can further generalize to a $q^{th}$-order banded model by specifying that the covariances on off-diagonals of the correlation matrix beyond the $q^{th}$ off-diagonal are zero, and otherwise  not imposing any structural restrictions on the remaining elements of the covariance matrix beyond those required for positive definiteness. The tradeoff of the additional flexibility of the general banded model over the MA and Toeplitz models is that the number of parameters in a general $q$-banded covariance structure is $O\left(n\right)$ rather than $O\left(1\right)$.

\bigskip

The aforementioned models are stationary, specifying constant variance and with equal same-lag correlations among responses when the data are observed on a regular grid. Heterogeneous extensions of these models specify the same form of the correlation but allow time-dependent response variances. Completely general time dependence (subject to positive definiteness constraints) requires the covariance structure to be characterized by $O\left(n\right)$ parameters, while specifying linear or quadratic dependence on time leads to more parsimonious heterogeneous models. 

\bigskip

An ARIMA($p,d,q$) model generalizes a stationary autoregressive moving average (ARMA) model by postulating that not the observations themselves, but rather the $d^{th}$-order differences among consecutive measurements follow a stationary ARMA($p,q$) model. A special case is the ARIMA($0,1,0$) model - the random walk:

\begin{equation}
y_t = \mu_t + \sum_{j = 1}^t \epsilon_j, \quad t = 1, \dots, M,
\end{equation}
\noindent
where the $\epsilon_t$ are independent mean zero Normal random variables with variance $\sigma_\epsilon^2$. The variance of the process increases linearly in time, and the correlation between $y_t$ and $y_{t-l}$
 also increases, but nonlinearly, in time:
 
\begin{equation}
\sigma_{ij} = \left\{ \begin{array}{ll}
\sqrt{i/j} &i \ne j \\ 
& \\
j\sigma_\epsilon^2 & i= j, \\
\end{array}\right.
\end{equation}
\noindent
This model is applicable to longitudinal data only when data are observed on a regular grid, however, its continuous time analogue permits this restriction to be relaxed. An important special case is the continuous time analogue to the random walk, the Weiner process, which has covariance function $Cov\left(y\left(t_i\right), y\left(t_j\right)\right) = \sigma^2 \min\left(t_i, t_j\right)$.

\bigskip

Random coefficient models are a broad class of models often used for clustered or longitudinal data. They offer reasonable flexibility for characterizing dependency structure but remain parsimonious because the number of model parameters is unrelated to the number of repeated measurements and can be applied to non-rectangular data.  The formulation of the covariance structure for these models is most usually a consideration of regressions that vary across subjects rather than a consideration of within-subject similarity, which is why they are most often considered distinct from parametric covariance models. Still, they yield parametric covariance structures that generally have non-constant variances and non-stationary correlations.  A general form of the random coefficient model is given by 

\begin{equation}
y_i = X_i\beta + Z_i \gamma_i + \epsilon_i, \quad i = 1, \dots, M,
\end{equation}
\noindent
where the $Z_i$ are specified matrices, the $\gamma_i$ are vectors of random coefficients distributed independently as $N \left(0, G_i\right)$, the $G_i$ are positive definite but otherwise unstructured matrices, and the $\epsilon_i$ are distributed independently (of the $\gamma_i$ and of each other) as $N \left(0, \sigma^2 \mathrm{I}_{n_i}\right)$. The $G_i$ are usually assumed to be equal, so the covariance matrix of $y_i$ is taken to be $\Sigma_i = Z_i GZ'_i + \sigma^2 \mathrm{I}_{n_i}$. Special cases include the linear random coefficients (RCL) and quadratic random coefficients (RCQ) models. In the linear case, $Z_i = \left[1_{m_i} , \left(t_{i1},\dots,t_{i, m_i}\right)'\right]$ and 

\begin{equation*}
G = \begin{bmatrix}
\sigma_{00} & \sigma_{01} \\
\sigma_{10} & \sigma_{11} 
\end{bmatrix}
\end{equation*}
\noindent
In the quadratic case, $Z_i =\begin{bmatrix}1_{m_i}, \left(t_{i1}, \dots, t_{i,m_i}\right)', \left(t^2_{i1}, \dots, t^2_{i,m_i}\right)'\end{bmatrix}$. It is worth noting that when $Z_i = 1_{m_i}$, the random coefficient model corresponds to the compound symmetric model \ref{eq:compound-symmetric-model}. The covariance structure for a subject having measurements $y_1,\dots, y_{m_i}$ taken at equally spaced measurement times $t_1 = 1, \dots,t_{m_i} = m_i$ is given by  


\begin{equation}
\sigma_{ij} = \left\{ \begin{array}{ll}
\frac{\sigma_{00} + \sigma_{01}\left(i + j\right) + \sigma_{11} ij}{\sqrt{\sigma^2 + \sigma_{00} + 2i\sigma_{01} + \sigma_{11}i^2\sqrt{\sigma^2 + \sigma_{00} + 2j\sigma_{01} +j^2\sigma_{11}} }} &  i \ne j \\ 
& \\
\sigma^2 + \sigma_{00} + 2\sigma_{01}j + \sigma_{11}j^2 &  i= j, \\
\end{array}\right.
\end{equation}
\newline

These models are can permit variance and covariances which exhibit several kinds time dependency, including increasing or decreasing variances and correlations of which some are negative while others are positive. However, this model does not permit variances which are concave-down in time, and it precludes the variances from being constant if the same-lag correlations are different.

\bigskip

The previous list is far from an exhaustive list of parametric covariance structures - we will later reference structures which we have not discussed here, such as antedependence models. For example, see \citet{jennrich1986unbalanced} for additional models for repeated measures data. While these models are computationally attractive and the choices for parametric model structure are seemingly unlimited, specifying the appropriate parametric covariance structure is a challenge even for the experts, and model misspecification can lead to considerably biased estimates. To strike a balance between the variability of the sample covariance matrix and the bias of the estimated structured covariance matrix, it is prudent to rely on the data to formulate structures for the unknown underlying dependence in the data.
\end{document}
