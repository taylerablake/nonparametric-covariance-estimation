%The smoothing matrix $\tildeA$ plays an integral role of calculating all of the model selection criteria we will discuss in the sections to follow;  the diagonal elements of $\tildeA$, $\tilde{a}_{kk}$ are of particular importance in quantifying  model complexity. In classical regression theory, the degrees of freedom are clearly defined as the number of variables included in the model. \citet{eilers1996flexible} and\citet{marx2005multidimensional} refer to this measure of model complexity as the model's \emph{effective dimension} ED; they follow \citet{hastie1990generalized}, who discuss the effective dimensions of linear smoother and propose to use the trace of the smoother matrix as an approximation. 

\bigskip

By varying smoothing parameters $\lambda$ and $\theta_\beta$, the minimizer $\phi_\lambda$ of \ref{eq:vectorized-normal-equations} defines a family of potential estimates. In practice, we need to choose a specific estimate from the family, which requires effective methods for smoothing parameter selection. We consider two criteria that are commonly used for smoothing parameter selection in the context of smoothing spline models for longitudinal data. The first score is an unbiased estimate of a relative loss and assumes a known variances $\sigma_t^2$. The unbiased risk estimate has attractive asymptotic properties; see \citet{gu2013smoothing} for a comprehensive examination. The second score, the leave-one-subject-out cross validation (losoCV) score, provides an estimate of the same loss without assuming a known variance function. We review a computationally convenient approximation of the losoCV score proposed by \citet{xu2012asymptotic}, who demonstrates the shortcut score's asymptotic optimality. To simplify notation for the initial presentation, we only make explicit the dependence of estimates and their components on $\lambda$ and conceal any dependence on $\theta_\beta$. 

\subsection{Model selection criteria}
\subsubsection{Unbiased risk estimate}

Define  $\tildeY = D^{-1/2} Y$, $\tildeB = D^{-1/2} X B $, and $\tildeQ = D^{-1/2} X Q$ as before. Let $\tildeepsilon = D^{-1/2} \epsilon$ denote the vector of length  $\sum_{i = 1}^Nm_i - N$ containing the standardized prediction errors $\epsilon_{ij} \sim N\left(0,1\right)$, and write the vector of transformed means 

\begin{equation} 
\Phi = D^{-1/2} X \left[ Bd + Qc \right].
\end{equation}
\noindent
We can assess $\hat{\tildeY}_\lambda$, an estimate of the mean of $\tildeY$ based on observed data $y_{ij}$, $i = 1,\dots, N$, $j = 1,\dots, m_i$, using the loss function

\begin{align}
\begin{split}
L\left(\lambda\right) &= \sum_{i = 1}^N \sum_{j = 1}^{m_i} \left(\hat{\tildey}_{ij} - E\left[\tildey_{ij}\right] \right)^2\\
&= \vert \vert \tildeY - \tilde{\mu} \vert \vert^2
\end{split}
\end{align}
\noindent
where $\mu = D^{-1/2}W \Phi^*$ denotes the $\left( \sum \limits_{i} m_i - N\right) \times 1$ with $i^{th}$ element equal to the expected value of the  $i^{th}$ element of $\tildeY$.  Then straightforward algebra yields that 

\begin{align} 
L\left(\lambda\right) = \mu'\left( I - \tildeA \right)^2\mu - 2\mu'\left( I - \tildeA \right)^2 \tildeA \tildeepsilon + \tildeepsilon' \tildeA^2 \tildeepsilon
\end{align}

Define the unbiased risk estimate
\begin{equation} 
U\left(\lambda\right) = \frac{1}{N}\tildeY'\left( I - \tildeA \right)^2\tildeY + \frac{2}{N}\mbox{tr}\tildeA
\end{equation}
 \noindent
Adding and substracting $\mu$ to the quadratic terms, one can verify with straightforward algebra that

\begin{align}
\begin{split}
U\left(\lambda\right) &= \left( \tildeY - \mu + \mu - \tildeA \tildeY \right)'\left( \tildeY - \mu + \mu - \tildeA \tildeY \right) + 2\mbox{tr}\tildeA \\
&= \left(\tildeA \tildeY - \mu \right)'\left( \tildeA \tildeY - \mu \right) + \tildeepsilon'\tildeepsilon + 2\tildeepsilon' \left( I- \tildeA\right)\mu- 2\left( \tildeepsilon'\tildeA \tildeepsilon -  \mbox{tr}\tildeA\right)
\end{split}
\end{align}
\noindent
This gives
\begin{equation} 
U\left(\lambda\right) - L\left(\lambda\right) - \tildeepsilon'\tildeepsilon  =  2\tildeepsilon' \left( I- \tildeA\right)\mu- 2\left( \tildeepsilon'\tildeA \tildeepsilon -  \mbox{tr}\tildeA\right), 
\end{equation}
 \noindent
 which allows one to easily see that $U\left(\lambda\right)$ is unbiased for the relative loss $L\left(\lambda\right) + \tildeepsilon'\tildeepsilon$.  Under mild conditions on the risk function
 
 \[
 R\left(\lambda\right) = E\left[L\left(\lambda\right)\right],
 \]
\noindent
one can establish that $U$ is also a consistent estimator. See \citet{gu2013smoothing} Chapter 3 for a formal theorem and proof.


\subsection{Leave-one-subject-out cross validation}  
The conditions under which the the cross validation and generalized cross validation scores traditionally used for smoothing parameter selection yield desirable properties generally do not hold when the data are clustered or longitudinal in nature. Instead, the leave-one-subject-out (LosoCV) cross validation score has been widely used for smoothing parameter selection for semiparametric and nonparametric models for longitudinal or functional data. The LosoCV criterion is defined as

\begin{equation} \label{eq:LOSOCV}
V_{loso}\left(\lambda\right) = \frac{1}{N}\sum_{i=1}^N \left( \tildeY_i - \widehat{\tilde{\mu}}^{\left[-i\right]}_{i}\right)'\left( \tildeY_i -  \widehat{\tilde{\mu}}^{\left[-i\right]}_{i}\right)
\end{equation}
\noindent
where $\widehat{\tilde{\mu}}^{\left[-i\right]}_{i}$ is the estimate of $E\left[ \tildeY_i \right]$ based on the data when $\tildeY_i$ is omitted. Intuitively, the LosoCV score is appealing because it preserves any within-subject dependence by leaving out all observations from the same subject together in the cross-validation.  However, despite its prevalent use, theoretical justifications for its use have not been established. In their seminal work, \citet{rice1991estimating} were the first to present a heuristic justification of LosoCV by demonstrating that it mimics the mean squared prediction error: consider new observations $\tildeY^*_i = \left(\tilde{y}_{i1}^*, \tilde{y}_{i1}^*, \dots, \tilde{y}_{i, m_i}^*\right)$. We may write the mean squared prediction error for the new observations as follows:  
\bigskip 

\begin{align}
\begin{split}\label{eq:MSPE}
MSPE &= \frac{1}{N}\sum_{i=1}^N E\left[ \vert \vert \tildeY^*_i - \widehat{\tilde{\mu}}_{i} \vert \vert^2 \right]\\
&=  \frac{1}{N}\sum_{i=1}^N E\left[ \vert \vert \tildeY^*_i - D_i^{-1/2}W_i \Phi^* + D_i^{-1/2}W_i \Phi^* - D_i^{-1/2}W_i \hat{\Phi}^*\vert \vert^2 \right]\\
&=  \frac{1}{N}\sum_{i=1}^N \left\{m_i + E\left[ \vert \vert \tilde{\mu}_{i} - \widehat{\tilde{\mu}}^{\left[ -i \right]}_{i} \vert \vert^2 \right] \right\}
\end{split}
\end{align}
\noindent
where $\tilde{\epsilon}_i = \tildeY^*_i - D_i^{-1/2}W_i \Phi^*$. When $\left\{ \sigma^2\left(t\right)\right\}$ is known, $\tilde{\epsilon}_i$ is a mean zero multivariate normal vector with $Cov\left(\tilde{\epsilon}_i\right) = I_{m_i}$, which gives the last equality. Since $\tildeY_i$ and $ \widehat{\tilde{\mu}}_{i} $ are independent, the expected LosoCV score can be written
\begin{equation} \label{eq:MSPE_LOSOCV}
E\left[V_{loso}\left(\lambda\right) \right] =  \frac{1}{N}\sum_{i=1}^N\left\{ m_i +  E\left[ \vert \vert \widehat{\tilde{\mu}}_{i} - \tilde{\mu}_{i} \vert \vert^2 \right] \right\}. 
\end{equation}
\noindent
When $N$ is large, we expect that $\widehat{\tilde{\mu}}_{i}$ should be close to $\widehat{\tilde{\mu}}^{\left[ -i \right]}_{i}$, so $E\left[V_{loso}\left(\lambda\right) \right]$ should be a good approximation to the mean-squared prediction error. For a formal proof of consistency, see \citet{xu2012asymptotic}.

  \subsubsection{Computation of the LosoCV score}
  
  \begin{lemma}[Shortcut formula for LosoCV] \label{lemma:losocv-shortcut}
  The LosoCV score satisfies the following identity:
  \begin{equation*}
 V_{loso}\left( \lambda \right) = \frac{1}{N} \sum_{i = 1}^N \left(\tildeY_i - \widehat{\tildeY_i}\right)' \left(I_{ii} - \tildeA_{ii}\right)^{-T}\left(I_{ii} - \tildeA_{ii}\right)^{-1}\left(\tildeY_i - \widehat{\tildeY_i}\right),
  \end{equation*}
  \noindent
  where $\tildeA_{ii}$ is the diagonal block of smoothing matrix $\tildeA$ corresponding to the observations on subject $i$, and $I_{ii}$ is a $m_i \times m_i$ identity matrix.
\end{lemma}

A detailed presentation and proof can be found in \citet{xu2012asymptotic} and supplementary materials \citet{xuasymptotic}.  The authors additionally proposed an approximation to the LosoCV score to further reduce the computational cost of evaluating $V_{loso}$, which can be expensive due to the inversion of the $I_{ii} - \tildeA_{ii}$. Using the Taylor expansion of $\left(I_{ii} - \tildeA_{ii}\right)^{-1} \approx I_{ii} + \tildeA_{ii}$, we can use the following to approximate $V_{loso}$:

\begin{equation} \label{eq:approx-losocv}
V_{loso}^*\left( \lambda \right) = \frac{1}{N} \vert \vert \left(I - \tildeA\right)\tildeY \vert \vert^2 + \frac{2}{N} \sum_{i = 1}^N \hat{\tilde{e}}'_{i}\tildeA_{ii}\hat{\tilde{e}}_i,
\end{equation}
\noindent
where $\hat{\tilde{e}}_i$ is the portion of the vector of prediction errors $\left(I - \tildeA\right)\tildeY$ corresponding to subject $i$. They show that under mild conditions, and for fixed, nonrandom $\lambda$, the approximate losoCV score $V^*$ and the true losoCV score $V_{loso}$ are asymptotically equivalent. See Theorem 3.1 of \citet{xu2012asymptotic}.
  
\vspace{0.8in} 


\subsection{Selection of multiple smoothing parameters}

With the definition of the unbiased risk estimate and the leave-one-subject-out criteria, the expression of the smoothing matrix in Equation~\ref{eq:smoothing-matrix-A-tilde} permits the straightforward evaluation of both scores $U\left(\lambda, \bftheta \right)$ and $V_{loso}^*\left(\lambda, \bftheta \right)$, where $\bftheta = \left(\theta_1,\dots, \theta_g\right)'$ denotes the vector of smoothing parameters associated with each RK.  In this section, we discuss a algorithm to minimize the unbiased risk estimate $U\left(\lambda, \bftheta\right)$ with respect to $\lambda$ and $\bftheta$ hidden in $Q = \sum_{\beta = 1}^q \theta_\beta Q_\beta$, where the $\left(i,j\right)$ entry of $Q_\beta$ is given by $R_\beta\left(\bfv_i,\bfv_j\right)$.  We present minimization of the unbiased risk estimate explicitly, but the mechanics of the optimization are very similar to those necessary for optimizing the leave-one-subject-out cross validation criterion. The details of a procedue for explicitly minimizing the alternative criterion are presented in \citet{xu2012asymptotic}, which is based on the algorithms of \citet{gu1991minimizing}, \citet{kim2004smoothing} (which is the basis for the algorithm which follows) and \citet{wood2004stable}. The key difference between the minimization of $U$ and the minimization of $V^*_{loso}$ lies in the calculation of the gradient and the Hessian matrix in the Newton update. To minimize the unbiased risk estimate,

\begin{enumerate}
\item Fix $\bftheta$; minimize $U\left(\lambda \vert \bftheta\right)$ with respect to $\lambda$.
\item Update $\bftheta$ using the current estimate of $\lambda$.
\end{enumerate}
\noindent
Executing step 1 follows immediately from the expression for the smoothing matrix. Step 2 requires evaluating the gradient and the Hessian of $U\left( \bftheta \vert \lambda \right)$ with respect to $\bfkappa = \log\left(\bftheta\right)$. Optimizing with respect to $\bfkappa$ rather than on the original scale is motivated by two driving factors: first, $\bfkappa$ is invariant to scale transformations. With examination of $U$ and $V^*$ and \ref{eq:cholesky-tildeA}, it is immediate that the $\theta_\beta \tildeQ_\beta$ are what matter in determining the minimum. Multiplying the $\tildeQ_\beta$ by any positive constant leaves the $\theta_\beta$ subject to rescaling, though the problem itself is unchanged by scale transformations. The derivatives of $U\left(\cdot\right)$ with respect to $\bfkappa$ are invariant to such transformations, while the derivatives with respect to $\bftheta$ are not. In addition, optimizing with respect to $\bfkappa$ converts a constrained optimization ($\theta_\beta \ge 0$) problem to an unconstrained one.

\subsubsection{Algorithms}

The following presents the main algorithm for minimizing $U\left(\lambda, \bftheta \right)$ and its key components are presented in the section to follow. The minimization of $U$ is done via two nested loops. Fixing tuning parameters, the outer loop minimizes $U$ with respect to smoothing parameters via quasi-Newton iteration of \citet{dennis1996numerical}, as implemented in the \texttt{nlm} function in \texttt{R}. The inner loop then minimizes $\ell_\lambda$ with fixed tuning parameters via Newton iteration. Fixing the $\theta_\beta$s in $J \left(\phi^*\right) = \sum_\beta \theta^{-1}_\beta J_\beta \left(\phi_\beta^*\right)$, the outer loop with a single $\lambda$ is a straightforward task. 

\begin{algorithm}[H]
\caption{ }
\begin{algorithmic}
\STATE \textbf{Initialization:} 
	\STATE Set $\Delta \bfkappa := 0$; \;$\bfkappa_{-}:=\bfkappa_{0}$; \;$V_- = \infty$; \;( or $M_- = \infty$)

\STATE \textbf{Iteration:} 
	\WHILE{not converged}
		\STATE For current value $\bfkappa^* = \bfkappa_- + \Delta \bfkappa$, compute $Q^*_\theta = \sum_{\beta = 1}^g \theta^*_\beta Q_\beta$ and scale so that $\mbox{tr}\left(Q_\beta\right)$ is fixed. 
		\STATE Compute $\tildeA\left(\lambda \vert \bftheta^* \right) = \tildeA\left(\lambda, \exp\left({\bfkappa^*} \right)\right)$.
		\STATE Minimize $U\left(\lambda \vert \bfkappa^* \right) =  \tildeY'\left( I - \tildeA \right)^2\tildeY + 2\mbox{tr}\tildeA $
		\STATE Set $U_* := \min \limits_\lambda Y\left( \lambda \vert \bfkappa^* \right) $
		\IF{$U^* > U_-$ }
		 		\STATE Set $\Delta \bfkappa := \Delta \bfkappa/2$
		 		\STATE Go to (1).
		\ELSE
		\STATE Continue
		\ENDIF
		\STATE Evaluate gradient $\mathbf{g} = \left(\partial /\partial \bfkappa\right) U\left(\bfkappa \vert \lambda\right)$
		\STATE Evaluate Hessian $H = \left(\partial^2 /\partial \bfkappa\partial \bfkappa' \right) U\left(\bfkappa \vert \lambda\right)$.
		\STATE Calculate step $\Delta \bfkappa$:
			\IF{$H$ positive definite}  
				\STATE $\Delta \bfkappa := -H^{-1} \mathbf{g}$
			\ELSE
				\STATE $\Delta \bfkappa := -\tilde{H}^{-1} \mathbf{g}$, where $\tilde{H} = \textup{diag}\left(\bfeps\right)$ is positive definite. \label{ensure-hessian-PD}
			\ENDIF
	\ENDWHILE
\STATE \textbf{Calculate optimal model:} 
	\IF{$\Delta \kappa_\beta < -\gamma$, for $\gamma$ large}
		\STATE Set $\kappa_{*\beta} := -\infty$
	\ENDIF
	\STATE Compute $Q^*_\theta = \sum_{\beta = 1}^g \theta^*{\beta} Q_\beta$;
	\STATE Calculate $\begin{bmatrix} d \\ c \end{bmatrix} = \tilde{C}^{-1} \tilde{C}^{-T} \begin{bmatrix} \tildeB' \\ {\tildeQ_*^\theta}' \end{bmatrix} \tildeY$
\end{algorithmic}
\end{algorithm}

Calculation of the gradient $\bfg$ and Hessian $H$ mirror the details in \citet{gu1991minimizing}, replacing the null basis matrix $B$ and representer matrix $Q$ with $D^{-1}XB$ and $D^{-1}XB$, respectively. They also present details on convergence criteria based on those suggested in \citet{gill1981practical}, who also present detailed discussion of the Newton method based on the Cholesky decomposition necessary for calculating the update direction for $\bfkappa$. The step in \ref{ensure-hessian-PD} returns a descent direction even when $H$ is not positive definite by adding positive mass to the diagonal elements of $H$ if necessary to produce $\tilde{H} = G'G$ where $G$ is upper triangular. See \citet{gill1981practical} 4.4.2.2 for details. 
\bigskip

The unbiased risk estimate $U\left(\lambda, \bftheta\right)$ is fully parameterized by 

\begin{equation}
\left(\lambda_1, \dots, \lambda_q\right) = \left(\lambda \theta^{-1}_1, \dots, \lambda \theta^{-1}_q\right),
\end{equation}
\noindent
so the smoothing parameters $\left(\lambda, \theta_1, \dots, \theta_q\right)$ over-parameterize the score, which is the reason for scaling the trace of $Q_\beta$. The starting values for the $\theta$ quasi-Newton iteration are obtained with two passes of the fixed-$\theta$ outer loop as follows:

\begin{enumerate}
\item Set $\breve{\theta}_\beta^{-1} \propto \mbox{tr}\left( \tildeQ_\beta \right)$, minimize $U\left(\lambda\right)$ with respect to $\lambda$ to obtain $\breve{\phi}$. \label{theta-starting-values-1}
\item Set $\check{\theta}_\beta^{-1} \propto  J_\beta\left(\breve{\phi}_\beta \right)$, minimize $U\left(\lambda\right)$ with respect to $\lambda$ to obtain $\check{\phi}$. \label{theta-starting-values-2}
\end{enumerate}
\noindent
The first pass allows equal opportunity for each penalty to contribute to the GCV score, allowing for arbitrary scaling of $J_\beta \left(\phi_\beta\right)$. The second pass grants greater allowance to terms exhibiting strength in the first pass. The following $\theta$ iteration fixes $\lambda$ and starts from $\check{\theta}_\beta$. These are the starting values adopted by \citet{gu1991minimizing}; the starting values for the first pass loop are arbitrary, but are invariant to scalings of the $\theta_\beta$. The starting values in \ref{theta-starting-values-2} for the second pass of the outer are based on more involved assumptions derived from the background formulation of the smoothing problem: the penalty is of the form

\[
J\left(\right)= \sum_{\beta = 1}^q \theta^{-1}_\beta \langle \phi, \phi\rangle_\beta
\]
\noindent
After the first pass, the initial fit $\breve{\phi}$ reveals where the structure in the true $\phi$ lie in terms of the components of the subspaces $\hilbert_\beta$. Less penalty should be applied to terms exhibiting strong signal.  
 


