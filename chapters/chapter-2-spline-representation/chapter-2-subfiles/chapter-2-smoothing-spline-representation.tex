This section presents a method for regularized estimation of the varying coefficient function $\phi$ using a reproducing kernel Hilbert space (RKHS) framework. To do so, we first must establish some notation and review the relevant mathematical details of the surrounding framework. A Hilbert space $\hilbert$ of functions on a set $\mathcal{V}$ with inner product $\langle \cdot, \cdot\rangle_\hilbert$ is defined as a complete inner product linear space. A Hilbert space is called a reproducing kernel Hilbert space if the evaluation functional $\left[\bfv\right]f = f\left(\bfv\right)$ is continuous in $\hilbert$ for all $\bfv \in \mathcal{V}$. The Reisz Representation Theorem gives that there exists $Q \in \hilbert$, the representer of the evaluation functional $\left[\bfv\right]\left(\cdot\right)$, such that $\langle Q_\bfv, \phi \rangle_\hilbert = \phi\left(\bfv\right)$ for all $\phi \in \mathcal{H}$. See \citet{gu2013smoothing} Theorem 2.2.

\bigskip

The symmetric, bivariate function $Q\left(\bfv_1, \bfv_2 \right) = Q_{\bfv_2 }\left(\bfv_1\right) = \langle Q_{\bfv_1}, Q_{\bfv_2} \rangle_\hilbert$ is called the reproducing kernel (RK) of $\hilbert$. The RK satisfies that for every $v \in \mathcal{V}$ and $f \in \mathcal{H}$,

\begin{enumerate}
\item $Q\left(\cdot, \bfv \right) \in \hilbert$ 
\item $f\left(\bfv\right) = \langle f, Q\left(\cdot, v\right)\rangle_\hilbert$\label{rkhs-reproducing-property}
\end{enumerate}
\noindent
The first property is called the reproducing property of $Q$. Every reproducing kernel uniquely determines the RKHS, and in turn, every RKHS has unique reproducing kernel. See \citet{gu2013smoothing}, Theorem 2.3. The kernel satisfies that for any $\left\{\bfv_1,\dots, \bfv_{n_1}\right\}$, $\left\{\breve{\bfv}_1,\dots, \breve{\bfv}_{n_2}\right\} \in \mathcal{V}$ and $\left\{a_1,\dots, a_{n_1}\right\}$, $\left\{a_1,\dots, a'_{n_2}\right\} \in \Re$,

\begin{equation}
 \langle\sum_{i = 1}^{n_1} a_i Q\left(\cdot, \bfv_i\right), \sum_{j = 1}^{n_2} a'_j Q\left(\cdot, \breve{\bfv}_j\right) \rangle_\hilbert.
\end{equation}

\bigskip


Let $\mathcal{N}_J = \left\{ \phi:\; J\left(\phi\right) = 0\right\}$ denote the null space of $J$, and consider the decomposition

\[
\hilbert = \mathcal{N}_J \oplus \hilbert_J.
\]
\noindent
The space $\hilbert_J$ is a RKHS having $J\left(\phi\right)$ as the squared norm. The minimizer of \ref{eq:penalized-likelihood-vectorized} has form 

\begin{equation} \label{eq:RKHS-functional-form}
\phi\left(\bfv\right) = \sum_{\nu = 1}^{d_0} d_\nu \eta_\nu\left( \bfv \right) + \sum_{i=1}^n c_i Q\left(\bfv_i, \bfv \right),
\end{equation} 
\bigskip
\noindent
where $\lbrace \eta_\nu \rbrace$ is a basis for $\mathcal{N}_J$, and $Q_J$ is the RK in $\hilbert_J$. 

\bigskip


The objective function \ref{eq:penalized-least-squares} can be rewritten in terms of the squared norm with respect to $\langle\cdot,\cdot\rangle_\hilbert$:
\begin{equation} \label{eq:penalized-least-squares-2}
-2\ell_\phi + \lambda J\left(\phi\right) = \sum_{i=1}^N \sum_{j=2}^{m_i} \sigma^{-2}_{ij}\left( y_{ij} - \sum_{k<j}\left( L_{ijk}\phi\right) y_{ik}  \right)^2 + \lambda \vert\vert P_J \phi \vert \vert^2
\end{equation}
\noindent
where $P_J$ is the projection operator which projects $\phi$ onto the subspace $\hilbert_J$, and $L_{ijk}$ denotes the evalutation functional $\left[v_{ijk}\right] \phi$. Let $\xi_{ijk}$ denote the representer of $L_{ijk}$; \citet{kimeldorf1971some} established that the minimizer of \ref{eq:penalized-least-squares-2} has form

\begin{equation} \label{eq:form-of-smoothing-spline-solution}
\phi\left( \bfv \right) = \sum_{\nu=1}^{m} d_\nu \eta_\nu \left( v \right) + \sum_{i = 1}^{\vert V \vert} c_{i}\left(P_J \xi_i \right)
\end{equation}
\noindent
where $V = \bigcup\limits_{i,j,k} \bfv_{ijk}$, and $\left\{\eta_1,\dots, \eta_{m}\right\}$ span $\hilbert_0$, the null space of $P_J$. To show this, we start by noting that any $\phi \in \mathcal{H}$ can be written 

\begin{equation} \label{eq:smoothing-spline-representer-expansion-1}
\phi\left( \bfv \right) = \sum_{\nu=1}^{m} d_\nu \eta_\nu \left( v \right) + \sum_{i = 1}^{\vert V \vert} c_{i}\left(P_J \xi_i \right) + \rho\left(\bfv\right)
\end{equation}
\noindent
where $\rho \perp \mathcal{H}_0,\; \textup{span}\lbrace \left(P_1 \xi_{j} \right) \rbrace_{j = 1}^{\vert V\vert}$. To establish that the solution has form \ref{eq:form-of-smoothing-spline-solution} requires showing that the minimizer of \ref{eq:penalized-least-squares-2} has $\rho = 0$. The proof entails demonstrating that $\rho$ does not improve the residual sums of squares and only adds to the penalty term, $J\left(\phi\right)$. Details are similar to those in the proof provided in \citet{wahba1990spline} and are left to the appendix \ref{chapter-7-appendix}.



\subfile{chapter-2-subfiles/tensor-product-hilbert-space-construction}
\subfile{chapter-2-subfiles/chapter-2-smoothing-spline-reproducing-kernel-example-1}

For $\bfv \in V$ where $V$ is a product domain, ANOVA decompositions can be characterized by 
\begin{equation}\label{eq:ssanova-decomposition-of-RKHS}
\hilbert = \bigoplus\limits_{\beta=0}^{g} \hilbert_\beta
\end{equation}
\noindent
and
\begin{equation}\label{eq:ssanova-decomposition-of-penalty}
J\left(\phi\right) = \sum_{\beta=0}^{g} \theta^{-1}_\beta J_\beta \left( \phi_\beta \right),
\end{equation}
\noindent
where $\phi_\beta \in \hilbert_\beta$, $J_\beta$ is the square norm in $\hilbert_\beta$, and $0 < \theta_\beta < \infty$. This gives 

\begin{align*}
\hilbert_0 &= \mathcal{N}_J \\
\hilbert_J &= \bigoplus\limits_{\beta=1}^{g} \hilbert_\beta, \mbox{ and} \\
Q &= \sum_{\beta=1}^g \theta_\beta Q_\beta,
\end{align*}
\noindent
where $Q_\beta$ is the RK in $\hilbert_\beta$. The $\left \{ \theta_\beta \right\}$ are additional smoothing parameters, which are implicit in notation to follow for the sake of concise demonstration. 

\bigskip

\bigskip


\bigskip
\noindent
Let $Y$ denote the vector 

\begin{align}
Y &= \left( Y'_1, Y'_2, \dots, Y'_{N} \right)'\\
 &= \left( y_{12}, y_{13},\dots, y_{1,m_1}, \dots, y_{N2}, y_{N3},\dots, y_{N,m_N} \right)'
\end{align}
\noindent
of length $n_y= \sum_{i} M_i - N$  constructed by stacking the $N$ observed response vectors $Y_1,\dots, Y_N$ less their first element $y_{i1}$ one on top of each other. Define $X_i$ to be the $m_i \times \vert V \vert$ matrix containing the covariates necessary for regressing each measurement $y_{i2}, \dots, y_{i,m_i}$ on its predecessors as in model~\ref{eq:cholesky-regression-model-2}, and stack these on top of one another to obtain

\begin{equation} \label{eq:ar-design-matrix-1}
X = \begin{bmatrix}
X_1 \\
X_2\\
\vdots \\
X_N
\end{bmatrix},
\end{equation}
\noindent
which has dimension $n_y \times \vert V \vert$. Then the solution $\phi$  minimizing \ref{eq:penalized-least-squares-2}  is the solution to the minimization problem

\begin{equation} \label{eq:ar-design-matrix-1}
\vert \vert D^{-1/2}\left( Y - X \left( Bd + Qc \right) \right) \vert \vert^2  + \lambda c^\prime Q c 
\end{equation}
\noindent
where the $\left(i,j\right)$ entry of the $\vert V \vert \times \vert V \vert$ matrix $Q$ is given by $\langle P_1 \xi_i,  P_1 \xi_j \rangle_\hilbert$, and $B$ is the $\vert V \vert \times d_0$ matrix with $i$-$\nu^{th}$ element $\eta_\nu\left(v_i\right)$, which we assume to be full column rank.  The diagonal matrix $D$ holds the $n_y \times n_y$  innovation variances $\sigma^2_{ijk}$. 

\bigskip

\begin{example}{Construction of $X_i$ with complete data} 
\\
\vspace{.5cm} 
\\
Straightforward construction of the autoregressive design matrix $X_i$ is straight forward in the case that there are an equal number of measurements on each subject at a common set of measurement times $t_1,\dots, t_M$. When complete data are available for measurement times $t_1, \dots, t_M$, 

\begin{equation}
X_i =  \begin{bmatrix} 
y_{i, t_1} & 0 & 0 &0&& \dots & 0 \\
 0 & y_{i, t_1} &  y_{i, t_2}&0 &0& \dots & 0 \\
 \vdots &&&&&&\\
 0 & \dots &0 & \dots& y_{i,t_1} & \dots &  y_{i, t_{M-1}}
\end{bmatrix}
\end{equation}
\noindent
for all $i = 1,\dots, N$. Note that this design matrix specification does not require that measurement times be regularly spaced.  
\end{example}

\begin{example}{Construction of $X_i$ with incomplete data}
\\
We demonstrate the construction of the autoregressive design matrices when subjects do not share a universal set of observation times for $N = 2$; the construction extends naturally for an arbitrary number of trajectories. Let subjects have corresponding sample sizes $m_1 = 4$, $m_2 = 4$, with measurements on subject 1 taken at $t_{11} = 0, t_{12} = 0.2, t_{13} = 0.5, t_{14} = 0.9$ and on subject 2 taken at $t_{21} = 0, t_{22} = 0.1, t_{23} = 0.5, t_{24} = 0.7$.  Then the unique within-subject pairs of observation times $\left(t,s\right)$ such that $0 \le s < t \le 1$ are

\begin{table}[H]
\centering
\begin{tabular}{l|r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r}
t & 0.1 & 0.2 & 0.5 & 0.5 & 0.5 & 0.7 & 0.7 & 0.7 & 0.9 & 0.9 & 0.9 \\ 
 s & 0.0 & 0.0 & 0.0 & 0.1 & 0.2 & 0.0 & 0.1 & 0.5 & 0.0 & 0.2 & 0.5 \\
\end{tabular}
\end{table}
\noindent
This gives that $V =  \left\{\bfv_{121},\dots, \bfv_{143}  \right\} \bigcup \left\{\bfv_{221},\dots, \bfv_{243}  \right\} = \left\{\bfv_1,\dots, \bfv_{11} \right\}$, where the distinct observed $v = \left(l, m\right)$ are 

\begin{table}[H]
\centering
\begin{tabular}{l|r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r}
l & 0.10 & 0.20 & 0.50 & 0.40 & 0.30 & 0.70 & 0.60 & 0.20 & 0.90 & 0.70 & 0.40 \\ 
  m & 0.05 & 0.10 & 0.25 & 0.30 & 0.35 & 0.35 & 0.40 & 0.60 & 0.45 & 0.55 & 0.70 \\ 
\end{tabular}
\end{table}
\noindent
Then a potential construction of the autoregressive design matrix for subject is given by:
\begin{equation}
X_1 =  \begin{bmatrix} 
0   & y_{1, 1}  &	0            &    0   &    0           & 0 & 0 & 0 & 0 & 0  \\
0   &	0  	      &	y_{1, 1}  &    0   & y_{1, 2}   &  0 & 0 & 0 & 0 & 0 \\
 0   &    0         & 0           &    0   &    0          & 0  & 0	&  y_{1, 1}    & y_{1, 2}& y_{1, 3} 
\end{bmatrix}
\end{equation}
\noindent
and similarly, for subject 2:

\begin{equation}
X_2 =  \begin{bmatrix} 
y_{2, 1}  & 	0  &	  0           &    0            &    0   & 0 & 0 & 0 & 0 & 0  \\
0   	      &  	0  &	y_{2, 1}  &    y_{2,2}   &    0   &  0 & 0 & 0 & 0 & 0 \\
 0   	      &        0  &    0           &    0            &  y_{2, 1}    & y_{2, 2}& y_{2, 3} &    0   & 0  & 0
\end{bmatrix}
\end{equation}
\end{example}

\subsubsection{Construction of the solution $\hat{\phi}$}

Differentiating $-2\ell_\phi + \lambda J\left(\phi\right)$ with respect to $c$ and $d$ and setting equal to zero, we have that 

\begin{align}
\frac{\partial}{\partial c}\left[-2\ell_\phi + \lambda J\left(\phi\right)\right] = Q X^\prime D^{-1}\left[ X\left(Bd + Qc\right) - Y  \right] + \lambda Qc &= 0 \nonumber \\
%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
\iff    X'D^{-1} X \bigg[ Bd + Qc \bigg] + \lambda c  &= X' D^{-1}Y \label{eq:normal-eq-1}
\end{align}

\begin{align}
\frac{\partial}{\partial d}\left[-2\ell_\phi + \lambda J\left(\phi\right)\right] = B^\prime X^\prime D^{-1}\left[ X\left(Bd + Qc\right) - Y  \right] &=0 \nonumber \\
%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
\iff   - \lambda B' c  &= 0  
\end{align}
\bigskip
\noindent
For fixed smoothing parameter, the solution $\phi$ is obtained by finding $c$ and $d$ which satisfy
\begin{align} 
Y &= X \bigg[ Bd + \left(Q  + \lambda \left(X^\prime D^{-1} X \right)^{-1} \right) c \bigg] \label{eq:ssanova-normal-eq-1} \\
B' c  &= 0  \label{eq:ssanova-normal-eq-2}
\end{align}
\noindent






%
%Differentiating $-2\ell_\phi + \frac{\lambda}{2} J\left(\phi\right)$ with respect to $c$ and $d$ and setting equal to zero, we have that 
%
%\begin{align}
%\frac{\partial\left[-2\ell_\phi + \frac{\lambda}{2} J\left(\phi\right)\right]}{\partial c} = Q X^\prime D^{-1}\left[ X\left(Bd + Qc\right) - Y  \right] + \lambda Qc &= 0 \nonumber \\
%%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
%\iff    X'D^{-1} X \bigg[ Bd + Qc \bigg] + \lambda c  &= X' D^{-1}Y \label{eq:normal-eq-1}
%\end{align}
%
%\begin{align}
%\frac{\partial\left[-2\ell_\phi + \frac{\lambda}{2} J\left(\phi\right)\right]}{\partial d} = B^\prime X^\prime D^{-1}\left[ X\left(Bd + Qc\right) - Y  \right] &=0 \nonumber \\
%%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
%\iff   - \lambda B' c  &= 0  
%\end{align}
%\bigskip
%\noindent
%Setting equal to zero, we have that $c$ and $d$ satisfy normal equations
%\begin{align} 
%Y &= X \bigg[ Bd + \left(Q  + \lambda \left(X^\prime D^{-1} X \right)^{-1} \right) c \bigg] \label{eq:ssanova-normal-eq-1} \\
%B' c  &= 0  \label{eq:ssanova-normal-eq-2}
%\end{align}
%\noindent
%
%\begin{align*}
%\tildeK &= \left(W'D^{-1}W\right) K \left(W'D^{-1}W\right)\\
% \tildec &= \left(W'D^{-1}W\right)^{-1}c\\
%\tildeB &= \left(W'D^{-1}W\right)B\\ 
%\tilded &= d\\
%\tildeY &= W'D^{-1}Y\\
%\end{align*}
%\bigskip
%\noindent
%then, the system defined by  \ref{eq:simple-normal-eq-1} and \ref{eq:simple-normal-eq-2} may be written
%
%\begin{align} 
%\tildeY &= \tildeB \tilded + \left(\tildeK  + \lambda \left(W^\prime D^{-1} W \right) \right) \tildec \label{eq:transform-normal-eq-1} \\
%\tildeB' \tildec  &= 0  \label{eq:transform-normal-eq-2}
%\end{align}
%
%Using the QR decomposition of $\tildeB$, we may write 
%
%\begin{equation*}
%\tildeB = \tildeQ\tildeR = \begin{bmatrix} \tildeQ_1 &  \tildeQ_2 \end{bmatrix} \begin{bmatrix} \tildeR \\  0 \end{bmatrix} = \tildeQ_1 \tildeR
%\end{equation*}
%\noindent
%where $\tildeQ$ is an orthogonal matrix; $\tildeQ_1$ has dimension $n \times d_0$,  and $\tildeQ_2$ has dimension $n \times \left(n-d_0\right)$. Since $\tildeB'\tildec = 0$, $\tildec$ must belong to the subspace spanned by the columns of $\tildeQ_2$, so 
%
%\[
%\tildec = \tildeQ_2 \gamma
%\] 
%\noindent
%for some $\gamma \in \mathrm{R}^{n-d_0}$. Premultiplying \ref{eq:transform-normal-eq-1} by $\tildeQ'_2$, it follows that 
%\begin{equation} \label{eq:solve-for-ctilde}
%\tildec =  \tildeQ_2\bigg[ \tildeQ'_2 \left( \tildeK + \lambda \left( W' D^{-1} W \right) \right)\tildeQ_2 \bigg]^{-1} \tildeQ'_2 \tildeY
%\end{equation}
%\bigskip
%\noindent
%Using $\tildeB = \tildeQ_1 \tildeR$, we can write
%\begin{equation} \label{eq:solve-for-dtilde}
%\tilded = \tildeR^{-1} \tildeQ'_1 \bigg[ \tildeY - \left( \tildeK + \lambda \left( W' D^{-1} W \right) \right) \tildec  \bigg]
%\end{equation}
%
%


Letting $\tildeY = D^{-1/2} Y$, $\tildeB = D^{-1/2} X B $, and $\tildeQ = D^{-1/2} X Q$, the penalized log likelihood \ref{eq:penalized-likelihood-vectorized} may be written

\begin{equation}\label{eq:penalized-loglik-tilde-vectorized}
-2\ell_\lambda \left(c, d \right) + \lambda J\left( \phi \right) = \bigg[ \tildeY - \tildeB d - \tildeQ c\bigg]'\bigg[ \tildeY - \tildeB d - \tildeQ c\bigg] + \lambda c'Qc.
\end{equation}
\noindent
Taking partial derivatives with respect to $d$ and $c$ and setting equal to zero yields normal equations 

\begin{align}
\begin{split}
\tildeB'\tildeB d + \tildeB'\tildeQ c &= \tildeB' \tildeY \\
\tildeQ'\tildeB d + \tildeQ'\tildeQ c + \lambda Q c &= \tildeQ' \tildeY, 
\end{split}
\end{align}

\noindent
Some algebra yields that this is equivalent to solving the system

\begin{equation} \label{eq:vectorized-normal-equations}
\begin{bmatrix}
\tildeB'\tildeB & \tildeB'\tildeQ \\
\tildeQ'\tildeB & \tildeQ'\tildeQ + \lambda Q\\
\end{bmatrix}
\begin{bmatrix}
d\\
c\\
\end{bmatrix}
= \begin{bmatrix}
\tildeB'\tildeY \\
 \tildeQ'\tildeY\\
\end{bmatrix}
\end{equation}


Fixing smoothing parameters $\lambda$ and $\theta_\beta$ (hidden in $Q$ and $\tildeQ$ if present), assuming that $\tildeQ$ is full column rank, \ref{eq:vectorized-normal-equations} can be solved by the Cholesky decomposition of the $\left( n + d_0 \right) \times \left( n + d_0 \right)$ matrix followed by forward and backward substitution. See \citet{golub2012matrix}. Singularity of $\tildeQ$ demands special consideration. Write the Cholesky decomposition

\begin{equation} \label{eq:normal-equation-cholesky}
\begin{bmatrix}
\tildeB'\tildeB & \tildeB'\tildeQ \\
\tildeQ'\tildeB & \tildeQ'\tildeQ + \lambda Q\\
\end{bmatrix}
= \begin{bmatrix}
C'_1 & 0 \\
C'_2  & C'_3 
\end{bmatrix}
\begin{bmatrix}
C_1 & C_2 \\
0  & C_3 
\end{bmatrix}
\end{equation}
\noindent
where $\tildeB'\tildeB = C'_1 C_1$, $C_2 = C_1^{-T} \tildeB' \tildeQ$, and $C'_3 C_3 = \lambda Q +  \tildeQ'\left( I - \tildeB\left( \tildeB' \tildeB \right)^{-1} \tildeB' \right)\tildeQ$. Using an exchange of indices known as pivoting, one may write 

\begin{equation*}
C_3 = \begin{bmatrix} H_1 & H_2 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} H \\  0 \end{bmatrix},
\end{equation*}
\noindent
where $H_1$ is nonsingular. Define
\begin{equation} \label{eq:cholesky-factor-mod}
\tilde{C}_3 = \begin{bmatrix}
H_1 & H_2 \\
0  & \delta I 
\end{bmatrix}, \;\;
\tilde{C} = \begin{bmatrix}
C_1 & C_2 \\
0  & \tilde{C}_3 
\end{bmatrix};
\end{equation}
\noindent
then
\begin{equation} \label{eq:cholesky-factor-mod-inverse}
\tilde{C}^{-1} = \begin{bmatrix}
C_1^{-1} & -C_1^{-1} C_2 \tilde{C}_3^{-1} \\
0  & \tilde{C}_3^{-1}
\end{bmatrix}.
\end{equation}

Premultiplying \ref{eq:normal-equation-cholesky} by $\tilde{C}^{-T}$, straightforward algebra gives 

\begin{equation} \label{eq:vectorized-normal-equations-cholesky}
\begin{bmatrix}
I & 0 \\
0 & \tilde{C}_3^{-T} C_3^{T} C_3 \tilde{C}_3^{-1}\\
\end{bmatrix}
\begin{bmatrix}
\tilde{d}\\
\tilde{c}\\
\end{bmatrix}
= \begin{bmatrix}
C_1^{-T} \tildeB'\tildeY \\
\tilde{C}_3^{-T} \tildeQ'\left( I - \tildeB\left( \tildeB' \tildeB \right)^{-1} \tildeB' \right) \tildeY\\
\end{bmatrix}
\end{equation}
\noindent
where $\left( \tilde{d}'\;\;\tilde{c}' \right)' =  \tilde{C}' \left( d\;\;c \right)'$. Partition $\tilde{C}_3 = \begin{bmatrix} K &  L\end{bmatrix}$; then $HK = I$ and $HL = 0$. So

\begin{align*}
\tilde{C}_3^{-T} C_3^{T} C_3 \tilde{C}_3^{-1} &= \begin{bmatrix} K' \\ L' \end{bmatrix} C'_3C_3 \begin{bmatrix} K &  L\end{bmatrix} \\
&= \begin{bmatrix} K' \\ L' \end{bmatrix} H'H \begin{bmatrix} K &  L\end{bmatrix} \\
&= \begin{bmatrix} I & 0 \\ 0 & 0 \end{bmatrix}.
\end{align*}
\noindent
If $L'C_3^{T} C_3 L = 0$, then $L'\tildeQ'\left( I - \tildeB\left( \tildeB' \tildeB \right)^{-1} \tildeB' \right)\tildeQ L = 0$, so $L'\tildeQ'\left( I - \tildeB\left( \tildeB' \tildeB \right)^{-1} \tildeB' \right) \tildeY = 0$. Thus, the linear system has form

\begin{equation} \label{eq:vectorized-normal-equations-cholesky-2}
\begin{bmatrix}
I & 0 & 0\\
0 & I & 0 \\
0 & 0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\tilde{d}\\
\tilde{c}_1\\
\tilde{c}_2
\end{bmatrix}
= \begin{bmatrix}
* \\
* \\
0
\end{bmatrix},
\end{equation}
\noindent
which can be solved, but with $c_2$ arbitrary. One may perform the Cholesky decomposition of \ref{eq:vectorized-normal-equations} with pivoting, replace the trailing $0$ with $\delta I$ for appropriate value of $\delta$, and proceed as if $\tildeQ$ were of full rank. 
\bigskip

It follows that

\begin{equation} \label{eq:tildeY-hat-equals-tildeA-tildeY}
\widehat{\tildeY} = \tildeB d + \tildeQ c = \begin{bmatrix} \tildeB & \tildeQ \end{bmatrix} \tilde{C}^{-1} \tilde{C}^{-T} \begin{bmatrix} \tildeB' \\ \tildeQ' \end{bmatrix} \tildeY = \tildeA\left(\lambda, \bftheta\right) \tildeY.
\end{equation} 
\noindent
where
\begin{align}
\begin{split} \label{eq:smoothing-matrix-A-tilde}
\tildeA\left(\lambda, \bftheta \right) =& \begin{bmatrix} \tildeB & \tildeQ \end{bmatrix} \tilde{C}^{-1} \tilde{C}^{-T} \begin{bmatrix} \tildeB' \\ \tildeQ' \end{bmatrix}  \\
&= G + \left(I - G\right) \tildeQ \left[\tildeQ'\left( I - G \right)\tildeQ + \lambda Q\right]^{-1} \tildeQ'\left(I - G\right),
\end{split}
\end{align} 
\noindent
for
\[
G = \tildeB\left(\tildeB' \tildeB \right)^{-1}\tildeB'.
\]





