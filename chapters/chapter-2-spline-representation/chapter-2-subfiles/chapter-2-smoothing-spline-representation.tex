This section presents a method for regularized estimation of the varying coefficient function $\phi$ using a reproducing kernel Hilbert space (RKHS) framework. To do so, we first must establish some notation and review the relevant mathematical details of the surrounding framework. A Hilbert space $\hilbert$ of functions on a set $\mathcal{V}$ with inner product $\langle \cdot, \cdot\rangle_\hilbert$ is defined as a complete inner product linear space. A Hilbert space is called a reproducing kernel Hilbert space if the evaluation functional $\left[v\right]f = f\left(v\right)$ is continuous in $\hilbert$ for all $v \in \mathcal{V}$. The Reisz Representation Theorem gives that there exists $R \in \hilbert$, the representer of the evaluation functional $\left[v\right]\left(\cdot\right)$, such that $\langle R_v, f \rangle_\hilbert = f\left(v\right)$ for all $f \in \mathcal{H}$. See \citet{gu2013smoothing} Theorem 2.2.

\bigskip

The symmetric, bivariate function $R\left(v_1, v_2 \right) = R_{v_2 }\left(v_1\right) = \langle R_{v_1}, R_{v_2} \rangle_\hilbert$ is called the reproducing kernel (RK) of $\hilbert$. The RK satisfies that for every $v \in \mathcal{V}$ and $f \in \mathcal{H}$,

\begin{enumerate}
\item $R\left(\cdot, v \right) \in \hilbert$ 
\item $f\left(v\right) = \langle f, R\left(\cdot, v\right)\rangle_\hilbert$\label{rkhs-reproducing-property}
\end{enumerate}
\noindent
The first property is called the reproducing property of $R$. Every reproducing kernel uniquely determines the RKHS, and in turn, every RKHS has unique reproducing kernel. See \citet{gu2013smoothing}, Theorem 2.3. The kernel satisfies that for any $\left\{v_1,\dots, v_{n_1}\right\}$, $\left\{\breve{v}_1,\dots, \breve{v}_{n_2}\right\} \in \mathcal{V}$ and $\left\{a_1,\dots, a_{n_1}\right\}$, $\left\{a_1,\dots, a'_{n_2}\right\} \in \Re$,

\begin{equation}
 \langle\sum_{i = 1}^{n_1} a_i R\left(\cdot, v_i\right), \sum_{j = 1}^{n_2} a'_j R\left(\cdot, \breve{v}_j\right) \rangle_\hilbert.
\end{equation}

The objective function \ref{eq:penalized-least-squares} can be rewritten in terms of the squared norm with respect to $\langle\cdot,\cdot\rangle_\hilbert$:
\begin{equation} \label{eq:penalized-least-squares-2}
-2\ell_\phi + \frac{\lambda}{2} J\left(\phi\right) = \sum_{i=1}^N \sum_{j=2}^{m_i} \sigma^{-2}_{ij}\left( y_{ij} - \sum_{k<j}\left( L_{ijk}\phi\right) y_{ik}  \right)^2 + \lambda \vert\vert P_1 \phi \vert \vert^2
\end{equation}
\noindent
where $P_1$ is the projection operator which projects $\phi$ onto the subspace $\hilbert_1$, and $L_{ijk}$ denotes the evalutation functional $\left[v_{ijk}\right] \phi$. Let $\xi_{ijk}$ denote the representer of $L_{ijk}$; \citet{kimeldorf1971some} established that the minimizer of \ref{eq:penalized-least-squares-2} has form

\begin{equation} \label{eq:form-of-smoothing-spline-solution}
\phi\left( v \right) = \sum_{\nu=1}^{m} d_\nu \eta_\nu \left( v \right) + \sum_{i = 1}^{\vert V \vert} c_{i}\left(P_1 \xi_i \right)
\end{equation}
\noindent
where $V = \bigcup\limits_{i = 1}^N v_{ijk}$, and $\left\{\eta_1,\dots, \eta_{m}\right\}$ span $\hilbert_0$, the null space of $P_1$,

\begin{equation*}
\hilbert_0 = \left\{ \phi \in \hilbert:\;J\left( \phi \right) = 0 \right\}.
\end{equation*}
\noindent
Any $\phi \in \mathcal{H}$ can be written 
\begin{equation} \label{eq:smoothing-spline-representer-expansion-1}
\phi\left( v \right) = \sum_{\nu=1}^{m} d_\nu \eta_\nu \left( v \right) + \sum_{i = 1}^{\vert V \vert} c_{i}\left(P_1 \xi_i \right) + \rho\left(v\right)
\end{equation}
\noindent
where $\rho \perp \mathcal{H}_0,\; \textup{span}\lbrace \left(P_1 \xi_{j} \right) \rbrace_{j = 1}^{\vert V\vert}$. To establish that the solution has form \ref{eq:form-of-smoothing-spline-solution} requires showing that the minimizer of \ref{eq:penalized-least-squares-2} has $\rho = 0$. The proof entails demonstrating that $\rho$ does not improve the residual sums of squares and only adds to the penalty term, $J\left(\phi\right)$. Details are left to the appendix \ref{chapter-7-appendix}.

Let $Y$ denote the vector 

\[
Y = \left( y_{12}, y_{13},\dots, y_{1,m_1}, \dots, y_{N2}, y_{N3},\dots, y_{N,m_N} \right)'
\]

of length $n_y=\left(\sum \limits_{i} M_i \right) - N$  constructed by stacking the $N$ observed response vectors $Y_1,\dots, Y_N$ less their first element $y_{i1}$ one on top of each other. Define $X_i$ to be the $m_i \times \vert V \vert$ matrix containing the covariates necessary for regressing each measurement $y_{i2}, \dots, y_{i,m_i}$ on its predecessors as in model~\ref{eq:cholesky-regression-model-2}. For example, in the case that there are an equal number of measurements on each subject at a common set of measurement times $t_1,\dots, t_M$, then 

\begin{equation}
X_i =  \begin{bmatrix} 
y_{i, t_1} & 0 & 0 &0& \dots & 0 \\
 0 & y_{i, t_1} &  y_{i, t_2}&0 & \dots & 0 \\
 \vdots &&&&&\\
 0 & \dots &0 & y_{i,t_1} & \dots &  y_{i, t_{M-1}}
\end{bmatrix}
\end{equation}
\noindent
for all $i = 1,\dots, N$. Note that this design matrix specification does not require that measurement times be regularly spaced.  To illustrate the construction of $X_i$ when observation times vary between subjects, consider a toy example: let $N = 2$, $m_1 = 4$, $m_2 = 4$ with measurements on subject 1 taken at $t_{11} = 0, t_{12} = 0.2, t_{13} = 0.5, t_{14} = 0.9$ and on subject 2 taken at $t_{21} = 0, t_{22} = 0.1, t_{23} = 0.5, t_{24} = 0.7$.  Then the unique within-subject pairs of observation times $\left(t,s\right)$ such that $0 \le s < t \le 1$ are

\begin{table}[H]
\centering
\begin{tabular}{l|r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r}
t & 0.1 & 0.2 & 0.5 & 0.5 & 0.5 & 0.7 & 0.7 & 0.7 & 0.9 & 0.9 & 0.9 \\ 
 s & 0.0 & 0.0 & 0.0 & 0.1 & 0.2 & 0.0 & 0.1 & 0.5 & 0.0 & 0.2 & 0.5 \\
\end{tabular}
\end{table}
\noindent
This gives that $V =  \left\{v_{121},\dots, v_{143}  \right\} \bigcup \left\{v_{221},\dots, v_{243}  \right\} = \left\{v_1,\dots, v{11} \right\}$, where the distinct $v_{i} = \left(l_i , m_i\right)$ are 

\begin{table}[H]
\centering
\begin{tabular}{l|r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r;{2pt/2pt}r}
% &$ v_1$ & $v_2$ & $v_3$ &$ v_4$ & $v_5$ & $v_6$ & $v_7$ & $v_8$ & $v_9$ & $v_{10}$ & $v_{11}$ \\ 
 % \hline
l & 0.10 & 0.20 & 0.50 & 0.40 & 0.30 & 0.70 & 0.60 & 0.20 & 0.90 & 0.70 & 0.40 \\ 
  m & 0.05 & 0.10 & 0.25 & 0.30 & 0.35 & 0.35 & 0.40 & 0.60 & 0.45 & 0.55 & 0.70 \\ 
\end{tabular}
\end{table}

Then a potential construction of the autoregressive design matrix for subject is given by:
\begin{equation}
X_1 =  \begin{bmatrix} 
0   & y_{1, 1}  &	0   	     &    0   &    0  	    &  0   &	 0   &	0   	& 	0   	&    0         & 	0  \\
0   &	0  	  &	y_{1, 1}  &    0   & y_{1, 2}   &  0   &	 0   &	0   	& 	0   	&    0         & 	0  \\
0   &	0   	  &	0    	     &    0   &  0   	    &  0   &	 0   &	0   	&  y_{1, 1}    & y_{1, 2}& y_{1, 3} 
\end{bmatrix},
\end{equation}
\noindent
and similarly, for subject 2:
\begin{equation}
X_2 =  \begin{bmatrix} 
y_{2, 1}   & 0  &	0   &    0   &  0   &  0   &	 0   &	0   	& 	0   	&    0   & 	0  \\
0   &	0   &	 y_{2, 1} & y_{2, 2}  & 0  &  0   &	 0   &	0   	& 	0   	&    0   & 	0  \\
0   &	0   &	0   &    0   &  0   &  0   &	 0   &	0   	&  y_{2, 1} & y_{2, 2}&y_{2, 3}
\end{bmatrix}.
\end{equation}

Let $S$ denote the $n \times d_0$ matrix with $i$-$\nu^{th}$ element $\eta_\nu\left(v_i\right)$, which we assume to be full column rank; let $Q$ denote the $n \times n$ kernel matrix with $i$-$j^{th}$ element $E_m\left(v_i, v_j\right)$, and let $D$ denote the $n_y \times n_y$ diagonal matrix of innovation variances $\sigma^2_{ijk}$. The $\phi^*$ minimizing \ref{eq:thin-plate-loss-function} corresponds to the coefficient vectors $c$, $d$ minimizing

\begin{align}
\begin{split} \label{eq:penalized-likelihood-vectorized} 
-\ell\left(Y \vert c,d\right) + \lambda J_m\left(\phi^*\right) = \left( Y - W \left( Bd + Kc \right) \right)^\prime D^{-1} \left( Y - W \left( Bd + Kc \right) \right) + \lambda c^\prime Q c 
\end{split}
\end{align} 
\bigskip
\noindent
 where $W$ is the matrix of autoregressive covariates constructed so that $\ref{eq:thin-plate-loss-function}$ and $\ref{eq:thin-plate-loss-function-matrix}$ are equivalent. 
\bigskip
Differentiating $Q_\lambda$ with respect to $c$ and $d$ and setting equal to zero, we have that 

\begin{align}
\frac{\partial Q_\lambda}{\partial c} = Q W^\prime D^{-1}\left[ W\left(Sd + Kc\right) - Y  \right] + \lambda Kc &= 0 \nonumber \\
%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
\iff    W'D^{-1} W \bigg[ Bd + Kc \bigg] + \lambda c  &= W' D^{-1}Y \label{eq:normal-eq-1}
\end{align}
\noindent

\begin{align*}
\frac{\partial Q_\lambda}{\partial d} = S^\prime W^\prime D^{-1}\left[ W\left(Sd + Qc\right) - Y  \right] &=0 \nonumber \\
%\Longleftrightarrow    W^\prime D^{-1} W \left( Bd + Kc\right) + \lambda c &= W^\prime D^{-1} Y \\
\iff   - \lambda S' c  &= 0  
\end{align*}
\bigskip
\noindent




