\documentclass[12pt]{article}
\usepackage{graphicx,psfrag,amsfonts,float,mathbbol,xcolor,cleveref}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage[mathscr]{euscript}
\usepackage{enumitem}
\usepackage{accents}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{mathtools}
\usepackage{IEEEtrantools}
\usepackage{times}
\usepackage{cite}
\usepackage{amsthm}
\usepackage[letterpaper, left=1in, top=1in, right=1in, bottom=1in,nohead,includefoot, verbose, ignoremp]{geometry}
\newcommand{\comment}[1]{\text{\phantom{(#1)}} \tag{#1}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*\needsparaphrased{\color{red}}
\newcommand*\needscited{\color{orange}}
\newcommand*\needsproof{\color{blue}}
\newcommand*\outlineskeleton{\color{green}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\bfeps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bfgamma}{\mbox{\boldmath $\gamma$}}
\newcommand{\bflam}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfphi}{\mbox{\boldmath $\phi$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bfalpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bfe}{\mbox{\boldmath $e$}}
\newcommand{\bff}{\mbox{\boldmath $f$}}
\newcommand{\bfone}{\mbox{\boldmath $1$}}
\newcommand{\bft}{\mbox{\boldmath $t$}}
\newcommand{\bfo}{\mbox{\boldmath $0$}}
\newcommand{\bfO}{\mbox{\boldmath $O$}}
\newcommand{\bfx}{\mbox{\boldmath $x$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}


\newcommand{\bfm}{\mbox{\boldmath $m}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfa}{\mbox{\boldmath $a$}}
\newcommand{\bfb}{\mbox{\boldmath $b$}}
\newcommand{\bfY}{\mbox{\boldmath $Y$}}
\newcommand{\bfS}{\mbox{\boldmath $S$}}
\newcommand{\bfZ}{\mbox{\boldmath $Z$}}
\newcommand{\cardT}{\vert \mathcal{T} \vert}
%\newenvironment{theorem}[1][Theorem]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{corollary}[1][Corollary]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{proposition}[1][Proposition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{definition}[1][Definition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\def\bL{\mathbf{L}}

\begingroup\lccode`~=`_
\lowercase{\endgroup\def~}#1{_{\scriptscriptstyle#1}}
\AtBeginDocument{\mathcode`_="8000 \catcode`_=12 }

\makeatletter
\renewcommand{\theenumi}{\Roman{enumi}}
\renewcommand{\labelenumi}{\theenumi.}
\renewcommand{\theenumii}{\Alph{enumii}}
\renewcommand{\labelenumii}{\theenumii.}
\renewcommand{\p@enumii}{\theenumi.}
\makeatother

\begin{document}

%\nocite{*}
\def\bL{\mathbf{L}}
%\usepackage{mathtime}

%%UNCOMMENT following line if you have package


\title{ Nonparametric Covariance Estimation for Longitudinal Data via Penalized Tensor Product Splines}

\author{Tayler A. Blake\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201} \and  Yoonkyung Lee\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201}}

\bibliographystyle{plainnat}
\maketitle

\subsection{Penalized likelihood estimation}
Let $Y$ hold the $N$ observed response vectors $y_1,\dots, y_N$ less their first element $y_{i1}$ stacked into a single vector of dimension $n_y=\left(\sum \limits_{i} M_i \right) - N$. Let $M$ denote the total number of distinct observation times across all subjects. For ease of exposition, let $\sigma_{ij} = \sigma\left( t_{ij} \right)$ and $\phi_{ijk} = \phi \left(t_{ijk} \right)$. The loglikelihood \ref{eq:loglik-general-form} becomes

\begin{align}
\begin{split}
-2\ell\left(Y, \Sigma \right) &=  \sum_{t = 1}^M \log \sigma_t^2  + \sum_{i = 1}^N \sum_{j = 1}^{m_i} \frac {\epsilon_{ijk}^2}{\sigma_{ij}^2}\\
&= \sum_{t = 1}^M \log \sigma_t^2  + \sum_{i = 1}^N \frac{\epsilon_{i1}^2}{\sigma_{i1}^2} + \sum_{i = 1}^N \sum_{j = 2}^{m_i} \frac{\epsilon_{ij}^2}{\sigma_{ij}^{2}} \\
&= \sum_{t = 1}^M \log \sigma_t^2  + \sum_{i = 1}^N \frac{y_{i1}^2}{\sigma_{i1}^2} + \sum_{i = 1}^N \sum_{j = 2}^{m_i} \sigma_{ij}^{-2} \left( y_{ij} - \sum \limits_{k < j}\phi_{ijk} y_{ik}  \right)^2.
\end{split}
\end{align}
\noindent


\begin{equation} \label{eq:loglik-sigma-component}
\sum_{t = 1}^M \log \sigma_t^2  + \sum_{i = 1}^N \frac{y_{i1}^2}{\sigma_{i1}^2} + \sum_{i = 1}^N \sum_{j = 2}^{m_i} \frac{\epsilon_{ij}^2}{\sigma_{ij}^{2}}
\end{equation}
\noindent


\indent
%For ease of exposition, we assume that $\sigma^2\left(t\right)$ is fixed and known.
An iterative procedure for minimizing \ref{eq:} starts by first initialising ?t, using for example the innovation standard error estimated without the penalty. We then minimise (10) to obtain ?tj, j = 1,...,t?1, and revise ?t2 as in (9). We iterate the process until convergence for each t, t = 2, · · · , n. For details about minimisation of (10) with fixed ?t see the Appendix.

\section{Computation of the smoothing spline estimator}


\section{Computation of the P-spline estimator}


\section{Smoothing parameter selection}


\end{document}
