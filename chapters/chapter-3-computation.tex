\documentclass[12pt]{article}
\usepackage{graphicx,psfrag,amsfonts,float,mathbbol,xcolor,cleveref}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage[mathscr]{euscript}
\usepackage{enumitem}
\usepackage{accents}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{mathtools}
\usepackage{IEEEtrantools}
\usepackage{times}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}
\usepackage[letterpaper, left=1in, top=1in, right=1in, bottom=1in,nohead,includefoot, verbose, ignoremp]{geometry}
\newcommand{\comment}[1]{\text{\phantom{(#1)}} \tag{#1}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\Hilbert}{\mathcal{H}}
\newcommand*\needsparaphrased{\color{red}}
\newcommand*\needscited{\color{orange}}
\newcommand*\needsproof{\color{blue}}
\newcommand*\outlineskeleton{\color{green}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\bfeps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bfgamma}{\mbox{\boldmath $\gamma$}}
\newcommand{\bflam}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfphi}{\mbox{\boldmath $\phi$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfkappa}{\mbox{\boldmath $\kappa$}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bfalpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bfe}{\mbox{\boldmath $e$}}
\newcommand{\bff}{\mbox{\boldmath $f$}}
\newcommand{\bfone}{\mbox{\boldmath $1$}}
\newcommand{\bft}{\mbox{\boldmath $t$}}
\newcommand{\bfo}{\mbox{\boldmath $0$}}
\newcommand{\bfO}{\mbox{\boldmath $O$}}
\newcommand{\bfx}{\mbox{\boldmath $x$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}


\newcommand{\bfm}{\mbox{\boldmath $m}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\tildeY}{\tilde{Y}}
\newcommand{\tildey}{\tilde{y}}
\newcommand{\tildeQ}{\tilde{Q}}
\newcommand{\tildeR}{\tilde{R}}
\newcommand{\tildeA}{\tilde{A}}
\newcommand{\tildeepsilon}{\tilde{\epsilon}}
\newcommand{\bftheta}{\mbox{\boldmath $\theta$}}
\newcommand{\bfepsilon}{\mbox{\boldmath $\epsilon$}}
\newcommand{\tildeS}{\tilde{S}}
\newcommand{\bfa}{\mbox{\boldmath $a$}}
\newcommand{\bfb}{\mbox{\boldmath $b$}}
\newcommand{\bfY}{\mbox{\boldmath $Y$}}
\newcommand{\bfS}{\mbox{\boldmath $S$}}
\newcommand{\bfv}{\mbox{\boldmath $v$}}
\newcommand{\bfZ}{\mbox{\boldmath $Z$}}
\newcommand{\cardT}{\vert \mathcal{T} \vert}
%\newenvironment{theorem}[1][Theorem]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{corollary}[1][Corollary]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{proposition}[1][Proposition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{definition}[1][Definition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\def\bL{\mathbf{L}}

\begingroup\lccode`~=`_
\lowercase{\endgroup\def~}#1{_{\scriptscriptstyle#1}}
\AtBeginDocument{\mathcode`_="8000 \catcode`_=12 }

\makeatletter
\renewcommand{\theenumi}{\Roman{enumi}}
\renewcommand{\labelenumi}{\theenumi.}
\renewcommand{\theenumii}{\Alph{enumii}}
\renewcommand{\labelenumii}{\theenumii.}
\renewcommand{\p@enumii}{\theenumi.}
\makeatother

\begin{document}

%\nocite{*}
\def\bL{\mathbf{L}}
%\usepackage{mathtime}

%%UNCOMMENT following line if you have package


\title{ Nonparametric Covariance Estimation for Longitudinal Data via Penalized Tensor Product Splines}

\author{Tayler A. Blake\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201} \and  Yoonkyung Lee\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201}}

\bibliographystyle{plainnat}
\maketitle

\subsection{Penalized likelihood estimation}
Let $Y$ hold the $N$ observed response vectors $y_1,\dots, y_N$ less their first element $y_{i1}$ stacked into a single vector of dimension $n_y=\left(\sum \limits_{i} M_i \right) - N$. Let $M$ denote the total number of distinct observation times across all subjects. For ease of exposition, let $\sigma_{ij} = \sigma\left( t_{ij} \right)$ and $\phi_{ijk} = \phi \left(t_{ijk} \right)$. The loglikelihood \ref{eq:loglik-general-form} becomes

\begin{align}
\begin{split} \label{eq:my-log-lik-phi}
-2\ell\left(Y, \Sigma \right) &=  \sum_{t = 1}^M \log \sigma_t^2  + \sum_{i = 1}^N \sum_{j = 1}^{m_i} \frac {\epsilon_{ijk}^2}{\sigma_{ij}^2}\\
&= \sum_{t = 1}^M \log \sigma_t^2  + \sum_{i = 1}^N \frac{\epsilon_{i1}^2}{\sigma_{i1}^2} + \sum_{i = 1}^N \sum_{j = 2}^{m_i} \frac{\epsilon_{ij}^2}{\sigma_{ij}^{2}} \\
&= \sum_{t = 1}^M \log \sigma_t^2  + \sum_{i = 1}^N \frac{y_{i1}^2}{\sigma_{i1}^2} + \sum_{i = 1}^N \sum_{j = 2}^{m_i} \sigma_{ij}^{-2} \left( y_{ij} - \sum \limits_{k < j}\phi_{ijk} y_{ik}  \right)^2.
\end{split}
\end{align}
\noindent

%For ease of exposition, we assume that $\sigma^2\left(t\right)$ is fixed and known.
An iterative procedure for minimizing \ref{eq:my-log-likelihood-phi} starts by first initializing $\sigma_{ij}$ using, for example, the innovation standard error estimated without the penalty. Then we minimize 

\begin{equation} \label{eq:loglik-phi-component}
 \sum_{i = 1}^N \sum_{j = 2}^{m_i} \frac{\epsilon_{ij}^2}{\sigma_{ij}^{2}}
\end{equation}
\noindent
to obtain $\tilde{\phi}^*\left(t,s\right)$. We then obtain an estimate for $\sigma\left(t\right)$ by fixing $\phi^* = \tilde{\phi}^*$ and minimizing \ref{eq:my-log-lik-phi}.  We iterate this process until convergence of the estimated coefficient vectors. 

\bigskip


\section{Computation of the smoothing spline estimator}

The minimization of the penalized smoothing spline log likelihood 

\begin{align}
\begin{split} \label{eq:penalized-likelihood-vectorized} 
 &\mbox{} -2\ell\left(Y \vert c,d\right) + \lambda J_m\left(\phi^*\right)\\
&= \left( Y - W \left( Sd + Qc \right) \right)^\prime D^{-1} \left( Y - W \left( Sd + Qc \right) \right) + \lambda c^\prime Q c 
\end{split}
\end{align} 
\bigskip
\noindent
lies within a space 
\[
\Hilbert \subseteq \left \{ \phi^*: J\left(\phi^*\right) < \infty \right\}
\]
\noindent
in which $J\left(\phi^*\right)$ is a square (semi) norm, or a subspace therein. The evaluation functional $\left[ \bfv \right] \phi^*$, which appears in the first term in \ref{eq:penalized-likelihood-vectorized}, is assumed to be continuous in $\Hilbert$. A space in which the evaluation functional is continuous is called a reproducing kernel Hilbert space (RKHS) endowed with reproducing kernel (RK) $Q\left(\cdot, \cdot\right)$, a non-negative definite function satisfying 

\[
\langle Q\left(\bfv,\cdot\right), \phi^*\left(\cdot\right) \rangle
\]
\bigskip
\noindent
$\forall \phi^* \in \Hilbert$, where $\langle \cdot, \cdot \rangle$ is an inner product in $\Hilbert$. The norm and RK determine each other uniquely.

Let $\mathcal{N}_J = \left\{ \phi^*:\; J\left(\phi^*\right) = 0\right\}$ denote the null space of $J$, and consider the tensor sum decomposition

\[
\Hilbert = \mathcal{N}_J \oplus \Hilbert_J.
\]
\noindent
The space $\Hilbert_J$ is a RKHS having $J\left(\phi^*\right)$ as the squared norm. The minimizer of \ref{eq:penalized-likelihood-vectorized} has form 

\begin{equation} \label{eq:RKHS-functional-form}
\phi^*\left(\bfv\right) = \sum_{\nu = 1}^{d_0} d_\nu \eta_\nu\left( \bfv \right) + \sum_{i=1}^n c_i Q\left(\bfv_i, \bfv \right),
\end{equation} 
\bigskip
\noindent
where $\lbrace \eta_\nu \rbrace$ is a basis for $\mathcal{N}_J$, and $Q_J$ is the RK in $\Hilbert_J$. 

\bigskip

For $\bfv \in \mathcal{X}$ where $\mathcal{X}$ is a product domain, ANOVA decompositions can be characterized by 
\begin{equation*}
\Hilbert = \bigoplus\limits_{\beta=0}^{g} \Hilbert_\beta
\end{equation*}
\noindent
and
\begin{equation*}
J\left(\phi^*\right) = \sum_{\beta=0}^{g} \theta^{-1}_\beta J_\beta \left( \phi^*_\beta \right),
\end{equation*}
\noindent
where $\phi^*_\beta \in \Hilbert_\beta$, $J_\beta$ is the square norm in $\Hilbert_\beta$, and $0 < \theta_\beta < \infty$. This gives 

\begin{align*}
\Hilbert_0 &= \mathcal{N}_J \\
\Hilbert_J &= \bigoplus\limits_{\beta=1}^{g} \Hilbert_\beta, \mbox{ and} \\
Q &= \sum_{\beta=1}^g \theta_\beta Q_\beta,
\end{align*}
\noindent
where $Q_\beta$ is the RK in $\Hilbert_\beta$. The $\left \{ \theta_\beta \right\}$ are additional smoothing parameters, which may or may not appear explicitly in notation to follow. 

\bigskip

Letting $\tildeY = D^{-1/2} Y$, $\tildeS = D^{-1/2} W S$, and $\tildeQ = D^{-1/2} W Q$, the penalized log likelihood \ref{eq:penalized-likelihood-vectorized} may be written

\begin{equation}\label{eq:penalized-loglik-tilde-vectorized}
-2\ell_\lambda \left(c, d \right) + \lambda J\left( \phi^* \right) = \bigg[ \tildeY - \tildeS d - \tildeQ c\bigg]'\bigg[ \tildeY - \tildeS d - \tildeQ c\bigg] + \lambda c'Qc.
\end{equation}
\noindent
Taking partial derivatives with respect to $d$ and $c$ and setting equal to zero yields normal equations 

\begin{align}
\begin{split}
\tildeS'\tildeS d + \tildeS'\tildeQ c &= \tildeS' \tildeY \\
\tildeQ'\tildeS d + \tildeQ'\tildeQ c + \lambda Q c &= \tildeQ' \tildeY, 
\end{split}
\end{align}

\noindent
Some algebra yields that this is equivalent to solving the system

\begin{equation} \label{eq:vectorized-normal-equations}
\begin{bmatrix}
\tildeS'\tildeS & \tildeS'\tildeQ \\
\tildeQ'\tildeS & \tildeQ'\tildeQ + \lambda Q\\
\end{bmatrix}
\begin{bmatrix}
d\\
c\\
\end{bmatrix}
= \begin{bmatrix}
\tildeS'\tildeY \\
 \tildeQ'\tildeY\\
\end{bmatrix}
\end{equation}

Fixing smoothing parameters $\lambda$ and $\theta_\beta$ (hidden in $Q$ and $\tildeQ$ if present), assuming that $\tildeQ$ is full column rank, \ref{eq:vectorized-normal-equations} can be solved by the Cholesky decomposition of the $\left( n + d_0 \right) \times \left( n + d_0 \right)$ matrix followed by forward and backward substitution. See \citet{golub2012matrix}. Singularity of $\tildeQ$ demands special consideration. Write the Cholesky decomposition

\begin{equation} \label{eq:normal-equation-cholesky}
\begin{bmatrix}
\tildeS'\tildeS & \tildeS'\tildeQ \\
\tildeQ'\tildeS & \tildeQ'\tildeQ + \lambda Q\\
\end{bmatrix}
= \begin{bmatrix}
C'_1 & 0 \\
C'_2  & C'_3 
\end{bmatrix}
\begin{bmatrix}
C_1 & C_2 \\
0  & C_3 
\end{bmatrix}
\end{equation}
\noindent
where $\tildeS'\tildeS = C'_1 C_1$, $C_2 = C_1^{-T} \tildeS' \tildeQ$, and $C'_3 C_3 = \lambda Q +  \tildeQ'\left( I - \tildeS\left( \tildeS' \tildeS \right)^{-1} \tildeS' \right)\tildeQ$. Using an exchange of indices known as pivoting, one may write 

\begin{equation*}
C_3 = \begin{bmatrix} H_1 & H_2 \\ 0 & 0 \end{bmatrix} = \begin{bmatrix} H \\  0 \end{bmatrix},
\end{equation*}
\noindent
where $H_1$ is nonsingular. Define
\begin{equation} \label{eq:cholesky-factor-mod}
\tilde{C}_3 = \begin{bmatrix}
H_1 & H_2 \\
0  & \delta I 
\end{bmatrix}, \;\;
\tilde{C} = \begin{bmatrix}
C_1 & C_2 \\
0  & \tilde{C}_3 
\end{bmatrix};
\end{equation}
\noindent
then
\begin{equation} \label{eq:cholesky-factor-mod-inverse}
\tilde{C}^{-1} = \begin{bmatrix}
C_1^{-1} & -C_1^{-1} C_2 \tilde{C}_3^{-1} \\
0  & \tilde{C}_3^{-1}
\end{bmatrix}.
\end{equation}

Premultiplying \ref{eq:normal-equation-cholesky} by $\tilde{C}^{-T}$, straightforward algebra gives 

\begin{equation} \label{eq:vectorized-normal-equations-cholesky}
\begin{bmatrix}
I & 0 \\
0 & \tilde{C}_3^{-T} C_3^{T} C_3 \tilde{C}_3^{-1}\\
\end{bmatrix}
\begin{bmatrix}
\tilde{d}\\
\tilde{c}\\
\end{bmatrix}
= \begin{bmatrix}
C_1^{-T} \tildeS'\tildeY \\
\tilde{C}_3^{-T} \tildeQ'\left( I - \tildeS\left( \tildeS' \tildeS \right)^{-1} \tildeS' \right) \tildeY\\
\end{bmatrix}
\end{equation}
\noindent
where $\left( \tilde{d}'\;\;\tilde{c}' \right)' =  \tilde{C}' \left( d\;\;c \right)'$. Partition $\tilde{C}_3 = \begin{bmatrix} K &  L\end{bmatrix}$; then $HK = I$ and $HL = 0$. So

\begin{align*}
\tilde{C}_3^{-T} C_3^{T} C_3 \tilde{C}_3^{-1} &= \begin{bmatrix} K' \\ L' \end{bmatrix} C'_3C_3 \begin{bmatrix} K &  L\end{bmatrix} \\
&= \begin{bmatrix} K' \\ L' \end{bmatrix} H'H \begin{bmatrix} K &  L\end{bmatrix} \\
&= \begin{bmatrix} I & 0 \\ 0 & 0 \end{bmatrix}.
\end{align*}
\noindent
If $L'C_3^{T} C_3 L = 0$, then $L'\tildeQ'\left( I - \tildeS\left( \tildeS' \tildeS \right)^{-1} \tildeS' \right)\tildeQ L = 0$, so $L'\tildeQ'\left( I - \tildeS\left( \tildeS' \tildeS \right)^{-1} \tildeS' \right) \tildeY = 0$. Thus, the linear system has form

\begin{equation} \label{eq:vectorized-normal-equations-cholesky-2}
\begin{bmatrix}
I & 0 & 0\\
0 & I & 0 \\
0 & 0 & 0 \\
\end{bmatrix}
\begin{bmatrix}
\tilde{d}\\
\tilde{c}_1\\
\tilde{c}_2
\end{bmatrix}
= \begin{bmatrix}
* \\
* \\
0
\end{bmatrix},
\end{equation}
\noindent
which can be solved, but with $c_2$ arbitrary. One may perform the Cholesky decomposition of \ref{eq:vectorized-normal-equations} with pivoting, replace the trailing $0$ with $\delta I$ for appropriate value of $\delta$, and proceed as if $\tildeQ$ were of full rank. 
\bigskip

It follows that

\begin{equation} \label{eq:tildeY-hat-equals-tildeA-tildeY}
\widehat{\tildeY} = \tildeS d + \tildeQ c = \begin{bmatrix} \tildeS & \tildeQ \end{bmatrix} \tilde{C}^{-1} \tilde{C}^{-T} \begin{bmatrix} \tildeS' \\ \tildeQ' \end{bmatrix} \tildeY = \tildeA\left(\lambda, \bftheta\right) \tildeY.
\end{equation} 
\noindent
where
\begin{align}
\begin{split} \label{eq:cholesky-tildeA}
\tildeA\left(\lambda, \bftheta \right) =& \begin{bmatrix} \tildeS & \tildeQ \end{bmatrix} \tilde{C}^{-1} \tilde{C}^{-T} \begin{bmatrix} \tildeS' \\ \tildeQ' \end{bmatrix}  \\
&= B + \left(I - B\right) \tildeQ \left[\tildeQ'\left( I - B \right)\tildeQ + \lambda Q\right]^{-1} \tildeQ'\left(I - B\right),
\end{split}
\end{align} 
\noindent
for
\[
B = \tildeS\left(\tildeS' \tildeS \right)^{-1}\tildeS'.
\]


The smoothing matrix $\tildeA$ plays an integral role of calculating all of the model selection criteria we will discuss in the sections to follow;  the diagonal elements of $\tildeA$, $\tilde{a}_{kk}$ are of particular importance in quantifying  model complexity. In classical regression theory, the degrees of freedom are clearly defined as the number of variables included in the model. \citet{eilers1996flexible} and\citet{marx2005multidimensional} refer to this measure of model complexity as the model's \emph{effective dimension} ED; they follow \citet{hastie1990generalized}, who discuss the effective dimensions of linear smoother and propose to use the trace of the smoother matrix as an approximation. 

\section{Model selection criteria}

By varying smoothing parameters $\lambda$ and $\theta_\beta$, the minimizer $\phi^*_\lambda$ of \ref{eq:penalized-loglik-tilde-vectorized} defines a family of potential estimates. In practice, we need to choose a specific estimate from the family, which requires effective methods for smoothing parameter selection. We consider three criteria that are commonly used for smoothing parameter selection in the context of smoothing spline models. The first score is an unbiased estimate of a relative loss and assumes a known variances $\sigma_t^2$. The second score, the generalized cross validation (GCV) score of \citet{craven1978smoothing}, provides an estimate of the same loss without assuming a known variance function. These scores have attractive asymptotic properties; see \citet{gu2013smoothing} for a comprehensive examination.   To simplify presentation for the initial presentation, we only make explicit the dependence of estimates and their components on $\lambda$ and conceal any dependence on $\theta_\beta$. 

\subsection{Unbiased risk estimate}

We can write

\begin{equation} 
\tildeY = D^{-1/2}W \Phi^* + \tildeepsilon,
\end{equation}
\noindent
where 
\[
\Phi^* = \left( \phi^*\left( \bfv_{121} \right), \phi^*\left( \bfv_{131} \right), \dots, \phi^*\left( \bfv_{N,m_N, m_n -1} \right) \right)'
\]
\noindent
denotes the vector holding the values of $\phi^*$ evaluated at the observed within-subject pairs of time points, and $\tildeepsilon = D^{-1/2} \epsilon$ where $\epsilon$ is the vector of $\sum_{i = 1}^Nm_i - N$ associated prediction errors.  We can assess $\hat{\tildeY}_\lambda$, an estimate of the mean of $\tildeY$ based on observed data $y_{ij}$, $i = 1,\dots, N$, $j = 1,\dots, m_i$, using the loss function

\begin{align}
\begin{split}
L\left(\lambda\right) &= \sum_{i = 1}^N \sum_{j = 1}^{m_i} \left(\hat{\tildey}_{ij} - E\left[\tildey_{ij}\right] \right)^2\\
&= \vert \vert \tildeY - \tilde{\mu} \vert \vert^2
\end{split}
\end{align}
\noindent
where $\mu = D^{-1/2}W \Phi^*$ denotes the $\left( \sum \limits_{i} m_i - N\right) \times 1$ with $i^{th}$ element equal to the expected value of the  $i^{th}$ element of $\tildeY$.  Then straightforward algebra yields that 

\begin{align} 
L\left(\lambda\right) = \mu'\left( I - \tildeA \right)^2\mu - 2\mu'\left( I - \tildeA \right)^2 \tildeA \tildeepsilon + \tildeepsilon' \tildeA^2 \tildeepsilon
\end{align}

Define the unbiased risk estimate
\begin{equation} 
U\left(\lambda\right) = \tildeY'\left( I - \tildeA \right)^2\tildeY + 2\mbox{tr}\tildeA
\end{equation}
 \noindent
Adding and substracting $\mu$ to the quadratic terms, one can verify with straightforward algebra that

\begin{align}
\begin{split}
U\left(\lambda\right) &= \left( \tildeY - \mu + \mu - \tildeA \tildeY \right)'\left( \tildeY - \mu + \mu - \tildeA \tildeY \right) + 2\mbox{tr}\tildeA \\
&= \left(\tildeA \tildeY - \mu \right)'\left( \tildeA \tildeY - \mu \right) + \tildeepsilon'\tildeepsilon + 2\tildeepsilon' \left( I- \tildeA\right)\mu- 2\left( \tildeepsilon'\tildeA \tildeepsilon -  \mbox{tr}\tildeA\right)
\end{split}
\end{align}
\noindent
This gives
\begin{equation} 
U\left(\lambda\right) - L\left(\lambda\right) - \tildeepsilon'\tildeepsilon  =  2\tildeepsilon' \left( I- \tildeA\right)\mu- 2\left( \tildeepsilon'\tildeA \tildeepsilon -  \mbox{tr}\tildeA\right), 
\end{equation}
 \noindent
 which allows one to easily see that $U\left(\lambda\right)$ is unbiased for the relative loss $L\left(\lambda\right) + \tildeepsilon'\tildeepsilon$.  Under mild conditions on the risk function
 
 \[
 R\left(\lambda\right) = E\left[L\left(\lambda\right)\right],
 \]
\noindent
one can establish that $U$ is also a consistent estimator. See \citet{gu2013smoothing} Chapter 3 for a formal theorem and proof.


\subsection{Leave-one-out and generalized cross validation}
The use of the unbiased risk estimate $U\left( \lambda \right)$ to select the optimal smoothing parameter requires knowledge of the innovation variance $\sigma\left(t \right)^2$, which is, in practice, unknown and we can at best approximate with an estimate. An alternative for selecting $\lambda$ is cross validation; it and its variants have long been utilized for smoothing parameter selection in spline models, and their properties have been studied extensively. A short list of supplemental references include \citet{craven1978smoothing}, \citet{wahba1985comparison}, \citet{hardle1988far}, \citet{hardle1992applied}, and \citet{wahba1990spline}. There are a number of ways to calculate a measure of cross validated prediction error; we first focus on the leave-one-out method. Let $\hat{\tildey}^{\left[-ij\right]}_{ij}$ denote the predicted value for the observation $\tildey_{ij}$ when $\tildey_{ij}$ itself is removed from the data used for fitting the model. We can calculate these predictions for each observation in the data set to obtain the leave-one-out (LOO) cross validation score:

\begin{equation} \label{eq:LOOCV}
V_0\left(\lambda\right) = \frac{1}{N} \sum_{i=1}^N \sum_{j=2}^{m_i} \frac{1}{m_i} \left(  y_{ij} - \hat{y}^{\left[-ij\right]}_{ij}\right)^2, 
\end{equation}

Brute force calculation of \ref{eq:LOOCV_def} is generally impractical, especially if the number of observations is large. However, this labor can be sidestepped using the following fact: 

\begin{equation*}
\widehat{\tildeY} = \tildeA \tildeY
\end{equation*}
\noindent

With some abuse of notation, let $\tildey_{k}$ denote the $k^{th}$ element of the full vector of responses $\tildeY$, for $k = 1, \dots, \sum_i m_i - N$. One can show that 

\begin{equation} \label{eq:LOO_residual}
y_k - \hat{y}^{\left[-k\right]}_{k} = \left(y_k - \hat{y}_{k}\right)/\left(1-\tilde{a}_{kk}\right),
\end{equation}
\noindent
where $\left\{\tilde{a}_{kk}\right\}$ denote the diagonal elements of the smoothing matrix $\tildeA$, which can be calculated quickly. An informal proof of \label{eq:LOO_residual} is as follows: suppose that we change the $i^{th}$ element of $\tildeY$, obtaining a new response vector $\tildeY^*$. Then $\widehat{\tildeY}^* = \tildeA \tildeY^*$.  Since 
 \[
 \hat{\tildey}_k = \sum_{l} \tilde{a}_{kl} \tildey_l 
 \]
 \noindent 
 and 
 \[
 \hat{\tildey}_{-k} = \sum_{l} \tilde{a}_{ij} \tildey_j^*,
 \]
 \noindent
 we have that
 \[
 \hat{\tildey}_k - \hat{\tildey}_{-k} = \sum_l \tilde{a}_{kl} \left(\tildey_k - \tildey_k^*\right) = \tilde{a}_{kk} \left(\tildey_k - \tildey_k^*\right).  
\]
\noindent
With this, \ref{eq:LOOCV} can be rewritten as 
\begin{align*} 
V_0\left(\lambda\right) &= \frac{1}{{\sum_{i}m_i - N}} \sum_{k = 1}^{\sum_{i}m_i - N}  y_k - \hat{y}^{\left[-k\right]}_{k}\\
 &=  \frac{1}{{\sum_{i}m_i - N}} \sum_{k = 1}^{\sum_{i}m_i - N}  \left(\tildey_k - \hat{\tildey}_{k}\right)^2/\left(1-\tilde{a}_{kk}\right)^2 , 
\end{align*}
\noindent
The best $\lambda$ is the value that minimizes the cross validation score. The leave-one-out cross validation score weighs all data points equally in the estimate of prediction error. However, one may wish to adjust for any imbalance in the contribution of the $t_{ij}$ to the estimate of $\phi$, which can be done by simply weighting observations when averaging the prediction errors:
   
\begin{equation} \label{eq:weighted_LOOCV} 
\bar{V}\left(\lambda\right) = \frac{1}{\sum_{i}m_i - N} \sum_{k = 1}^{\sum_{i}m_i - N}  \tildey_k - \hat{\tildey}^{\left[-k\right]}_{k} = \omega_k \left(\tildey_k - \hat{\tildey}_{k}\right)^2/\left(1-\tilde{a}_{kk}\right)^2 . 
\end{equation}
\noindent
We obtain Craven and Wahba's generalized cross validation score (GCV) if we take
\[
\omega_i = \left(1 - \tilde{a}_{kk}\right)^2/\left[\frac{\mbox{tr}\left( I - \tildeA\left(\lambda\right)\right)}{\sum_{i}m_i - N}\right]^2,
\]
which is equivalent to substituting $\tilde{a}_{kk}$ in \ref{eq:weighted_LOOCV} for the average $\left(\sum_{i}m_i - N \right)^{-1} \sum_{k=1}^{\sum_{i}m_i - N} \tilde{a}_{kk}$. Under mild conditions, the GCV score is a consistent estimator of relative loss \ref{eq:}, see \citet{craven1978smoothing} for detailed discussion.
  
\subsection{Leave-one-subject-out cross validation}  
The conditions under which the the cross validation and GCV scores yield desirable properties generally do not hold when the data are clustered or longitudinal in nature. Instead, the leave-one-subject-out (LosoCV) cross validation score has been widely used for smoothing parameter selection for semiparametric and nonparametric models for longitudinal or functional data. The LosoCV criterion is defined as

\begin{equation} \label{eq:LOSOCV}
V_{loso}\left(\lambda\right) = \frac{1}{N}\sum_{i=1}^N \left( Y_i - \widehat{\tilde{\mu}}^{\left[-i\right]}_{i}\right)'\left( Y_i -  \widehat{\tilde{\mu}}^{\left[-i\right]}_{i}\right)
\end{equation}
\noindent
where $\widehat{\tilde{\mu}}^{\left[-i\right]}_{i}$ is the estimate of $E\left[ \tildeY_i \right]$ based on the data when $\tildeY_i$ is omitted. Intuitively, the LosoCV score is appealing because it preserves any within-subject dependence by leaving out all observations from the same subject together in the cross-validation.  However, despite its prevalent use, theoretical justifications for its use have not been established. In their seminal work, \citet{rice1991estimating} were the first to present a heuristic justification of LosoCV by demonstrating that it mimics the mean squared prediction error.
\bigskip

Consider new observations $Y^*_i = \left(y_{i1}^*, y_{i1}^*, \dots, y_{i, m_i}^*\right)$  
\bigskip 

[Present heuristic argument here. See Xu, G.,  Huang, J. Z. (2012). Asymptotic optimality and efficient computation of the leave-subject-out cross-validation. The Annals of Statistics, 40(6), 3003-3030.] 

\begin{align}
\begin{split}\label{eq:MSPE}
MSPE &= \frac{1}{N}\sum_{i=1}^N E\left[ \vert \vert \tildeY^*_i - \widehat{\tilde{\mu}}_{i} \vert \vert^2 \right]\\
&=  \frac{1}{N}\sum_{i=1}^N E\left[ \vert \vert \tildeY^*_i - D_i^{-1/2}W_i \Phi^* + D_i^{-1/2}W_i \Phi^* - D_i^{-1/2}W_i \hat{\Phi}^*\vert \vert^2 \right]\\
&=  \frac{1}{N}\sum_{i=1}^N \left\{m_i + E\left[ \vert \vert \tilde{\mu}_{i} - \widehat{\tilde{\mu}}^{\left[ -i \right]}_{i} \vert \vert^2 \right] \right\}
\end{split}
\end{align}
\noindent
where $\tilde{\epsilon}_i = \tildeY^*_i - D_i^{-1/2}W_i \Phi^*$. When $\left\{ \sigma^2\left(t\right)\right\}$ is known, $\tilde{\epsilon}_i$ is a mean zero multivariate normal vector with $Cov\left(\tilde{\epsilon}_i\right) = I_{m_i}$, which gives the last equality. Since $\tildeY_i$ and $ \widehat{\tilde{\mu}}_{i} $ are independent, the expected LosoCV score can be written
\begin{equation} \label{eq:MSPE_LOSOCV}
E\left[V_{loso}\left(\lambda\right) \right] =  \frac{1}{N}\sum_{i=1}^N\left\{ m_i +  E\left[ \vert \vert \widehat{\tilde{\mu}}_{i} - \tilde{\mu}_{i} \vert \vert^2 \right] \right\}. 
\end{equation}
\noindent
When $N$ is large, we expect that $\widehat{\tilde{\mu}}_{i}$ should be close to $\widehat{\tilde{\mu}}^{\left[ -i \right]}_{i}$, so $E\left[V_{loso}\left(\lambda\right) \right]$ should be a good approximation to the mean-squared prediction error. 
 
 \subsubsection{Formal justification for LosoCV by showing that it is asymptotically equivalent to loss function when appropriately defined}
 
 
 \subsubsection{Regularity conditions necessary for asymptotic properties of LosoCV score to hold} 
 
 \subsubsection{Optimality of LosoCV score}
 
  \subsubsection{Computation of the LosoCV score}
  
  \begin{lemma}[Shortcut formula for LosoCV] \label{lemma:losocv-shortcut}
  The LosoCV score satisfies the following identity:
  \begin{equation*}
 V_{loso}\left( \lambda \right) = \frac{1}{N} \sum_{i = 1}^N \left(\tildeY_i - \widehat{\tildeY_i}\right)' \left(I_{ii} - \tildeA_{ii}\right)^{-T}\left(I_{ii} - \tildeA_{ii}\right)^{-1}\left(\tildeY_i - \widehat{\tildeY_i}\right),
  \end{equation*}
  \noindent
  where $\tildeA_{ii}$ is the diagonal block of smoothing matrix $\tildeA$ corresponding to the observations on subject $i$, and $I_{ii}$ is a $m_i \times m_i$ identity matrix.
\end{lemma}

See \citet{xu2012asymptotic} and supplementary materials \citet{xuasymptotic} for a detailed presentation and proof.  

\begin{proof} \label{proof:losocv-shortcut}
For fixed $\lambda$, $\bftheta$, $\sigma^2\left(t \right)$, let $\hat{a}^{\left[-i\right]} = \left(\hat{d}^{\left[-i\right]}, \hat{c}^{\left[-i\right]}\right)$ denote the minimizer of the penalized log likelihood 
\end{proof}  

\subsubsection{Approximation of leave-one-subject-out cross validation}
  
\citet{xu2012asymptotic} additionally proposed an approximation to the LosoCV score to further reduce the computational cost of evaluating $V_{loso}$, which can be expensive due to the inversion of the $I_{ii} - \tildeA_{ii}$. Using the Taylor expansion of $\left(I_{ii} - \tildeA_{ii}\right)^{-1} \approx I_{ii} + \tildeA_{ii}$, we can use the following to approximate $V_{loso}$:

\begin{equation} \label{eq:approx-losocv}
V_{loso}^*\left( \lambda \right) = \frac{1}{N} \vert \vert \left(I - \tildeA\right)\tildeY \vert \vert^2 + \frac{2}{N} \sum_{i = 1}^N \hat{\tilde{e}}'_{i}\tildeA_{ii}\hat{\tilde{e}}_i,
\end{equation}
\noindent
where $\hat{\tilde{e}}_i$ is the portion of the vector of prediction errors $\left(I - \tildeA\right)\tildeY$ corresponding to subject $i$.   

\begin{theorem} \label{thm:asymptotic-equiv-of-losocv}
Under conditions 1-5, for predetermined $\sigma^2\left(t\right)$ and nonrandom $\lambda$, 
\begin{equation}
V_{loso}\left(\lambda \right) - L\left(\lambda \right) - \frac{1}{N} \epsilon'\epsilon - o_p\left( L\left(\lambda \right)\right).
\end{equation}
\noindent
as $n \rightarrow \infty$ 
\end{theorem}
  
\vspace{0.8in} 


\subsection{Selection of multiple smoothing parameters}

The expression in \ref{eq:cholesky-tildeA} permits the straightforward evaluation of the GCV score

\begin{equation} \label{eq:general-GCV-score}
V\left(\lambda, \bftheta \right) = \frac{\left( 1/n_y \right)  \left \lVert \left(I - \tildeA\left(\lambda, \bftheta \right)\right) \tildeY \right\rVert^2}{\left[\left( 1/n_y \right) \textup{tr} \left(I - \tildeA\left(\lambda, \bftheta \right)\right)  \right]^2}
\end{equation}
\noindent
and the GML score
\begin{equation} \label{eq:general-GML-score}
M\left(\lambda, \bftheta \right) = \frac{\left( 1/n_y \right) \tildeY' \left(I - \tildeA\left(\lambda, \bftheta \right)\right) \tildeY }{\left[ \mbox{det}^+\left(I - \tildeA\left(\lambda, \bftheta \right)\right)  \right]^{1/n_y}}.
\end{equation}
\noindent
where $\bftheta = \left(\theta_1,\dots, \theta_g\right)'$ denotes the vector of smoothing parameters associated with each RK.
\bigskip
To minimize the functions $V\left(\lambda, \bftheta\right)$ and $M\left(\lambda, \bftheta\right)$ with respect to $\bftheta$ and $\lambda$, we iterate as follows:

\begin{enumerate}
\item Fix $\bftheta$; minimize $V\left(\lambda \vert \bftheta\right)$ or $M\left(\lambda \vert \bftheta\right)$ with respect to $\lambda$.
\item Update $\bftheta$ using the current estimate of $\lambda$.
\end{enumerate}
\noindent
Executing step 1 follows immediately from the expression for the smoothing matrix. Step 2 requires evaluating the gradient and the Hessian of $V\left( \bftheta \vert \lambda \right)$ or $M\left(\bftheta \vert \lambda\right)$ with respect to $\bfkappa = \log\left(\bftheta\right)$. Optimizing with respect to $\bfkappa$ rather than on the original scale is motivated by two driving factors: first, $\bfkappa$ is invariant to scale transformations. With examination of $V$ and $M$ and \ref{eq:cholesky-tildeA}, it is immediate that the $\theta_\beta \tildeQ_\beta$ are what matter in determining the minimum. Multiplying the $\tildeQ_\beta$ by any positive constant leaves the $\theta_\beta$ subject to rescaling, though the problem itself is unchanged by scale transformations. The derivatives of $V\left(\cdot\right)$ and $M\left(\cdot\right)$ with respect to $\bfkappa$ are invariant to such transformations, while the derivatives with respect to $\bftheta$ are not. In addition, optimizing with respect to $\bfkappa$ converts a constrained optimization ($\theta_\beta \ge 0$) problem to an unconstrained one.

\subsection{Algorithms}

The main algorithm and discussion of its key components are presented in the section to follow. The minimization of the model selection criterion is done via two nested loops. Fixing tuning parameters, the outer loop minimizes $V$ (or $M$) with respect to smoothing parameters via quasi-Newton iteration of \citet{dennis1996numerical}, as implemented in the \texttt{nlm} function in \texttt{R}. The inner loop then minimizes $\ell_\lambda$ with fixed tuning parameters via Newton iteration with step-halving as safeguards. Fixing the $\theta_\beta$s in $J \left(\phi^*\right) = \sum_\beta \theta^{-1}_\beta J_\beta \left(\phi_\beta^*\right)$, the outer loop with a single $\lambda$ is a straightforward task. 



\begin{algorithm}[H]
\caption{ }
\begin{algorithmic}
\STATE \textbf{Initialization:} 
	\STATE Set $\Delta \bfkappa := 0$; \;$\bfkappa_{-}:=\bfkappa_{0}$; \;$V_- = \infty$; \;( or $M_- = \infty$)

\STATE \textbf{Iteration:} 
	\WHILE{not converged}
		\STATE For current value $\bfkappa_* = \bfkappa_- + \Delta \bfkappa$, compute $Q_*^\theta = \sum_{\beta = 1}^g \theta_\beta Q_\beta$. 
		\STATE Compute $\tildeA\left(\lambda \vert \bftheta_* \right) = \tildeA\left(\lambda, \exp\left({\bfkappa_*} \right)\right)$.
		\STATE Minimize \begin{equation*} V\left(\lambda \vert \bfkappa_* \right) = \frac{\left( 1/n_y \right)  \left \lVert \left(I - \tildeA\left(\lambda \vert \bftheta_* \right)\right) \tildeY \right\rVert^2}{\left[\left( 1/n_y \right) \textup{tr} \left(I - \tildeA\left(\lambda \vert \bftheta_* \right)\right)  \right]^2} \end{equation*} or  \begin{equation*} 
	M\left(\lambda \vert \bfkappa_* \right) = \frac{\left( 1/n_y \right) \tildeY' \left(I - \tildeA \left(\lambda \vert \bftheta_* \right)\right) \tildeY }{\left[ \mbox{det}^+\left(I - \tildeA\left(\lambda \vert \bftheta_* \right)\right)  \right]^{1/n_y}}.\end{equation*} \\
			Set 
			\begin{align*}
			V_* := \min \limits_\lambda V\left( \lambda \vert \bfkappa_* \right) \\
			\left( M_* := \min \limits_\lambda M\left(\lambda \vert \bfkappa_* \right) \right)
			\end{align*}
		\IF{$V_* > V_-$ (or $M_* > M_-$)}
		 		\STATE Set $\Delta \bfkappa := \Delta \bfkappa/2$
		 		\STATE Go to (1).
		\ELSE
		\STATE Continue
		\ENDIF
		\STATE Evaluate gradient $\mathbf{g} = \left(\partial /\partial \bfkappa\right) V\left(\bfkappa \vert \lambda\right)$ $\left(\mbox{or }\left(\partial /\partial \bfkappa\right) M\left(\bfkappa \vert \lambda\right) \right)$
		\STATE Evaluate Hessian $H = \left(\partial^2 /\partial \bfkappa\partial \bfkappa' \right) V\left(\bfkappa \vert \lambda\right)$ $\left(\mbox{or } \left(\partial^2 /\partial \bfkappa\partial \bfkappa' \right) M\left(\bfkappa \vert \lambda \right) \right)$.
		\STATE Calculate step $\Delta \bfkappa$:
			\IF{$H$ positive definite}  
				\STATE $\Delta \bfkappa := -H^{-1} \mathbf{g}$
			\ELSE
				\STATE $\Delta \bfkappa := -\tilde{H}^{-1} \mathbf{g}$, where $\tilde{H} = \textup{diag}\left(\bfepsilon\right)$ is positive definite.
			\ENDIF
	\ENDWHILE
\STATE \textbf{Calculate optimal model:} 
	\IF{$\Delta \kappa_\beta < -\gamma$, for $\gamma$ large}
		\STATE Set $\kappa_{*\beta} := -\infty$
	\ENDIF
	\STATE Compute $Q_*^\theta = \sum_{\beta = 1}^g \theta_{*\beta} Q_\beta$;
	\STATE Calculate $\begin{bmatrix} d \\ c \end{bmatrix} = \tilde{C}^{-1} \tilde{C}^{-T} \begin{bmatrix} \tildeS' \\ {\tildeQ_*^\theta}' \end{bmatrix} \tildeY$
					
\end{algorithmic}
\end{algorithm}

The update direction $\Delta \bfkappa = -\tilde{H}^{-1} \mathbf{g}$ is calculated via the modified Newton method on the modified Cholesky decomposition given in \ref{eq:cholesky-factor-mod}. Detailed discussion can be found in \citet{gill1981practical}.
\bigskip

The starting values for the $\theta$ quasi-Newton iteration are obtained with two passes of the fixed-$\theta$ outer loop as follows:

\begin{enumerate}
\item Set $\breve{\theta}_\beta^{-1} \propto \mbox{tr}\left( \tildeQ_\beta \right)$, minimize $V\left(\lambda\right)$ with respect to $\lambda$ to obtain $\breve{\phi}^*$. \label{theta-starting-values-1}
\item Set $\check{\theta}_\beta^{-1} \propto  J_\beta\left(\breve{\phi}^*_\beta \right)$, minimize $V\left(\lambda\right)$ with respect to $\lambda$ to obtain $\check{\phi}^*$. \label{theta-starting-values-2}
\end{enumerate}
\noindent
The first pass allows equal opportunity for each penalty to contribute to the GCV score, allowing for arbitrary scaling of $J_\beta \left(\phi^*_\beta\right)$. The second pass grants greater allowance to terms exhibiting strength in the first pass. The following $\theta$ iteration fixes $\lambda$ and starts from $\check{\theta}_\beta$. These are the starting values adopted by \citet{gu1991minimizing}; the starting values for the first pass loop are somewhat arbitrary, but are invariant to scalings of the $\theta_\beta$. The starting values in \ref{theta-starting-values-2} for the second pass of the outer are based on more involved assumptions derived from the background formulation of the smoothing problem. See \citet{gu1991minimizing} for a detailed discussion.

\bigskip
TO DO: Outline the argument for using the starting values $\breve{\theta}_\beta$
\bigskip

\subsection{Algorithm}

\subsubsection{Computation of the gradient and the Hessian of $V\left(\lambda\right)$}

\subsubsection{Starting values for the Newton iteration}


\section{Computation of the P-spline estimator}

\section{Smoothing parameter selection for tensor product P-splines}


\end{document}
