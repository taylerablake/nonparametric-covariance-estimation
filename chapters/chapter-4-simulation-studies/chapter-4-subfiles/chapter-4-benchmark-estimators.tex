
In the case of complete data with common observation times across all subjects, we consider four additional covariance estimators for comparison. Three of these are the sample covariance matrix $S$ and two of its regularized variants: the soft thresholding estimator of \citet{rothman2009generalized},  $S^\lambda$, and the tapering estimator of \citet{cai2010optimal}, $S^\omega$. See Chapter 2, Section~\ref{section:element-wise-shrinkage-estimators} for additional discussion of these estimators and those belonging to similar classes. We also consider a parametric estimator based on the modified Cholesky decomposition; the MCD polynomial estimator proposed by   \citet{pourahmadi1999joint}, \citet{pan2006regression}, \citet{pourahmadi2002dynamic} and others is discussed in detail in Section~\ref{chapter-1-matrix-decompositions}. 

\bigskip

\citet{rothman2009generalized} presented a class of generalized thresholding estimators, including the soft-thresholding estimator given by

\[
S^{\lambda}=   \begin{bmatrix} \mbox{sign}\left(s_{ij}\right) \left(s_{ij} - \lambda\right)_+ \end{bmatrix},
\]
\noindent 
where $\sigma^*_{ij}$ denotes the $i$-$j^{th}$ entry of the sample covariance matrix, and $\lambda$ is a penalty parameter controlling the amount of shrinkage applied to the empirical estimator. \citet{cai2010optimal} derived optimal rates of convergence under the operator norm for the tapering estimator:
\[
S^{\omega} =  \begin{bmatrix} \omega_{ij}^k s_{ij} \end{bmatrix},
\]
\noindent
where the $\omega_{ij}^k$ are given by 
\begin{equation*}
\omega^k_{ij} = k_h^{-1} \left[ \left( k - \vert i-j\vert\right)_+ - \left(k_h - \vert i-j\vert\right)_+ \right],
\end{equation*}
\noindent
The weights $\omega^k_{ij}$ are indexed with superscript to indicate that they  are controlled by a tuning parameter, $k$,  which can take integer values between 0 and $M$, the dimension of the covariance matrix.  Without loss of generality,  we assume that $k_h = k/2$ is even. The weights may be rewritten as
\begin{align*}
\omega_{ij} = \left\{\begin{array}{ll} 1, & \vert \vert i -j \vert \vert \le k_h \\
                             2 - \frac{i - j}{k_h}, & k_h < \vert \vert i -j \vert \vert \le k, \\
                             0, & \mbox{otherwise}  \end{array} \right.
\end{align*}
\noindent
This expression of the weights makes it clear how the selection of $k$ controls the amount of shrinkage applied to different elements of the sample covariance matrix. The estimator applies no shrinkage to elements of $S$ belonging to the subdiagonals closest to the main diagonal. As one moves away from the main diagonal, shrinkage increases. A shrinkage factor of $2 - \frac{i - j}{k_h}$ is applied to elements belonging to subdiagonals $k_h,\dots,k-1,k$, and elements further than $k$ subdiagonals from the main diagonal are shrunk to zero.   

\bigskip

In the spirit of the GLM, the MCD polynimal estimator is a particular case of estimators which model the components of the Cholesky decomposition using covariates. The polynomial estimator takes the GARPs and IVs to be polynomials of lag and time, respectively:

\begin{align}
\begin{split}  \label{eq:GARP-IV-parametric-model}
\phi_{jk} &= z'_{jk} \gamma \\
\log \sigma^2_{jk} &= z'_{i}\lambda, 
\end{split}
\end{align}
\noindent
for $j = 1,\dots, M$, $k = 1,\dots, j-1$. The vectors $z_j$ and $z_{jk}$ are of dimension $q \times 1$ and $p \times 1$  containing covariates
\begin{align}
\begin{split}  \label{eq:GARP-IV-parametric-model}
z'_{jk} &= \left(1, t_j - t_k, \left(t_j - t_k\right)^2,\dots, \left(t_j - t_k\right)^{p-1}\right)', \\
z'_{i}  &= \left(1, t, \dots, t^{q-1}\right)'.
\end{split}
\end{align}
\noindent
where polynomial orders $p$, $q$ are chosen by BIC. 
